{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Direct Implementation of Linear Regression and PCA \n",
    "import numpy as np\n",
    "from scipy import linalg as la\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Linear regression fits a linear model to a set of data such that the sum of squares error is minimised. \n",
    "\n",
    "- Often formulated separately for the 1 dimentional case, and yet again for multiple regression and multivariate regression (the most general form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted parameters: [1.65795389 0.09025386]\n",
      "True parameters: [1.70870116 0.05053263]\n",
      "Squared loss: 3.83359231972194\n"
     ]
    }
   ],
   "source": [
    "#Begin with Linear Regression: (multiple regression case)\n",
    "def Fit_Linear_Model(X, y):\n",
    "    #Through formulating the sum of squares loss function and taking the derivative with respect to the parameters,\n",
    "    #one can obtain the well known linear regression matrix equation: inv(X^T@X)@X^T@y \n",
    "    return np.linalg.inv(np.transpose(X)@X)@np.transpose(X)@y\n",
    "\n",
    "#generate some toy data to test the regression. \n",
    "\n",
    "d = 2\n",
    "\n",
    "#generate example slope and offset to try predict (case of dim(y)=1)\n",
    "true_params = np.random.randn(d)\n",
    "samples = 10000\n",
    "X = np.random.rand(samples, d)\n",
    "y = [true_params@x for x in X] \n",
    "y_noisy = np.array([y_i + np.random.randn() for y_i in y])\n",
    "\n",
    "#plt.plot(x_vals, X_noisy)\n",
    "#plt.show()\n",
    "\n",
    "res = Fit_Linear_Model(X, y_noisy)\n",
    "fitted_values = [res@x for x in X]\n",
    "\n",
    "def squared_loss(x, y):\n",
    "    return np.sum([(x[i] - y[i])**2 for i in range(0,len(x))])\n",
    "\n",
    "\n",
    "#check to see how close the estimated parameters are\n",
    "print(\"Fitted parameters: \" + str(res))\n",
    "print(\"True parameters: \" + str(true_params))\n",
    "\n",
    "#find loss for fit\n",
    "loss_reg = squared_loss(y, fitted_values)\n",
    "print(\"Squared loss: \" + str(loss_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PCA Notes\n",
    "Decomposes a matrix into a new coordinate system, where each axis is orthogonal (uncorrelated). The axes are constructed such that the original data projected onto the first axis will have maximum variance, with the remaining axes being in decreasing order of their explained variance. Can then project onto a smaller set of axes while minimising the amount of variance which is lost.   \n",
    "\n",
    "- Is an example of a biplot, in the sense that the data is represented with respect to 2 entities: the principal axes and the component score. \n",
    "- Is formulated in terms of maximising covariance for each component, but is equivalent to a singular value decomposition (and is implemented as such for numerical stability)\n",
    "- Requires normalisation as, for example, different covariates will be on different scales (and so have different variance, which would effect the influence they have on how the axes are derived). This point explains why centering is a large issue in many of the CoDA papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#implement PCA\n",
    "def PCA(X, d=X.shape[1]):\n",
    "    #normalise the data\n",
    "    X = [(x_i - np.mean(x_i))/np.std(x_i) for x_i in X]\n",
    "    \n",
    "    #easiest method: compute the singular value decomposition of X\n",
    "    #the right singular vectors will be the principal axes, and the matrix US will be the scores  \n",
    "    #(since S is the scaling of the unit directions corresponding to normalised matrix U)\n",
    "    \n",
    "    U, S, V_T = la.svd(X, full_matrices=False)\n",
    "    Sigma_d = np.diag(S[:d])\n",
    "    \n",
    "    #score matrix T_d in terms of truncated U_d and Sigma_d\n",
    "    \n",
    "    T_d = U[:,:d]@Sigma_d\n",
    "    return T_d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

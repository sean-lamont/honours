{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0705 16:28:28.496432 140087811557184 deprecation_wrapper.py:119] From /home/sean/Documents/Honours/honours/Code/coda-pca-orig/coda/codes/CodaPCA.py:14: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W0705 16:28:28.497196 140087811557184 deprecation_wrapper.py:119] From /home/sean/Documents/Honours/honours/Code/coda-pca-orig/coda/codes/CodaPCA.py:14: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import CodaPCA\n",
    "import CodaRegress\n",
    "import numpy as np\n",
    "from runpca import read_csv\n",
    "import os\n",
    "import sklearn\n",
    "import torch\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#change module for newer sklearn versions\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from sklearn.model_selection  import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data. Given an array of which files are regression, classification or unlabelled\n",
    "data_path = os.getcwd() + \"/Aitchinson\"\n",
    "\n",
    "regression_list = [3,4,5,18,21,34,39]\n",
    "classification_list = [7,8,9,11,12,16,17,19,23,24,25,26,28,29,33,37]\n",
    "unlabelled_list=[1,2,6,10,13,14,15,20,22,27,30,31,32,35,36,38,40]\n",
    "\n",
    "r_files = []\n",
    "c_files = []\n",
    "u_files = []\n",
    "\n",
    "\n",
    "for file in os.listdir(data_path):\n",
    "    for i in regression_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            r_files.append(\"Aitchinson/\" + file)\n",
    "    for i in classification_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            c_files.append(\"Aitchinson/\" + file)\n",
    "    for i in unlabelled_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            u_files.append(\"Aitchinson/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coda_val(features, targets, n_components, folds):\n",
    "    targets = targets.reshape(-1,1)\n",
    "    param_splits = 3\n",
    "    kfold_scores = []\n",
    "    for train, test in folds:        \n",
    "        Y_train = targets[train]\n",
    "        X_train = features[train]\n",
    "        \n",
    "       \n",
    "        Y_test = targets[test]\n",
    "        X_test = features[test]\n",
    "        \n",
    "        \n",
    "        kf_inner = KFold(param_splits)\n",
    "        \n",
    "        inner_folds = [i for i in kf_inner.split(X_train)]      \n",
    "            \n",
    "        #inner loop for parameter selection (lambda term in combined loss):\n",
    "        param_grid = [0.01,0.05,0.1,0.5,1.0,5.0,10.0,50.0,100.0]\n",
    "        \n",
    "        \n",
    "        for a in param_grid:\n",
    "            max_error = np.inf\n",
    "            \n",
    "   \n",
    "            \n",
    "            cval_error = []\n",
    "            #find the parameter which obtains the best inner cross val score\n",
    "            for train_inner, test_inner in inner_folds:\n",
    "                \n",
    "                model = CodaRegress.CoDA_Regress(features.shape[1], n_components, [10,], [2,])\n",
    "                \n",
    "                model.fit(torch.FloatTensor(X_train[train_inner]),  torch.FloatTensor(Y_train[train_inner]), a, lr=1e-2)\n",
    "\n",
    "                y_inner_pred = model.predict(torch.FloatTensor(X_train[test_inner]))\n",
    "    \n",
    "                cval_error.append(sklearn.metrics.mean_squared_error(Y_train[test_inner],y_inner_pred.detach().numpy()))\n",
    "        \n",
    "        \n",
    "                \n",
    "    \n",
    "   \n",
    "            curr_error = np.mean(cval_error)\n",
    "            if curr_error < max_error:\n",
    "                max_error = curr_error\n",
    "                best_param = a\n",
    "                print (\"Current best\", best_param)\n",
    "        \n",
    "        #compute test score based on best parameter\n",
    "        best_model = CodaRegress.CoDA_Regress(features.shape[1], n_components, [10,], [2,])\n",
    "        best_model.fit(torch.FloatTensor(X_train),  torch.FloatTensor(Y_train), a, lr=1e-2)\n",
    "\n",
    "        y_pred = best_model.predict(torch.FloatTensor(X_test))\n",
    "        kfold_scores.append(sklearn.metrics.mean_squared_error(Y_test,y_pred.detach().numpy()))\n",
    "                \n",
    "    return kfold_scores\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Aitchinson/Data 21. Permeabilities of bayesite for 21 mixtures of fibres and bonding pressures..csv...\n",
      "21 samples 6 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  3.9608\n",
      "epoch 10000, loss 1.352860927581787\n",
      "epoch 10000, loss 0.37952518463134766\n",
      "epoch 10000, loss 1.2401155233383179\n",
      "Current best 0.01\n",
      "epoch 10000, loss 1.9284913539886475\n",
      "epoch 10000, loss 1.8880974054336548\n",
      "epoch 10000, loss 2.592592716217041\n",
      "Current best 0.05\n",
      "epoch 10000, loss 4.752969741821289\n",
      "epoch 10000, loss 3.771939754486084\n",
      "epoch 10000, loss 4.553487300872803\n",
      "Current best 0.1\n",
      "epoch 10000, loss 19.803091049194336\n",
      "epoch 10000, loss 18.811616897583008\n",
      "epoch 10000, loss 18.56463623046875\n",
      "Current best 0.5\n",
      "epoch 10000, loss 38.04826736450195\n",
      "epoch 10000, loss 37.60554885864258\n",
      "epoch 10000, loss 37.873409271240234\n",
      "Current best 1.0\n",
      "epoch 10000, loss 186.2340087890625\n",
      "epoch 10000, loss 186.15283203125\n",
      "epoch 10000, loss 183.7068634033203\n",
      "Current best 5.0\n",
      "epoch 10000, loss 370.5549011230469\n",
      "epoch 10000, loss 369.43719482421875\n",
      "epoch 10000, loss 362.72039794921875\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1844.07421875\n",
      "epoch 10000, loss 1834.2908935546875\n",
      "epoch 10000, loss 1793.96435546875\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3685.931884765625\n",
      "epoch 10000, loss 3665.3095703125\n",
      "epoch 10000, loss 3582.9736328125\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5478.85546875\n",
      "epoch 10000, loss 0.3840675354003906\n",
      "epoch 10000, loss 1.2474944591522217\n",
      "epoch 10000, loss 0.40263229608535767\n",
      "Current best 0.01\n",
      "epoch 10000, loss 1.9063271284103394\n",
      "epoch 10000, loss 2.035158395767212\n",
      "epoch 10000, loss 1.9555339813232422\n",
      "Current best 0.05\n",
      "epoch 10000, loss 3.830274820327759\n",
      "epoch 10000, loss 4.871918201446533\n",
      "epoch 10000, loss 4.219933986663818\n",
      "Current best 0.1\n",
      "epoch 10000, loss 18.988637924194336\n",
      "epoch 10000, loss 20.825178146362305\n",
      "epoch 10000, loss 19.504093170166016\n",
      "Current best 0.5\n",
      "epoch 10000, loss 38.047462463378906\n",
      "epoch 10000, loss 41.169944763183594\n",
      "epoch 10000, loss 39.19077682495117\n",
      "Current best 1.0\n",
      "epoch 10000, loss 186.34161376953125\n",
      "epoch 10000, loss 195.3358154296875\n",
      "epoch 10000, loss 189.16908264160156\n",
      "Current best 5.0\n",
      "epoch 10000, loss 370.58343505859375\n",
      "epoch 10000, loss 387.3500671386719\n",
      "epoch 10000, loss 375.2057800292969\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1844.1983642578125\n",
      "epoch 10000, loss 1923.304931640625\n",
      "epoch 10000, loss 1862.48681640625\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3685.92724609375\n",
      "epoch 10000, loss 3836.1015625\n",
      "epoch 10000, loss 3723.71142578125\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5652.19921875\n",
      "epoch 10000, loss 1.261338710784912\n",
      "epoch 10000, loss 0.4087601602077484\n",
      "epoch 10000, loss 2.0439422130584717\n",
      "Current best 0.01\n",
      "epoch 10000, loss 1.8916494846343994\n",
      "epoch 10000, loss 2.8476219177246094\n",
      "epoch 10000, loss 3.6737139225006104\n",
      "Current best 0.05\n",
      "epoch 10000, loss 3.801236391067505\n",
      "epoch 10000, loss 4.847413539886475\n",
      "epoch 10000, loss 3.892793893814087\n",
      "Current best 0.1\n",
      "epoch 10000, loss 19.671525955200195\n",
      "epoch 10000, loss 21.362173080444336\n",
      "epoch 10000, loss 22.00953483581543\n",
      "Current best 0.5\n",
      "epoch 10000, loss 37.786346435546875\n",
      "epoch 10000, loss 40.151512145996094\n",
      "epoch 10000, loss 38.828453063964844\n",
      "Current best 1.0\n",
      "epoch 10000, loss 186.24652099609375\n",
      "epoch 10000, loss 195.14743041992188\n",
      "epoch 10000, loss 195.43605041503906\n",
      "Current best 5.0\n",
      "epoch 10000, loss 369.43792724609375\n",
      "epoch 10000, loss 386.8000793457031\n",
      "epoch 10000, loss 371.1041564941406\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1834.302734375\n",
      "epoch 10000, loss 1923.294189453125\n",
      "epoch 10000, loss 1834.1378173828125\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3665.31103515625\n",
      "epoch 10000, loss 3843.244384765625\n",
      "epoch 10000, loss 3663.450927734375\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5610.1728515625\n",
      "epoch 10000, loss 1.2345126867294312\n",
      "epoch 10000, loss 0.402224063873291\n",
      "epoch 10000, loss 2.359293222427368\n",
      "Current best 0.01\n",
      "epoch 10000, loss 1.859415054321289\n",
      "epoch 10000, loss 3.0856122970581055\n",
      "epoch 10000, loss 3.6717963218688965\n",
      "Current best 0.05\n",
      "epoch 10000, loss 3.744534969329834\n",
      "epoch 10000, loss 5.080297470092773\n",
      "epoch 10000, loss 5.711026668548584\n",
      "Current best 0.1\n",
      "epoch 10000, loss 19.37657928466797\n",
      "epoch 10000, loss 21.059677124023438\n",
      "epoch 10000, loss 22.009510040283203\n",
      "Current best 0.5\n",
      "epoch 10000, loss 37.87422180175781\n",
      "epoch 10000, loss 40.3397331237793\n",
      "epoch 10000, loss 40.79433059692383\n",
      "Current best 1.0\n",
      "epoch 10000, loss 183.7069854736328\n",
      "epoch 10000, loss 189.03335571289062\n",
      "epoch 10000, loss 188.2325897216797\n",
      "Current best 5.0\n",
      "epoch 10000, loss 362.7228698730469\n",
      "epoch 10000, loss 375.2061462402344\n",
      "epoch 10000, loss 371.1000671386719\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1793.9647216796875\n",
      "epoch 10000, loss 1863.4346923828125\n",
      "epoch 10000, loss 1834.0689697265625\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3582.454833984375\n",
      "epoch 10000, loss 3720.3125\n",
      "epoch 10000, loss 3661.74072265625\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5499.037109375\n",
      "CoDA-PCA:\n",
      "[4.499228, 7.297087, 5.0944376, 3.6930287]\n",
      "CLR-PCA:\n",
      "[4.5172668, 7.288266, 5.090907, 3.68751]\n",
      "Naive regression:\n",
      "[4.5017495, 7.2919145, 5.0983486, 3.6929336]\n",
      "CoDA-Regress:\n",
      "[5.5446553, 7.2394257, 4.916358, 3.6606908]\n",
      "loading Aitchinson/Data 21. Permeabilities of bayesite for 21 mixtures of fibres and bonding pressures..csv...\n",
      "21 samples 6 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  4.0866\n",
      "epoch 10000, loss 7067.6845703125\n",
      "epoch 10000, loss 362.46441650390625\n",
      "epoch 10000, loss 6479.783203125\n",
      "Current best 0.01\n",
      "epoch 10000, loss 7176.1201171875\n",
      "epoch 10000, loss 364.0069580078125\n",
      "epoch 10000, loss 649.7941284179688\n",
      "Current best 0.05\n",
      "epoch 10000, loss 622.291259765625\n",
      "epoch 10000, loss 365.79376220703125\n",
      "epoch 10000, loss 651.603515625\n",
      "Current best 0.1\n",
      "epoch 10000, loss 7091.0791015625\n",
      "epoch 10000, loss 380.7564392089844\n",
      "epoch 10000, loss 18.984161376953125\n",
      "Current best 0.5\n",
      "epoch 10000, loss 655.5881958007812\n",
      "epoch 10000, loss 6558.16943359375\n",
      "epoch 10000, loss 6478.3876953125\n",
      "Current best 1.0\n",
      "epoch 10000, loss 812.974365234375\n",
      "epoch 10000, loss 554.6549072265625\n",
      "epoch 10000, loss 6658.5546875\n",
      "Current best 5.0\n",
      "epoch 10000, loss 7439.43212890625\n",
      "epoch 10000, loss 747.2801513671875\n",
      "epoch 10000, loss 1015.8591918945312\n",
      "Current best 10.0\n",
      "epoch 10000, loss 2563.59912109375\n",
      "epoch 10000, loss 8535.6708984375\n",
      "epoch 10000, loss 8525.5361328125\n",
      "Current best 50.0\n",
      "epoch 10000, loss 11089.490234375\n",
      "epoch 10000, loss 4100.51025390625\n",
      "epoch 10000, loss 3719.22412109375\n",
      "Current best 100.0\n",
      "epoch 10000, loss 6610.59423828125\n",
      "epoch 10000, loss 618.7887573242188\n",
      "epoch 10000, loss 8740.8017578125\n",
      "epoch 10000, loss 454.7671203613281\n",
      "Current best 0.01\n",
      "epoch 10000, loss 7143.7197265625\n",
      "epoch 10000, loss 153.1829071044922\n",
      "epoch 10000, loss 456.3908386230469\n",
      "Current best 0.05\n",
      "epoch 10000, loss 622.2938842773438\n",
      "epoch 10000, loss 4.050279140472412\n",
      "epoch 10000, loss 458.41986083984375\n",
      "Current best 0.1\n",
      "epoch 10000, loss 637.8574829101562\n",
      "epoch 10000, loss 8810.9580078125\n",
      "epoch 10000, loss 474.65509033203125\n",
      "Current best 0.5\n",
      "epoch 10000, loss 657.3192138671875\n",
      "epoch 10000, loss 801.9896240234375\n",
      "epoch 10000, loss 494.9517517089844\n",
      "Current best 1.0\n",
      "epoch 10000, loss 812.984375\n",
      "epoch 10000, loss 199.09828186035156\n",
      "epoch 10000, loss 657.3341064453125\n",
      "Current best 5.0\n",
      "epoch 10000, loss 7418.5751953125\n",
      "epoch 10000, loss 1153.7235107421875\n",
      "epoch 10000, loss 860.126953125\n",
      "Current best 10.0\n",
      "epoch 10000, loss 2476.8310546875\n",
      "epoch 10000, loss 10898.5830078125\n",
      "epoch 10000, loss 10701.7421875\n",
      "Current best 50.0\n",
      "epoch 10000, loss 4507.83349609375\n",
      "epoch 10000, loss 4879.15087890625\n",
      "epoch 10000, loss 12652.76953125\n",
      "Current best 100.0\n",
      "epoch 10000, loss 14021.15234375\n",
      "epoch 10000, loss 362.454345703125\n",
      "epoch 10000, loss 8684.3369140625\n",
      "epoch 10000, loss 0.39980632066726685\n",
      "Current best 0.01\n",
      "epoch 10000, loss 363.9149169921875\n",
      "epoch 10000, loss 148.30230712890625\n",
      "epoch 10000, loss 8287.095703125\n",
      "Current best 0.05\n",
      "epoch 10000, loss 365.78277587890625\n",
      "epoch 10000, loss 1777.670654296875\n",
      "epoch 10000, loss 3.9960334300994873\n",
      "Current best 0.1\n",
      "epoch 10000, loss 382.00579833984375\n",
      "epoch 10000, loss 8756.7314453125\n",
      "epoch 10000, loss 8253.75390625\n",
      "Current best 0.5\n",
      "epoch 10000, loss 6547.724609375\n",
      "epoch 10000, loss 46.71015548706055\n",
      "epoch 10000, loss 1219.3138427734375\n",
      "Current best 1.0\n",
      "epoch 10000, loss 554.9352416992188\n",
      "epoch 10000, loss 8968.158203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10000, loss 200.0272216796875\n",
      "Current best 5.0\n",
      "epoch 10000, loss 6913.39599609375\n",
      "epoch 10000, loss 1153.532958984375\n",
      "epoch 10000, loss 8619.7841796875\n",
      "Current best 10.0\n",
      "epoch 10000, loss 2287.504638671875\n",
      "epoch 10000, loss 2715.96875\n",
      "epoch 10000, loss 3067.94189453125\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3825.27685546875\n",
      "epoch 10000, loss 4667.95068359375\n",
      "epoch 10000, loss 4949.87109375\n",
      "Current best 100.0\n",
      "epoch 10000, loss 7283.83447265625\n",
      "epoch 10000, loss 648.259521484375\n",
      "epoch 10000, loss 8538.2412109375\n",
      "epoch 10000, loss 0.39940282702445984\n",
      "Current best 0.01\n",
      "epoch 10000, loss 649.7942504882812\n",
      "epoch 10000, loss 456.390625\n",
      "epoch 10000, loss 785.1847534179688\n",
      "Current best 0.05\n",
      "epoch 10000, loss 651.7039794921875\n",
      "epoch 10000, loss 8550.830078125\n",
      "epoch 10000, loss 8203.2138671875\n",
      "Current best 0.1\n",
      "epoch 10000, loss 666.9908447265625\n",
      "epoch 10000, loss 474.6551513671875\n",
      "epoch 10000, loss 20.135704040527344\n",
      "Current best 0.5\n",
      "epoch 10000, loss 686.088134765625\n",
      "epoch 10000, loss 492.33599853515625\n",
      "epoch 10000, loss 39.851924896240234\n",
      "Current best 1.0\n",
      "epoch 10000, loss 186.40032958984375\n",
      "epoch 10000, loss 8743.263671875\n",
      "epoch 10000, loss 8413.8193359375\n",
      "Current best 5.0\n",
      "epoch 10000, loss 6923.73193359375\n",
      "epoch 10000, loss 9067.5576171875\n",
      "epoch 10000, loss 398.45794677734375\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1859.99853515625\n",
      "epoch 10000, loss 2480.988037109375\n",
      "epoch 10000, loss 1964.8084716796875\n",
      "Current best 50.0\n",
      "epoch 10000, loss 4464.35498046875\n",
      "epoch 10000, loss 12583.693359375\n",
      "epoch 10000, loss 4950.62060546875\n",
      "Current best 100.0\n",
      "epoch 10000, loss 7399.7802734375\n",
      "CoDA-PCA:\n",
      "[3105.9744, 4449.598, 1132.5059, 4799.8438]\n",
      "CLR-PCA:\n",
      "[3057.426, 4381.585, 1112.0743, 4753.583]\n",
      "Naive regression:\n",
      "[3150.7986, 4493.2646, 1137.2876, 4845.043]\n",
      "CoDA-Regress:\n",
      "[5665.242, 5161.708, 3028.5737, 3657.039]\n",
      "loading Aitchinson/Data 34. Foraminiferal compositions at 30 different depths.csv...\n",
      "30 samples 5 features\n",
      "sparsity: 3.3333333333333335%\n",
      "[epoch     0] L=  4.7556\n",
      "epoch 10000, loss -65067096.0\n",
      "epoch 10000, loss -32769422.0\n",
      "epoch 10000, loss -32897164.0\n",
      "Current best 0.01\n",
      "epoch 10000, loss -325344992.0\n",
      "epoch 10000, loss -163856896.0\n",
      "epoch 10000, loss -164877376.0\n",
      "Current best 0.05\n",
      "epoch 10000, loss -657601088.0\n",
      "epoch 10000, loss -329721984.0\n",
      "epoch 10000, loss -328348704.0\n",
      "Current best 0.1\n",
      "epoch 10000, loss -3287967232.0\n",
      "epoch 10000, loss -1638457856.0\n",
      "epoch 10000, loss -1641897216.0\n",
      "Current best 0.5\n",
      "epoch 10000, loss -6565936640.0\n",
      "epoch 10000, loss -3297220352.0\n",
      "epoch 10000, loss -3297548544.0\n",
      "Current best 1.0\n",
      "epoch 10000, loss -32534614016.0\n",
      "epoch 10000, loss -16384762880.0\n",
      "epoch 10000, loss -16416673792.0\n",
      "Current best 5.0\n",
      "epoch 10000, loss -65733189632.0\n",
      "epoch 10000, loss -32769466368.0\n",
      "epoch 10000, loss -32834590720.0\n",
      "Current best 10.0\n",
      "epoch 10000, loss -328790573056.0\n",
      "epoch 10000, loss -163806380032.0\n",
      "epoch 10000, loss -164877156352.0\n",
      "Current best 50.0\n",
      "epoch 10000, loss -657575706624.0\n",
      "epoch 10000, loss -329720594432.0\n",
      "epoch 10000, loss -329754836992.0\n",
      "Current best 100.0\n",
      "epoch 10000, loss -657574068224.0\n",
      "epoch 10000, loss -65001940.0\n",
      "epoch 10000, loss -49168860.0\n",
      "epoch 10000, loss -49391924.0\n",
      "Current best 0.01\n",
      "epoch 10000, loss -325334432.0\n",
      "epoch 10000, loss -247487120.0\n",
      "epoch 10000, loss -246958624.0\n",
      "Current best 0.05\n",
      "epoch 10000, loss -657533632.0\n",
      "epoch 10000, loss -494953984.0\n",
      "epoch 10000, loss -493914560.0\n",
      "Current best 0.1\n",
      "epoch 10000, loss -3253719552.0\n",
      "epoch 10000, loss -2474714624.0\n",
      "epoch 10000, loss -2468935168.0\n",
      "Current best 0.5\n",
      "epoch 10000, loss -6575515136.0\n",
      "epoch 10000, loss -4949033472.0\n",
      "epoch 10000, loss -4944487936.0\n",
      "Current best 1.0\n",
      "epoch 10000, loss -32880478208.0\n",
      "epoch 10000, loss -24749019136.0\n",
      "epoch 10000, loss -24694454272.0\n",
      "Current best 5.0\n",
      "epoch 10000, loss -65758568448.0\n",
      "epoch 10000, loss -49497817088.0\n",
      "epoch 10000, loss -49398697984.0\n",
      "Current best 10.0\n",
      "epoch 10000, loss -328785526784.0\n",
      "epoch 10000, loss -247477043200.0\n",
      "epoch 10000, loss -247915773952.0\n",
      "Current best 50.0\n",
      "epoch 10000, loss -657519280128.0\n",
      "epoch 10000, loss -494947598336.0\n",
      "epoch 10000, loss -493905444864.0\n",
      "Current best 100.0\n",
      "epoch 10000, loss -823925014528.0\n",
      "epoch 10000, loss -32767442.0\n",
      "epoch 10000, loss -49488956.0\n",
      "epoch 10000, loss -16636487.0\n",
      "Current best 0.01\n",
      "epoch 10000, loss -164278880.0\n",
      "epoch 10000, loss -245949072.0\n",
      "epoch 10000, loss -83182480.0\n",
      "Current best 0.05\n",
      "epoch 10000, loss -329720416.0\n",
      "epoch 10000, loss -494942464.0\n",
      "epoch 10000, loss -166362880.0\n",
      "Current best 0.1\n",
      "epoch 10000, loss -1648610560.0\n",
      "epoch 10000, loss -2463046144.0\n",
      "epoch 10000, loss -831804224.0\n",
      "Current best 0.5\n",
      "epoch 10000, loss -3297016064.0\n",
      "epoch 10000, loss -4949594624.0\n",
      "epoch 10000, loss -1663649024.0\n",
      "Current best 1.0\n",
      "epoch 10000, loss -16485969920.0\n",
      "epoch 10000, loss -24746928128.0\n",
      "epoch 10000, loss -8316520960.0\n",
      "Current best 5.0\n",
      "epoch 10000, loss -32771143680.0\n",
      "epoch 10000, loss -49497120768.0\n",
      "epoch 10000, loss -16636497920.0\n",
      "Current best 10.0\n",
      "epoch 10000, loss -163846946816.0\n",
      "epoch 10000, loss -245951070208.0\n",
      "epoch 10000, loss -83182460928.0\n",
      "Current best 50.0\n",
      "epoch 10000, loss -329713188864.0\n",
      "epoch 10000, loss -494939439104.0\n",
      "epoch 10000, loss -166364971008.0\n",
      "Current best 100.0\n",
      "epoch 10000, loss -494937505792.0\n",
      "epoch 10000, loss -32834574.0\n",
      "epoch 10000, loss -49588888.0\n",
      "epoch 10000, loss -16636480.0\n",
      "Current best 0.01\n",
      "epoch 10000, loss -164331488.0\n",
      "epoch 10000, loss -247945152.0\n",
      "epoch 10000, loss -83182464.0\n",
      "Current best 0.05\n",
      "epoch 10000, loss -328376064.0\n",
      "epoch 10000, loss -493910976.0\n",
      "epoch 10000, loss -166364752.0\n",
      "Current best 0.1\n",
      "epoch 10000, loss -1648773376.0\n",
      "epoch 10000, loss -2479475968.0\n",
      "epoch 10000, loss -831824832.0\n",
      "Current best 0.5\n",
      "epoch 10000, loss -3297439488.0\n",
      "epoch 10000, loss -4958955520.0\n",
      "epoch 10000, loss -1663649920.0\n",
      "Current best 1.0\n",
      "epoch 10000, loss -16487385088.0\n",
      "epoch 10000, loss -24695087104.0\n",
      "epoch 10000, loss -8318249472.0\n",
      "Current best 5.0\n",
      "epoch 10000, loss -32975478784.0\n",
      "epoch 10000, loss -49390178304.0\n",
      "epoch 10000, loss -16636498944.0\n",
      "Current best 10.0\n",
      "epoch 10000, loss -164861411328.0\n",
      "epoch 10000, loss -246922756096.0\n",
      "epoch 10000, loss -83181944832.0\n",
      "Current best 50.0\n",
      "epoch 10000, loss -329735962624.0\n",
      "epoch 10000, loss -493901774848.0\n",
      "epoch 10000, loss -166364987392.0\n",
      "Current best 100.0\n",
      "epoch 10000, loss -493810843648.0\n",
      "CoDA-PCA:\n",
      "[227.94249, 21.463764, 40.15769, 228.2201]\n",
      "CLR-PCA:\n",
      "[212.0833, 17.84351, 73.573265, 246.33154]\n",
      "Naive regression:\n",
      "[230.17953, 21.89151, 38.426308, 228.93114]\n",
      "CoDA-Regress:\n",
      "[225.00827, 14.864202, 40.417645, 237.84587]\n",
      "loading Aitchinson/Data 3. Compositions and depths of 25 specimens of boxite (Percentages by weight).csv...\n",
      "25 samples 6 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  6.9113\n",
      "epoch 10000, loss 2.28676176071167\n",
      "epoch 10000, loss 0.542072057723999\n",
      "epoch 10000, loss 0.4573970139026642\n",
      "Current best 0.01\n",
      "epoch 10000, loss 4.09000825881958\n",
      "epoch 10000, loss 2.3838958740234375\n",
      "epoch 10000, loss 2.279862403869629\n",
      "Current best 0.05\n",
      "epoch 10000, loss 9.288627624511719\n",
      "epoch 10000, loss 4.587212562561035\n",
      "epoch 10000, loss 4.54904842376709\n",
      "Current best 0.1\n",
      "epoch 10000, loss 23.806333541870117\n",
      "epoch 10000, loss 29.834104537963867\n",
      "epoch 10000, loss 23.434709548950195\n",
      "Current best 0.5\n",
      "epoch 10000, loss 48.364192962646484\n",
      "epoch 10000, loss 52.6579704284668\n",
      "epoch 10000, loss 45.40392303466797\n",
      "Current best 1.0\n",
      "epoch 10000, loss 233.6151123046875\n",
      "epoch 10000, loss 228.22731018066406\n",
      "epoch 10000, loss 227.28517150878906\n",
      "Current best 5.0\n",
      "epoch 10000, loss 455.8959045410156\n",
      "epoch 10000, loss 457.9879455566406\n",
      "epoch 10000, loss 453.9602966308594\n",
      "Current best 10.0\n",
      "epoch 10000, loss 2276.16455078125\n",
      "epoch 10000, loss 2282.613037109375\n",
      "epoch 10000, loss 2269.943359375\n",
      "Current best 50.0\n",
      "epoch 10000, loss 4546.74853515625\n",
      "epoch 10000, loss 4568.42724609375\n",
      "epoch 10000, loss 4539.748046875\n",
      "Current best 100.0\n",
      "epoch 10000, loss 6843.8662109375\n",
      "epoch 10000, loss 2.284710168838501\n",
      "epoch 10000, loss 24.22614097595215\n",
      "epoch 10000, loss 3.436018466949463\n",
      "Current best 0.01\n",
      "epoch 10000, loss 3.1835641860961914\n",
      "epoch 10000, loss 2.488492488861084\n",
      "epoch 10000, loss 18.213157653808594\n",
      "Current best 0.05\n",
      "epoch 10000, loss 9.602516174316406\n",
      "epoch 10000, loss 4.977653503417969\n",
      "epoch 10000, loss 6.8009934425354\n",
      "Current best 0.1\n",
      "epoch 10000, loss 24.549163818359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10000, loss 24.922040939331055\n",
      "epoch 10000, loss 25.27458953857422\n",
      "Current best 0.5\n",
      "epoch 10000, loss 49.27713394165039\n",
      "epoch 10000, loss 60.38109588623047\n",
      "epoch 10000, loss 61.37883758544922\n",
      "Current best 1.0\n",
      "epoch 10000, loss 232.16317749023438\n",
      "epoch 10000, loss 248.79904174804688\n",
      "epoch 10000, loss 248.7668914794922\n",
      "Current best 5.0\n",
      "epoch 10000, loss 457.16754150390625\n",
      "epoch 10000, loss 498.136474609375\n",
      "epoch 10000, loss 495.71728515625\n",
      "Current best 10.0\n",
      "epoch 10000, loss 2276.17919921875\n",
      "epoch 10000, loss 2487.893798828125\n",
      "epoch 10000, loss 2476.24462890625\n",
      "Current best 50.0\n",
      "epoch 10000, loss 4546.40576171875\n",
      "epoch 10000, loss 4975.26953125\n",
      "epoch 10000, loss 4952.052734375\n",
      "Current best 100.0\n",
      "epoch 10000, loss 7252.83349609375\n",
      "epoch 10000, loss 0.46492159366607666\n",
      "epoch 10000, loss 0.49765533208847046\n",
      "epoch 10000, loss 0.5101235508918762\n",
      "Current best 0.01\n",
      "epoch 10000, loss 9.314949989318848\n",
      "epoch 10000, loss 2.5132832527160645\n",
      "epoch 10000, loss 2.5169312953948975\n",
      "Current best 0.05\n",
      "epoch 10000, loss 4.585757255554199\n",
      "epoch 10000, loss 5.115309715270996\n",
      "epoch 10000, loss 5.24139928817749\n",
      "Current best 0.1\n",
      "epoch 10000, loss 23.003320693969727\n",
      "epoch 10000, loss 35.34405517578125\n",
      "epoch 10000, loss 25.772314071655273\n",
      "Current best 0.5\n",
      "epoch 10000, loss 46.182952880859375\n",
      "epoch 10000, loss 50.091800689697266\n",
      "epoch 10000, loss 50.1374626159668\n",
      "Current best 1.0\n",
      "epoch 10000, loss 228.37469482421875\n",
      "epoch 10000, loss 248.79022216796875\n",
      "epoch 10000, loss 249.78448486328125\n",
      "Current best 5.0\n",
      "epoch 10000, loss 456.893310546875\n",
      "epoch 10000, loss 497.665771484375\n",
      "epoch 10000, loss 504.0295104980469\n",
      "Current best 10.0\n",
      "epoch 10000, loss 2283.705810546875\n",
      "epoch 10000, loss 2486.0966796875\n",
      "epoch 10000, loss 2483.22900390625\n",
      "Current best 50.0\n",
      "epoch 10000, loss 4564.35986328125\n",
      "epoch 10000, loss 4972.2041015625\n",
      "epoch 10000, loss 4969.32763671875\n",
      "Current best 100.0\n",
      "epoch 10000, loss 7287.2646484375\n",
      "epoch 10000, loss 0.47697877883911133\n",
      "epoch 10000, loss 3.038524866104126\n",
      "epoch 10000, loss 0.7776671648025513\n",
      "Current best 0.01\n",
      "epoch 10000, loss 2.2991065979003906\n",
      "epoch 10000, loss 5.343490123748779\n",
      "epoch 10000, loss 2.547719717025757\n",
      "Current best 0.05\n",
      "epoch 10000, loss 4.5486979484558105\n",
      "epoch 10000, loss 20.741853713989258\n",
      "epoch 10000, loss 5.124039173126221\n",
      "Current best 0.1\n",
      "epoch 10000, loss 22.713640213012695\n",
      "epoch 10000, loss 27.33156394958496\n",
      "epoch 10000, loss 25.176889419555664\n",
      "Current best 0.5\n",
      "epoch 10000, loss 45.41350173950195\n",
      "epoch 10000, loss 52.7229118347168\n",
      "epoch 10000, loss 57.345619201660156\n",
      "Current best 1.0\n",
      "epoch 10000, loss 227.3120880126953\n",
      "epoch 10000, loss 250.4198455810547\n",
      "epoch 10000, loss 249.4016876220703\n",
      "Current best 5.0\n",
      "epoch 10000, loss 454.64208984375\n",
      "epoch 10000, loss 504.9867248535156\n",
      "epoch 10000, loss 497.2059631347656\n",
      "Current best 10.0\n",
      "epoch 10000, loss 2270.239013671875\n",
      "epoch 10000, loss 2477.856201171875\n",
      "epoch 10000, loss 2486.658935546875\n",
      "Current best 50.0\n",
      "epoch 10000, loss 4539.16455078125\n",
      "epoch 10000, loss 4952.28564453125\n",
      "epoch 10000, loss 4968.27783203125\n",
      "Current best 100.0\n",
      "epoch 10000, loss 7242.333984375\n",
      "CoDA-PCA:\n",
      "[160.2325, 13.7298975, 24.099098, 159.17197]\n",
      "CLR-PCA:\n",
      "[160.22293, 13.775093, 24.047392, 159.17717]\n",
      "Naive regression:\n",
      "[160.25475, 13.741725, 24.122057, 159.16808]\n",
      "CoDA-Regress:\n",
      "[153.22243, 856.26074, 56.61232, 97.793945]\n",
      "loading Aitchinson/Data 39. Microhardness of 18 glass specimens and their (Ge, Sb, Se) compositions.csv...\n",
      "18 samples 4 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  3.6667\n",
      "epoch 10000, loss 0.16802260279655457\n",
      "epoch 10000, loss 0.1865137815475464\n",
      "epoch 10000, loss 0.16293950378894806\n",
      "Current best 0.01\n",
      "epoch 10000, loss 0.8324667811393738\n",
      "epoch 10000, loss 0.9289042353630066\n",
      "epoch 10000, loss 0.806564450263977\n",
      "Current best 0.05\n",
      "epoch 10000, loss 1.6608936786651611\n",
      "epoch 10000, loss 1.8546541929244995\n",
      "epoch 10000, loss 1.6101508140563965\n",
      "Current best 0.1\n",
      "epoch 10000, loss 8.273880958557129\n",
      "epoch 10000, loss 9.250701904296875\n",
      "epoch 10000, loss 8.03848648071289\n",
      "Current best 0.5\n",
      "epoch 10000, loss 16.542863845825195\n",
      "epoch 10000, loss 18.492399215698242\n",
      "epoch 10000, loss 16.073457717895508\n",
      "Current best 1.0\n",
      "epoch 10000, loss 82.68541717529297\n",
      "epoch 10000, loss 92.44131469726562\n",
      "epoch 10000, loss 80.35493469238281\n",
      "Current best 5.0\n",
      "epoch 10000, loss 165.39920043945312\n",
      "epoch 10000, loss 184.8535919189453\n",
      "epoch 10000, loss 160.70455932617188\n",
      "Current best 10.0\n",
      "epoch 10000, loss 826.7412719726562\n",
      "epoch 10000, loss 924.1151733398438\n",
      "epoch 10000, loss 803.733154296875\n",
      "Current best 50.0\n",
      "epoch 10000, loss 1653.4422607421875\n",
      "epoch 10000, loss 1848.64599609375\n",
      "epoch 10000, loss 1607.1380615234375\n",
      "Current best 100.0\n",
      "epoch 10000, loss 2567.03125\n",
      "epoch 10000, loss 0.16810745000839233\n",
      "epoch 10000, loss 0.14798355102539062\n",
      "epoch 10000, loss 0.1509505659341812\n",
      "Current best 0.01\n",
      "epoch 10000, loss 0.8325073719024658\n",
      "epoch 10000, loss 0.7338128685951233\n",
      "epoch 10000, loss 0.6222193241119385\n",
      "Current best 0.05\n",
      "epoch 10000, loss 1.662936806678772\n",
      "epoch 10000, loss 1.4662904739379883\n",
      "epoch 10000, loss 1.2427161931991577\n",
      "Current best 0.1\n",
      "epoch 10000, loss 8.273813247680664\n",
      "epoch 10000, loss 7.3200225830078125\n",
      "epoch 10000, loss 6.209094047546387\n",
      "Current best 0.5\n",
      "epoch 10000, loss 16.54262351989746\n",
      "epoch 10000, loss 14.634012222290039\n",
      "epoch 10000, loss 12.414294242858887\n",
      "Current best 1.0\n",
      "epoch 10000, loss 82.68000793457031\n",
      "epoch 10000, loss 73.16352844238281\n",
      "epoch 10000, loss 62.0611572265625\n",
      "Current best 5.0\n",
      "epoch 10000, loss 165.35073852539062\n",
      "epoch 10000, loss 146.3179168701172\n",
      "epoch 10000, loss 124.11610412597656\n",
      "Current best 10.0\n",
      "epoch 10000, loss 826.7621459960938\n",
      "epoch 10000, loss 731.5885009765625\n",
      "epoch 10000, loss 620.5753173828125\n",
      "Current best 50.0\n",
      "epoch 10000, loss 1653.47412109375\n",
      "epoch 10000, loss 1463.2962646484375\n",
      "epoch 10000, loss 1257.839111328125\n",
      "Current best 100.0\n",
      "epoch 10000, loss 2201.400146484375\n",
      "epoch 10000, loss 0.18649166822433472\n",
      "epoch 10000, loss 0.19402241706848145\n",
      "epoch 10000, loss 0.1451653391122818\n",
      "Current best 0.01\n",
      "epoch 10000, loss 0.930078387260437\n",
      "epoch 10000, loss 0.7340840101242065\n",
      "epoch 10000, loss 0.738061785697937\n",
      "Current best 0.05\n",
      "epoch 10000, loss 1.8545351028442383\n",
      "epoch 10000, loss 1.5042152404785156\n",
      "epoch 10000, loss 1.468061089515686\n",
      "Current best 0.1\n",
      "epoch 10000, loss 9.249982833862305\n",
      "epoch 10000, loss 7.319797039031982\n",
      "epoch 10000, loss 7.082352161407471\n",
      "Current best 0.5\n",
      "epoch 10000, loss 18.49375343322754\n",
      "epoch 10000, loss 14.636191368103027\n",
      "epoch 10000, loss 14.156566619873047\n",
      "Current best 1.0\n",
      "epoch 10000, loss 92.43879699707031\n",
      "epoch 10000, loss 73.16403198242188\n",
      "epoch 10000, loss 70.74801635742188\n",
      "Current best 5.0\n",
      "epoch 10000, loss 184.86849975585938\n",
      "epoch 10000, loss 146.32591247558594\n",
      "epoch 10000, loss 141.56248474121094\n",
      "Current best 10.0\n",
      "epoch 10000, loss 924.3108520507812\n",
      "epoch 10000, loss 731.5525512695312\n",
      "epoch 10000, loss 707.415283203125\n",
      "Current best 50.0\n",
      "epoch 10000, loss 1848.19873046875\n",
      "epoch 10000, loss 1463.0594482421875\n",
      "epoch 10000, loss 1414.774169921875\n",
      "Current best 100.0\n",
      "epoch 10000, loss 2376.319091796875\n",
      "epoch 10000, loss 0.1630060225725174\n",
      "epoch 10000, loss 0.12547820806503296\n",
      "epoch 10000, loss 0.14521843194961548\n",
      "Current best 0.01\n",
      "epoch 10000, loss 0.8064142465591431\n",
      "epoch 10000, loss 0.6836313605308533\n",
      "epoch 10000, loss 0.7165989875793457\n",
      "Current best 0.05\n",
      "epoch 10000, loss 1.6100932359695435\n",
      "epoch 10000, loss 1.2432421445846558\n",
      "epoch 10000, loss 1.4607821702957153\n",
      "Current best 0.1\n",
      "epoch 10000, loss 8.038204193115234\n",
      "epoch 10000, loss 6.208006858825684\n",
      "epoch 10000, loss 7.082496166229248\n",
      "Current best 0.5\n",
      "epoch 10000, loss 16.073320388793945\n",
      "epoch 10000, loss 12.649386405944824\n",
      "epoch 10000, loss 14.158063888549805\n",
      "Current best 1.0\n",
      "epoch 10000, loss 80.35525512695312\n",
      "epoch 10000, loss 62.06772994995117\n",
      "epoch 10000, loss 70.74889373779297\n",
      "Current best 5.0\n",
      "epoch 10000, loss 160.70509338378906\n",
      "epoch 10000, loss 124.11636352539062\n",
      "epoch 10000, loss 141.48658752441406\n",
      "Current best 10.0\n",
      "epoch 10000, loss 803.5368041992188\n",
      "epoch 10000, loss 620.583740234375\n",
      "epoch 10000, loss 707.3786010742188\n",
      "Current best 50.0\n",
      "epoch 10000, loss 1607.007080078125\n",
      "epoch 10000, loss 1241.1773681640625\n",
      "epoch 10000, loss 1414.75390625\n",
      "Current best 100.0\n",
      "epoch 10000, loss 2154.69921875\n",
      "CoDA-PCA:\n",
      "[0.062411197, 0.0039685713, 0.011311675, 0.057702586]\n",
      "CLR-PCA:\n",
      "[0.060506113, 0.0038533728, 0.0099628195, 0.05535987]\n",
      "Naive regression:\n",
      "[0.06243059, 0.0039659482, 0.011303257, 0.05774308]\n",
      "CoDA-Regress:\n",
      "[0.04667331, 0.0059578186, 0.0030933088, 0.0060907025]\n",
      "loading Aitchinson/Data 4. Compositions, depths and porosities of 25 specimens of coxite (Percentages by weight).csv...\n",
      "25 samples 7 features\n",
      "sparsity: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch     0] L=  4.2066\n",
      "epoch 10000, loss 2.852203607559204\n",
      "epoch 10000, loss 3.242548704147339\n",
      "epoch 10000, loss 0.9063756465911865\n",
      "Current best 0.01\n",
      "epoch 10000, loss 13.60297966003418\n",
      "epoch 10000, loss 13.733473777770996\n",
      "epoch 10000, loss 2.5717506408691406\n",
      "Current best 0.05\n",
      "epoch 10000, loss 5.784623146057129\n",
      "epoch 10000, loss 5.241025924682617\n",
      "epoch 10000, loss 3.6351211071014404\n",
      "Current best 0.1\n",
      "epoch 10000, loss 19.249937057495117\n",
      "epoch 10000, loss 18.983810424804688\n",
      "epoch 10000, loss 20.87687110900879\n",
      "Current best 0.5\n",
      "epoch 10000, loss 42.73836135864258\n",
      "epoch 10000, loss 36.243873596191406\n",
      "epoch 10000, loss 35.46378707885742\n",
      "Current best 1.0\n",
      "epoch 10000, loss 172.97796630859375\n",
      "epoch 10000, loss 170.5129852294922\n",
      "epoch 10000, loss 174.11720275878906\n",
      "Current best 5.0\n",
      "epoch 10000, loss 335.3902587890625\n",
      "epoch 10000, loss 354.0248107910156\n",
      "epoch 10000, loss 351.64263916015625\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1683.796875\n",
      "epoch 10000, loss 1703.63916015625\n",
      "epoch 10000, loss 1743.1007080078125\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3357.76611328125\n",
      "epoch 10000, loss 3412.607177734375\n",
      "epoch 10000, loss 3489.22265625\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5118.2998046875\n",
      "epoch 10000, loss 9.480445861816406\n",
      "epoch 10000, loss 15.94005298614502\n",
      "epoch 10000, loss 2.065835475921631\n",
      "Current best 0.01\n",
      "epoch 10000, loss 4.219571113586426\n",
      "epoch 10000, loss 3.755772590637207\n",
      "epoch 10000, loss 7.51300048828125\n",
      "Current best 0.05\n",
      "epoch 10000, loss 5.8568644523620605\n",
      "epoch 10000, loss 19.51929473876953\n",
      "epoch 10000, loss 5.254184722900391\n",
      "Current best 0.1\n",
      "epoch 10000, loss 25.836933135986328\n",
      "epoch 10000, loss 33.29662322998047\n",
      "epoch 10000, loss 23.45123291015625\n",
      "Current best 0.5\n",
      "epoch 10000, loss 35.9675407409668\n",
      "epoch 10000, loss 124.23768615722656\n",
      "epoch 10000, loss 41.11072540283203\n",
      "Current best 1.0\n",
      "epoch 10000, loss 172.20362854003906\n",
      "epoch 10000, loss 181.8173828125\n",
      "epoch 10000, loss 186.38140869140625\n",
      "Current best 5.0\n",
      "epoch 10000, loss 337.20343017578125\n",
      "epoch 10000, loss 358.4599609375\n",
      "epoch 10000, loss 368.1727600097656\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1684.4716796875\n",
      "epoch 10000, loss 1787.2830810546875\n",
      "epoch 10000, loss 1815.4093017578125\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3357.76123046875\n",
      "epoch 10000, loss 3546.428955078125\n",
      "epoch 10000, loss 3625.95947265625\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5269.9189453125\n",
      "epoch 10000, loss 2.22180438041687\n",
      "epoch 10000, loss 88.91303253173828\n",
      "epoch 10000, loss 4.054253578186035\n",
      "Current best 0.01\n",
      "epoch 10000, loss 3.5081610679626465\n",
      "epoch 10000, loss 17.517532348632812\n",
      "epoch 10000, loss 2.332218885421753\n",
      "Current best 0.05\n",
      "epoch 10000, loss 5.238405227661133\n",
      "epoch 10000, loss 7.0670552253723145\n",
      "epoch 10000, loss 3.9647257328033447\n",
      "Current best 0.1\n",
      "epoch 10000, loss 18.80721092224121\n",
      "epoch 10000, loss 106.3969497680664\n",
      "epoch 10000, loss 18.839101791381836\n",
      "Current best 0.5\n",
      "epoch 10000, loss 36.226600646972656\n",
      "epoch 10000, loss 36.08572769165039\n",
      "epoch 10000, loss 37.116188049316406\n",
      "Current best 1.0\n",
      "epoch 10000, loss 172.4344482421875\n",
      "epoch 10000, loss 177.81961059570312\n",
      "epoch 10000, loss 184.08499145507812\n",
      "Current best 5.0\n",
      "epoch 10000, loss 342.1527099609375\n",
      "epoch 10000, loss 445.37078857421875\n",
      "epoch 10000, loss 370.0308837890625\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1713.068603515625\n",
      "epoch 10000, loss 1778.0543212890625\n",
      "epoch 10000, loss 1839.5250244140625\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3403.332275390625\n",
      "epoch 10000, loss 3551.035888671875\n",
      "epoch 10000, loss 3675.96875\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5320.66015625\n",
      "epoch 10000, loss 0.4487784206867218\n",
      "epoch 10000, loss 2.503765344619751\n",
      "epoch 10000, loss 2.7967917919158936\n",
      "Current best 0.01\n",
      "epoch 10000, loss 2.3257052898406982\n",
      "epoch 10000, loss 7.559057712554932\n",
      "epoch 10000, loss 2.135016918182373\n",
      "Current best 0.05\n",
      "epoch 10000, loss 6.954609394073486\n",
      "epoch 10000, loss 4.49082612991333\n",
      "epoch 10000, loss 7.364074230194092\n",
      "Current best 0.1\n",
      "epoch 10000, loss 29.445465087890625\n",
      "epoch 10000, loss 23.015560150146484\n",
      "epoch 10000, loss 22.158327102661133\n",
      "Current best 0.5\n",
      "epoch 10000, loss 34.96648025512695\n",
      "epoch 10000, loss 37.92657470703125\n",
      "epoch 10000, loss 40.41384506225586\n",
      "Current best 1.0\n",
      "epoch 10000, loss 177.54843139648438\n",
      "epoch 10000, loss 187.05816650390625\n",
      "epoch 10000, loss 186.58592224121094\n",
      "Current best 5.0\n",
      "epoch 10000, loss 348.4061279296875\n",
      "epoch 10000, loss 367.3831481933594\n",
      "epoch 10000, loss 370.4497985839844\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1743.15576171875\n",
      "epoch 10000, loss 1816.428466796875\n",
      "epoch 10000, loss 1839.83935546875\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3485.98193359375\n",
      "epoch 10000, loss 3626.671875\n",
      "epoch 10000, loss 3682.252197265625\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5394.4990234375\n",
      "CoDA-PCA:\n",
      "[160.24573, 13.731499, 24.120987, 159.15999]\n",
      "CLR-PCA:\n",
      "[160.20894, 13.718182, 24.081589, 159.13301]\n",
      "Naive regression:\n",
      "[160.2496, 13.736397, 24.124407, 159.16609]\n",
      "CoDA-Regress:\n",
      "[59.5476, 13.831532, 15.891057, 59.581226]\n",
      "loading Aitchinson/Data 4. Compositions, depths and porosities of 25 specimens of coxite (Percentages by weight).csv...\n",
      "25 samples 7 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  6.4765\n",
      "epoch 10000, loss 0.44227334856987\n",
      "epoch 10000, loss 0.4940240979194641\n",
      "epoch 10000, loss 0.5925577878952026\n",
      "Current best 0.01\n",
      "epoch 10000, loss 1.7809454202651978\n",
      "epoch 10000, loss 1.8553615808486938\n",
      "epoch 10000, loss 1.9864624738693237\n",
      "Current best 0.05\n",
      "epoch 10000, loss 3.4586682319641113\n",
      "epoch 10000, loss 3.5935444831848145\n",
      "epoch 10000, loss 3.769587516784668\n",
      "Current best 0.1\n",
      "epoch 10000, loss 16.8519287109375\n",
      "epoch 10000, loss 17.14807891845703\n",
      "epoch 10000, loss 17.699220657348633\n",
      "Current best 0.5\n",
      "epoch 10000, loss 33.67713928222656\n",
      "epoch 10000, loss 34.162418365478516\n",
      "epoch 10000, loss 35.07406234741211\n",
      "Current best 1.0\n",
      "epoch 10000, loss 167.514892578125\n",
      "epoch 10000, loss 170.08413696289062\n",
      "epoch 10000, loss 174.34429931640625\n",
      "Current best 5.0\n",
      "epoch 10000, loss 334.80340576171875\n",
      "epoch 10000, loss 340.063232421875\n",
      "epoch 10000, loss 348.4514465332031\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1673.7119140625\n",
      "epoch 10000, loss 1699.65283203125\n",
      "epoch 10000, loss 1741.0804443359375\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3346.738037109375\n",
      "epoch 10000, loss 3399.1767578125\n",
      "epoch 10000, loss 3481.904541015625\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5113.9599609375\n",
      "epoch 10000, loss 0.49341949820518494\n",
      "epoch 10000, loss 0.6234166622161865\n",
      "epoch 10000, loss 0.6742730736732483\n",
      "Current best 0.01\n",
      "epoch 10000, loss 1.7882417440414429\n",
      "epoch 10000, loss 2.0453133583068848\n",
      "epoch 10000, loss 2.1222901344299316\n",
      "Current best 0.05\n",
      "epoch 10000, loss 3.4655210971832275\n",
      "epoch 10000, loss 3.8097894191741943\n",
      "epoch 10000, loss 3.9353363513946533\n",
      "Current best 0.1\n",
      "epoch 10000, loss 16.84339141845703\n",
      "epoch 10000, loss 17.972122192382812\n",
      "epoch 10000, loss 18.416990280151367\n",
      "Current best 0.5\n",
      "epoch 10000, loss 33.587711334228516\n",
      "epoch 10000, loss 35.67186737060547\n",
      "epoch 10000, loss 36.51932144165039\n",
      "Current best 1.0\n",
      "epoch 10000, loss 167.4431915283203\n",
      "epoch 10000, loss 177.2200927734375\n",
      "epoch 10000, loss 181.32284545898438\n",
      "Current best 5.0\n",
      "epoch 10000, loss 335.7551574707031\n",
      "epoch 10000, loss 354.0482482910156\n",
      "epoch 10000, loss 362.3611145019531\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1673.543701171875\n",
      "epoch 10000, loss 1769.2901611328125\n",
      "epoch 10000, loss 1810.3851318359375\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3346.7158203125\n",
      "epoch 10000, loss 3538.307373046875\n",
      "epoch 10000, loss 3620.577392578125\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5252.943359375\n",
      "epoch 10000, loss 0.4940917491912842\n",
      "epoch 10000, loss 0.6224808096885681\n",
      "epoch 10000, loss 0.6630069613456726\n",
      "Current best 0.01\n",
      "epoch 10000, loss 1.8584200143814087\n",
      "epoch 10000, loss 2.039025068283081\n",
      "epoch 10000, loss 2.138472318649292\n",
      "Current best 0.05\n",
      "epoch 10000, loss 3.5642895698547363\n",
      "epoch 10000, loss 3.817492961883545\n",
      "epoch 10000, loss 3.98244047164917\n",
      "Current best 0.1\n",
      "epoch 10000, loss 17.15465545654297\n",
      "epoch 10000, loss 17.868377685546875\n",
      "epoch 10000, loss 18.672895431518555\n",
      "Current best 0.5\n",
      "epoch 10000, loss 34.162235260009766\n",
      "epoch 10000, loss 35.762489318847656\n",
      "epoch 10000, loss 37.04166030883789\n",
      "Current best 1.0\n",
      "epoch 10000, loss 170.1863555908203\n",
      "epoch 10000, loss 177.16873168945312\n",
      "epoch 10000, loss 183.8438262939453\n",
      "Current best 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10000, loss 340.0734558105469\n",
      "epoch 10000, loss 354.24176025390625\n",
      "epoch 10000, loss 367.57366943359375\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1699.6719970703125\n",
      "epoch 10000, loss 1769.66943359375\n",
      "epoch 10000, loss 1836.65869140625\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3399.187255859375\n",
      "epoch 10000, loss 3538.567138671875\n",
      "epoch 10000, loss 3672.991455078125\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5305.3408203125\n",
      "epoch 10000, loss 0.5915667414665222\n",
      "epoch 10000, loss 0.6770172119140625\n",
      "epoch 10000, loss 0.6673576831817627\n",
      "Current best 0.01\n",
      "epoch 10000, loss 1.9878535270690918\n",
      "epoch 10000, loss 2.133329153060913\n",
      "epoch 10000, loss 2.13737154006958\n",
      "Current best 0.05\n",
      "epoch 10000, loss 3.7181525230407715\n",
      "epoch 10000, loss 3.948502540588379\n",
      "epoch 10000, loss 3.975196123123169\n",
      "Current best 0.1\n",
      "epoch 10000, loss 17.656967163085938\n",
      "epoch 10000, loss 18.481124877929688\n",
      "epoch 10000, loss 18.623790740966797\n",
      "Current best 0.5\n",
      "epoch 10000, loss 35.071346282958984\n",
      "epoch 10000, loss 36.518314361572266\n",
      "epoch 10000, loss 37.01380157470703\n",
      "Current best 1.0\n",
      "epoch 10000, loss 174.33749389648438\n",
      "epoch 10000, loss 181.35739135742188\n",
      "epoch 10000, loss 183.86439514160156\n",
      "Current best 5.0\n",
      "epoch 10000, loss 348.4259033203125\n",
      "epoch 10000, loss 362.3704833984375\n",
      "epoch 10000, loss 367.5735778808594\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1741.3594970703125\n",
      "epoch 10000, loss 1810.387939453125\n",
      "epoch 10000, loss 1836.6500244140625\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3481.90234375\n",
      "epoch 10000, loss 3620.67138671875\n",
      "epoch 10000, loss 3673.0966796875\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5387.96044921875\n",
      "CoDA-PCA:\n",
      "[4.875369, 5.687342, 4.392523, 6.428678]\n",
      "CLR-PCA:\n",
      "[4.8603997, 5.665487, 4.3765583, 6.4088416]\n",
      "Naive regression:\n",
      "[4.8753433, 5.687715, 4.3937287, 6.4285874]\n",
      "CoDA-Regress:\n",
      "[0.44727993, 0.36412492, 0.39563742, 0.26034594]\n",
      "loading Aitchinson/Data 5. Sand, silt, clay compositions of 39 sediment samples at different water depths in an Arctic lake.csv...\n",
      "39 samples 4 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  3.5026\n",
      "epoch 10000, loss 199.74716186523438\n",
      "epoch 10000, loss 239.5500946044922\n",
      "epoch 10000, loss 87.55422973632812\n",
      "Current best 0.01\n",
      "epoch 10000, loss 201.068359375\n",
      "epoch 10000, loss 240.79153442382812\n",
      "epoch 10000, loss 91.4853744506836\n",
      "Current best 0.05\n",
      "epoch 10000, loss 188.4266357421875\n",
      "epoch 10000, loss 242.32749938964844\n",
      "epoch 10000, loss 91.0691909790039\n",
      "Current best 0.1\n",
      "epoch 10000, loss 201.77239990234375\n",
      "epoch 10000, loss 263.78936767578125\n",
      "epoch 10000, loss 108.6859359741211\n",
      "Current best 0.5\n",
      "epoch 10000, loss 232.427001953125\n",
      "epoch 10000, loss 269.77587890625\n",
      "epoch 10000, loss 125.29179382324219\n",
      "Current best 1.0\n",
      "epoch 10000, loss 364.3310852050781\n",
      "epoch 10000, loss 391.3312072753906\n",
      "epoch 10000, loss 271.4293212890625\n",
      "Current best 5.0\n",
      "epoch 10000, loss 518.315673828125\n",
      "epoch 10000, loss 543.1349487304688\n",
      "epoch 10000, loss 452.6578063964844\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1842.296630859375\n",
      "epoch 10000, loss 1752.47705078125\n",
      "epoch 10000, loss 1884.42626953125\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3495.806396484375\n",
      "epoch 10000, loss 3830.294677734375\n",
      "epoch 10000, loss 3680.07568359375\n",
      "Current best 100.0\n",
      "epoch 10000, loss 5184.56884765625\n",
      "epoch 10000, loss 185.43020629882812\n",
      "epoch 10000, loss 73.03650665283203\n",
      "epoch 10000, loss 49.07872009277344\n",
      "Current best 0.01\n",
      "epoch 10000, loss 201.06617736816406\n",
      "epoch 10000, loss 67.1186294555664\n",
      "epoch 10000, loss 64.06058502197266\n",
      "Current best 0.05\n",
      "epoch 10000, loss 202.7186737060547\n",
      "epoch 10000, loss 75.5345458984375\n",
      "epoch 10000, loss 65.64981842041016\n",
      "Current best 0.1\n",
      "epoch 10000, loss 215.9042510986328\n",
      "epoch 10000, loss 86.61051940917969\n",
      "epoch 10000, loss 78.3633041381836\n",
      "Current best 0.5\n",
      "epoch 10000, loss 232.43357849121094\n",
      "epoch 10000, loss 94.75912475585938\n",
      "epoch 10000, loss 94.19564819335938\n",
      "Current best 1.0\n",
      "epoch 10000, loss 351.712158203125\n",
      "epoch 10000, loss 208.41317749023438\n",
      "epoch 10000, loss 212.41590881347656\n",
      "Current best 5.0\n",
      "epoch 10000, loss 529.2109375\n",
      "epoch 10000, loss 337.7970275878906\n",
      "epoch 10000, loss 516.0929565429688\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1848.2965087890625\n",
      "epoch 10000, loss 1326.2637939453125\n",
      "epoch 10000, loss 1587.185791015625\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3600.96728515625\n",
      "epoch 10000, loss 2570.237060546875\n",
      "epoch 10000, loss 3101.794189453125\n",
      "Current best 100.0\n",
      "epoch 10000, loss 4578.02587890625\n",
      "epoch 10000, loss 248.71231079101562\n",
      "epoch 10000, loss 66.3590316772461\n",
      "epoch 10000, loss 116.59783935546875\n",
      "Current best 0.01\n",
      "epoch 10000, loss 240.79196166992188\n",
      "epoch 10000, loss 74.14691162109375\n",
      "epoch 10000, loss 118.3715591430664\n",
      "Current best 0.05\n",
      "epoch 10000, loss 242.3280029296875\n",
      "epoch 10000, loss 75.53430938720703\n",
      "epoch 10000, loss 120.58871459960938\n",
      "Current best 0.1\n",
      "epoch 10000, loss 264.8301086425781\n",
      "epoch 10000, loss 86.61054229736328\n",
      "epoch 10000, loss 62.40811538696289\n",
      "Current best 0.5\n",
      "epoch 10000, loss 269.7844543457031\n",
      "epoch 10000, loss 100.39810180664062\n",
      "epoch 10000, loss 76.90593719482422\n",
      "Current best 1.0\n",
      "epoch 10000, loss 391.8437805175781\n",
      "epoch 10000, loss 209.16455078125\n",
      "epoch 10000, loss 187.41273498535156\n",
      "Current best 5.0\n",
      "epoch 10000, loss 553.838623046875\n",
      "epoch 10000, loss 334.19879150390625\n",
      "epoch 10000, loss 322.5045471191406\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1764.5836181640625\n",
      "epoch 10000, loss 1319.546875\n",
      "epoch 10000, loss 1396.37109375\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3274.39013671875\n",
      "epoch 10000, loss 2570.2431640625\n",
      "epoch 10000, loss 2753.9658203125\n",
      "Current best 100.0\n",
      "epoch 10000, loss 4992.56591796875\n",
      "epoch 10000, loss 89.90876770019531\n",
      "epoch 10000, loss 62.7945442199707\n",
      "epoch 10000, loss 47.83607482910156\n",
      "Current best 0.01\n",
      "epoch 10000, loss 91.64527893066406\n",
      "epoch 10000, loss 64.09169006347656\n",
      "epoch 10000, loss 49.04084777832031\n",
      "Current best 0.05\n",
      "epoch 10000, loss 91.06912994384766\n",
      "epoch 10000, loss 65.64973449707031\n",
      "epoch 10000, loss 120.58870697021484\n",
      "Current best 0.1\n",
      "epoch 10000, loss 108.79469299316406\n",
      "epoch 10000, loss 65.55470275878906\n",
      "epoch 10000, loss 138.32591247558594\n",
      "Current best 0.5\n",
      "epoch 10000, loss 127.26042175292969\n",
      "epoch 10000, loss 94.1957015991211\n",
      "epoch 10000, loss 160.49740600585938\n",
      "Current best 1.0\n",
      "epoch 10000, loss 272.65087890625\n",
      "epoch 10000, loss 219.020263671875\n",
      "epoch 10000, loss 187.55833435058594\n",
      "Current best 5.0\n",
      "epoch 10000, loss 452.6479187011719\n",
      "epoch 10000, loss 366.94427490234375\n",
      "epoch 10000, loss 384.4369812011719\n",
      "Current best 10.0\n",
      "epoch 10000, loss 1884.427490234375\n",
      "epoch 10000, loss 1579.3140869140625\n",
      "epoch 10000, loss 1395.2353515625\n",
      "Current best 50.0\n",
      "epoch 10000, loss 3670.33251953125\n",
      "epoch 10000, loss 3348.69140625\n",
      "epoch 10000, loss 2734.215576171875\n",
      "Current best 100.0\n",
      "epoch 10000, loss 4754.7080078125\n",
      "CoDA-PCA:\n",
      "[1437.0137, 311.39185, 124.500755, 2354.0117]\n",
      "CLR-PCA:\n",
      "[1160.9021, 291.15286, 83.87587, 2103.9946]\n",
      "Naive regression:\n",
      "[1837.7295, 403.7649, 241.57817, 2755.8904]\n",
      "CoDA-Regress:\n",
      "[131.37111, 377.55435, 272.43222, 1180.4347]\n",
      "loading Aitchinson/Data 18. Compositions and total pebble counts of 92 glacial tills.csv...\n",
      "92 samples 5 features\n",
      "sparsity: 10.434782608695652%\n",
      "[epoch     0] L=  5.4783\n",
      "epoch 10000, loss -101375712.0\n",
      "epoch 10000, loss -50419012.0\n",
      "epoch 10000, loss -118218328.0\n",
      "Current best 0.01\n",
      "epoch 10000, loss -508156736.0\n",
      "epoch 10000, loss -253268640.0\n",
      "epoch 10000, loss -589117632.0\n",
      "Current best 0.05\n",
      "epoch 10000, loss -1009760192.0\n",
      "epoch 10000, loss -502041344.0\n",
      "epoch 10000, loss -1178474752.0\n",
      "Current best 0.1\n",
      "epoch 10000, loss -5068824576.0\n",
      "epoch 10000, loss -2520946688.0\n",
      "epoch 10000, loss -5910833152.0\n",
      "Current best 0.5\n",
      "epoch 10000, loss -10163204096.0\n",
      "epoch 10000, loss -5029618176.0\n",
      "epoch 10000, loss -11822400512.0\n",
      "Current best 1.0\n",
      "epoch 10000, loss -50686427136.0\n",
      "epoch 10000, loss -25207314432.0\n",
      "epoch 10000, loss -58850885632.0\n",
      "Current best 5.0\n",
      "epoch 10000, loss -100906729472.0\n",
      "epoch 10000, loss -50358906880.0\n",
      "epoch 10000, loss -118224175104.0\n",
      "Current best 10.0\n",
      "epoch 10000, loss -505032540160.0\n",
      "epoch 10000, loss -252025044992.0\n",
      "epoch 10000, loss -591259369472.0\n",
      "Current best 50.0\n",
      "epoch 10000, loss -1010660278272.0\n",
      "epoch 10000, loss -506557693952.0\n",
      "epoch 10000, loss -1181925244928.0\n",
      "Current best 100.0\n",
      "epoch 10000, loss -1350677037056.0\n",
      "epoch 10000, loss -101382624.0\n",
      "epoch 10000, loss -84600912.0\n",
      "epoch 10000, loss -151591536.0\n",
      "Current best 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10000, loss -504853664.0\n",
      "epoch 10000, loss -422340832.0\n",
      "epoch 10000, loss -753893504.0\n",
      "Current best 0.05\n",
      "epoch 10000, loss -1013793600.0\n",
      "epoch 10000, loss -842072640.0\n",
      "epoch 10000, loss -1522960768.0\n",
      "Current best 0.1\n",
      "epoch 10000, loss -5054588416.0\n",
      "epoch 10000, loss -4229857792.0\n",
      "epoch 10000, loss -7603624448.0\n",
      "Current best 0.5\n",
      "epoch 10000, loss -10137572352.0\n",
      "epoch 10000, loss -8415312896.0\n",
      "epoch 10000, loss -15087070208.0\n",
      "Current best 1.0\n",
      "epoch 10000, loss -50821820416.0\n",
      "epoch 10000, loss -42240327680.0\n",
      "epoch 10000, loss -76136792064.0\n",
      "Current best 5.0\n",
      "epoch 10000, loss -101031870464.0\n",
      "epoch 10000, loss -84124680192.0\n",
      "epoch 10000, loss -152087756800.0\n",
      "Current best 10.0\n",
      "epoch 10000, loss -505607225344.0\n",
      "epoch 10000, loss -420497948672.0\n",
      "epoch 10000, loss -761370836992.0\n",
      "Current best 50.0\n",
      "epoch 10000, loss -1011108151296.0\n",
      "epoch 10000, loss -840127086592.0\n",
      "epoch 10000, loss -1513601892352.0\n",
      "Current best 100.0\n",
      "epoch 10000, loss -1681920098304.0\n",
      "epoch 10000, loss -50641388.0\n",
      "epoch 10000, loss -84474064.0\n",
      "epoch 10000, loss -101143632.0\n",
      "Current best 0.01\n",
      "epoch 10000, loss -250919376.0\n",
      "epoch 10000, loss -420530976.0\n",
      "epoch 10000, loss -505752000.0\n",
      "Current best 0.05\n",
      "epoch 10000, loss -501839328.0\n",
      "epoch 10000, loss -840554432.0\n",
      "epoch 10000, loss -1014124352.0\n",
      "Current best 0.1\n",
      "epoch 10000, loss -2517191680.0\n",
      "epoch 10000, loss -4223460096.0\n",
      "epoch 10000, loss -5056780800.0\n",
      "Current best 0.5\n",
      "epoch 10000, loss -5038829056.0\n",
      "epoch 10000, loss -8448298496.0\n",
      "epoch 10000, loss -10134453248.0\n",
      "Current best 1.0\n",
      "epoch 10000, loss -25298993152.0\n",
      "epoch 10000, loss -42239008768.0\n",
      "epoch 10000, loss -50668822528.0\n",
      "Current best 5.0\n",
      "epoch 10000, loss -50585456640.0\n",
      "epoch 10000, loss -84109074432.0\n",
      "epoch 10000, loss -101171765248.0\n",
      "Current best 10.0\n",
      "epoch 10000, loss -252060368896.0\n",
      "epoch 10000, loss -422406258688.0\n",
      "epoch 10000, loss -505616498688.0\n",
      "Current best 50.0\n",
      "epoch 10000, loss -502654369792.0\n",
      "epoch 10000, loss -842038444032.0\n",
      "epoch 10000, loss -1013147435008.0\n",
      "Current best 100.0\n",
      "epoch 10000, loss -1173666267136.0\n",
      "epoch 10000, loss -118215312.0\n",
      "epoch 10000, loss -151392064.0\n",
      "epoch 10000, loss -101330608.0\n",
      "Current best 0.01\n",
      "epoch 10000, loss -589195072.0\n",
      "epoch 10000, loss -761423040.0\n",
      "epoch 10000, loss -505828992.0\n",
      "Current best 0.05\n",
      "epoch 10000, loss -1177499776.0\n",
      "epoch 10000, loss -1520631680.0\n",
      "epoch 10000, loss -1013391680.0\n",
      "Current best 0.1\n",
      "epoch 10000, loss -5883967488.0\n",
      "epoch 10000, loss -7586192896.0\n",
      "epoch 10000, loss -5057984000.0\n",
      "Current best 0.5\n",
      "epoch 10000, loss -11727263744.0\n",
      "epoch 10000, loss -15133387776.0\n",
      "epoch 10000, loss -10117742592.0\n",
      "Current best 1.0\n",
      "epoch 10000, loss -58751582208.0\n",
      "epoch 10000, loss -76037193728.0\n",
      "epoch 10000, loss -50503409664.0\n",
      "Current best 5.0\n",
      "epoch 10000, loss -118208503808.0\n",
      "epoch 10000, loss -152295702528.0\n",
      "epoch 10000, loss -101142069248.0\n",
      "Current best 10.0\n",
      "epoch 10000, loss -591238791168.0\n",
      "epoch 10000, loss -757476098048.0\n",
      "epoch 10000, loss -506685489152.0\n",
      "Current best 50.0\n",
      "epoch 10000, loss -1182084759552.0\n",
      "epoch 10000, loss -1520676241408.0\n",
      "epoch 10000, loss -1011506675712.0\n",
      "Current best 100.0\n",
      "epoch 10000, loss -1849671024640.0\n",
      "CoDA-PCA:\n",
      "[39.36669, 4.3364425, 14.845026, 4.0098796]\n",
      "CLR-PCA:\n",
      "[38.536205, 5.076206, 13.842892, 4.3879657]\n",
      "Naive regression:\n",
      "[39.31268, 4.00156, 14.825169, 4.140946]\n",
      "CoDA-Regress:\n",
      "[39.43065, 4.8479123, 13.741797, 4.4096828]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#need to specify where the the targets and features are in the dataset, and whether there are non compositional features\n",
    "\n",
    "def PCA_Regression(data, co_feature_indices, target_index, \n",
    "                   other_feature_indices = [], alg=CodaPCA.Alg.CODAPCA, verbose=True):\n",
    "    \n",
    "    #can loop through/optimise this in another way?\n",
    "    \n",
    "    headers = data[1]\n",
    "    features = data[0][:,co_feature_indices]\n",
    "    targets = data[0][:,target_index]\n",
    "    \n",
    "    #normalise the compositional features. TODO anything extra to deal with non compositional features?\n",
    "    features = np.array([feat/sum(feat) for feat in features])\n",
    "\n",
    "    #can be empty\n",
    "    extra_features = data[0][:,other_feature_indices]\n",
    "    \n",
    "    #TODO double check this\n",
    "    features = np.hstack([features, extra_features])\n",
    "    \n",
    "    #compute the CoDA-PCA projection \n",
    "    #TODO add component number as a hyperparameter to optimise \n",
    "    n_components=len(co_feature_indices)-2\n",
    "\n",
    "    pca = CodaPCA.CodaPCA(n_components,lrate=1e-4,nn_shape=[100,100], alg=alg)\n",
    "    #TODO: check why this is numerically unstable\n",
    "    #pca = CodaPCA.NonParametricCodaPCA(n_components)\n",
    "\n",
    "    pca.fit(features)\n",
    "    \n",
    "    Y_coda = pca.transform(features)\n",
    "\n",
    "    pca_clr = CodaPCA.CLRPCA(n_components)\n",
    "    pca_clr.fit(features)\n",
    "    \n",
    "    Y_clr = pca_clr.transform(features)\n",
    "    \n",
    "    \n",
    "    splits = 4\n",
    "        \n",
    "    #split data \n",
    "    kf = KFold(splits)\n",
    "        \n",
    "    folds = [i for i in kf.split(features)]      \n",
    "    \n",
    "\n",
    "    lm = Ridge()\n",
    "    \n",
    "    coda_score = enhanced_cross_val(lm,Y_coda, targets, folds)\n",
    "    clr_score = enhanced_cross_val(lm,Y_clr, targets, folds) \n",
    "    naive_score = enhanced_cross_val(lm, features, targets, folds)\n",
    "    \n",
    "    regress_score = coda_val(features, targets, n_components, folds)\n",
    "    \n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"CoDA-PCA:\")\n",
    "        print(coda_score)\n",
    "        print(\"CLR-PCA:\")\n",
    "        print(clr_score)\n",
    "        print (\"Naive regression:\")\n",
    "        print (naive_score)\n",
    "        print (\"CoDA-Regress:\")\n",
    "        print (regress_score)\n",
    "    \n",
    "\n",
    "    return coda_score,clr_score,naive_score,regress_score\n",
    "\n",
    "#training methodology as described in:\n",
    "#https://papers.nips.cc/paper/3215-learning-with-transformation-invariant-kernels.pdf\n",
    "def enhanced_cross_val(model, features, targets, folds):\n",
    "    assert len(features) == len(targets), \"Mismatch in length of features and targets\"\n",
    "    \n",
    "    #define the number of splits and folds uised in the parameter selection\n",
    "    #stick to smaller splits since we have small datasets\n",
    "    #splits = 4\n",
    "    param_splits = 3\n",
    "    #split data \n",
    "    #kf = KFold(splits)\n",
    "    kfold_scores = []\n",
    "    for train, test in folds:        \n",
    "        Y_train = targets[train]\n",
    "        X_train = features[train]\n",
    "        \n",
    "       \n",
    "        Y_test = targets[test]\n",
    "        X_test = features[test]\n",
    "        \n",
    "        #inner loop for parameter selection (regularisation term in Ridge Regression):\n",
    "        param_grid = [0.01,0.05,0.1,0.5,1.0,5.0,10.0,50.0,100.0]\n",
    "        for a in param_grid:\n",
    "            max_score = -np.inf\n",
    "            lm = Ridge(a)\n",
    "   \n",
    "            curr_score = np.mean(cross_val_score(lm, X_train, Y_train,cv=param_splits))\n",
    "            if curr_score > max_score:\n",
    "                max_score = curr_score\n",
    "                best_param = a\n",
    "        \n",
    "        #compute test score based on best parameter\n",
    "        lm = Ridge(best_param)\n",
    "        lm.fit(X_train, Y_train)\n",
    "        y_pred = lm.predict(X_test)\n",
    "        kfold_scores.append(sklearn.metrics.mean_squared_error(Y_test,y_pred))\n",
    "                \n",
    "    return kfold_scores\n",
    "\n",
    "\n",
    "\n",
    "#can automate this if we had assume a certain structure for the indices of features and targets, or an array per dataset \n",
    "\n",
    "score_dict = {}\n",
    "\n",
    "#TODO r_files isn't always consistent\n",
    "\n",
    "data_21_scores = PCA_Regression(read_csv(r_files[0], normalize=False), co_feature_indices=[0,1,2,3], target_index=4)\n",
    "score_dict['21'] = data_21_scores\n",
    "\n",
    "data_21_scores2 = PCA_Regression(read_csv(r_files[0], normalize=False), co_feature_indices=[0,1,2,3], target_index=5)\n",
    "score_dict['21,2'] = data_21_scores2\n",
    "\n",
    "data_34_scores = PCA_Regression(read_csv(r_files[1], normalize=False), co_feature_indices=[0,1,2,3], target_index=4) \n",
    "score_dict['34'] = data_34_scores\n",
    "\n",
    "data_3_scores = PCA_Regression(read_csv(r_files[2], normalize=False), co_feature_indices=[0,1,2,3,4], target_index=5) \n",
    "score_dict['3'] = data_3_scores\n",
    "\n",
    "data_39_scores = PCA_Regression(read_csv(r_files[3], normalize=False), co_feature_indices=[0,1,2], target_index=3)\n",
    "score_dict['39'] = data_39_scores\n",
    "\n",
    "data_4_scores = PCA_Regression(read_csv(r_files[4], normalize=False), co_feature_indices=[0,1,2,3,4], target_index=5)\n",
    "score_dict['4'] = data_4_scores\n",
    "\n",
    "data_4_scores2 = PCA_Regression(read_csv(r_files[4], normalize=False), co_feature_indices=[0,1,2,3,4], target_index=6)\n",
    "score_dict['4,2'] = data_4_scores2\n",
    "\n",
    "data_5_scores = PCA_Regression(read_csv(r_files[5], normalize=False), co_feature_indices=[0,1,2], target_index=3)\n",
    "score_dict['5'] = data_4_scores\n",
    "\n",
    "data_18_scores = PCA_Regression(read_csv(r_files[6], normalize=False), co_feature_indices=[0,1,2], target_index=3)\n",
    "score_dict['18'] = data_5_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'score_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6c74a2d091fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtable_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmean_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtable_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmean_scores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score_dict' is not defined"
     ]
    }
   ],
   "source": [
    "#note: plotly code works fine, but gives a jupyter warning when saving with a rendered table\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "table_rows = []\n",
    "for key in score_dict.keys():\n",
    "    mean_scores = list(map(np.mean, score_dict[key]))\n",
    "    table_rows.append([key,*mean_scores])\n",
    "    \n",
    "results = [go.Table(\n",
    "    header=dict(values=[\"Dataset\",\"CoDA-PCA\", \"CLR-PCA\", \"Naive Regression\", \"CoDA-Regress\"]),\n",
    "    cells=dict(values=np.array(table_rows).T))]\n",
    "\n",
    "iplot(results, filename = 'basic_table')    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0704 15:40:32.875234 139989270914880 deprecation_wrapper.py:119] From /home/sean/Documents/Honours/honours/Code/coda-pca-orig/coda/codes/CodaPCA.py:14: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W0704 15:40:32.875713 139989270914880 deprecation_wrapper.py:119] From /home/sean/Documents/Honours/honours/Code/coda-pca-orig/coda/codes/CodaPCA.py:14: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import CodaPCA\n",
    "import CodaRegress\n",
    "import numpy as np\n",
    "from runpca import read_csv\n",
    "import os\n",
    "import sklearn\n",
    "import torch\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#change module for newer sklearn versions\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from sklearn.model_selection  import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data. Given an array of which files are regression, classification or unlabelled\n",
    "data_path = os.getcwd() + \"/Aitchinson\"\n",
    "\n",
    "regression_list = [3,4,5,18,21,34,39]\n",
    "classification_list = [7,8,9,11,12,16,17,19,23,24,25,26,28,29,33,37]\n",
    "unlabelled_list=[1,2,6,10,13,14,15,20,22,27,30,31,32,35,36,38,40]\n",
    "\n",
    "r_files = []\n",
    "c_files = []\n",
    "u_files = []\n",
    "\n",
    "\n",
    "for file in os.listdir(data_path):\n",
    "    for i in regression_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            r_files.append(\"Aitchinson/\" + file)\n",
    "    for i in classification_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            c_files.append(\"Aitchinson/\" + file)\n",
    "    for i in unlabelled_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            u_files.append(\"Aitchinson/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coda_val(features, targets, n_components):\n",
    "        #training code stub, read in data as X and targets as y\n",
    "    #TODO substitute this into model class, and set up API similar to original CoDA-PCA paper\n",
    "    kf = KFold(4)\n",
    "    splits = [i for i in kf.split(features)]\n",
    "\n",
    "    X = features\n",
    "    y = targets.reshape(-1,1)\n",
    "    #define the combined loss with hyperparameter lambda\n",
    "    scores = []\n",
    "    for ind in splits: \n",
    "        model = CodaRegress.CoDA_Regress(X.shape[1], 2, [100,], [20,])\n",
    "\n",
    "        #model = Ridge(1)\n",
    "\n",
    "        \n",
    "        #TODO loop through lambda \n",
    "        model.fit(torch.FloatTensor(X[ind[0]]),  torch.FloatTensor(y[ind[0]]), 1, lr=1e-2)\n",
    "\n",
    "        y_pred = model.predict(torch.FloatTensor(X[ind[1]]))\n",
    "\n",
    "        scores.append(sklearn.metrics.mean_squared_error(y[ind[1]],y_pred.detach().numpy()))\n",
    "    return scores \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Aitchinson/Data 21. Permeabilities of bayesite for 21 mixtures of fibres and bonding pressures..csv...\n",
      "21 samples 6 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  4.0065\n",
      "epoch 1000, loss 58.13300323486328\n",
      "epoch 2000, loss 57.70204544067383\n",
      "epoch 3000, loss 57.550758361816406\n",
      "epoch 4000, loss 57.2678108215332\n",
      "epoch 5000, loss 57.243064880371094\n",
      "epoch 6000, loss 57.22691345214844\n",
      "epoch 7000, loss 57.222938537597656\n",
      "epoch 8000, loss 57.222137451171875\n",
      "epoch 9000, loss 57.21965026855469\n",
      "epoch 10000, loss 57.21968078613281\n",
      "epoch 1000, loss 62.60455322265625\n",
      "epoch 2000, loss 62.623260498046875\n",
      "epoch 3000, loss 62.74311065673828\n",
      "epoch 4000, loss 59.947853088378906\n",
      "epoch 5000, loss 59.77859115600586\n",
      "epoch 6000, loss 59.7716178894043\n",
      "epoch 7000, loss 59.6322021484375\n",
      "epoch 8000, loss 59.61018371582031\n",
      "epoch 9000, loss 59.66316223144531\n",
      "epoch 10000, loss 59.60496139526367\n",
      "epoch 1000, loss 60.435752868652344\n",
      "epoch 2000, loss 60.3230094909668\n",
      "epoch 3000, loss 60.54515075683594\n",
      "epoch 4000, loss 60.305152893066406\n",
      "epoch 5000, loss 60.318580627441406\n",
      "epoch 6000, loss 60.31065368652344\n",
      "epoch 7000, loss 60.30317687988281\n",
      "epoch 8000, loss 60.33260726928711\n",
      "epoch 9000, loss 60.30332565307617\n",
      "epoch 10000, loss 60.456783294677734\n",
      "epoch 1000, loss 59.86827087402344\n",
      "epoch 2000, loss 59.81446838378906\n",
      "epoch 3000, loss 59.765113830566406\n",
      "epoch 4000, loss 59.85165786743164\n",
      "epoch 5000, loss 59.72816467285156\n",
      "epoch 6000, loss 59.716209411621094\n",
      "epoch 7000, loss 59.70732879638672\n",
      "epoch 8000, loss 59.6949348449707\n",
      "epoch 9000, loss 59.689571380615234\n",
      "epoch 10000, loss 59.68854904174805\n",
      "CoDA-PCA:\n",
      "[4.523825, 7.285652, 5.1015506, 3.6911576]\n",
      "CLR-PCA:\n",
      "[4.5172668, 7.288266, 5.090907, 3.68751]\n",
      "Naive regression:\n",
      "[4.5017495, 7.2919145, 5.0983486, 3.6929336]\n",
      "CoDA-Regress:\n",
      "[160.82034, 18.001652, 5.009728, 3.6166377]\n",
      "loading Aitchinson/Data 34. Foraminiferal compositions at 30 different depths.csv...\n",
      "30 samples 5 features\n",
      "sparsity: 3.3333333333333335%\n",
      "[epoch     0] L=  5.5257\n",
      "epoch 1000, loss -6506655744.0\n",
      "epoch 2000, loss -6506561536.0\n",
      "epoch 3000, loss -5903270912.0\n",
      "epoch 4000, loss -6506238976.0\n",
      "epoch 5000, loss -6506608640.0\n",
      "epoch 6000, loss -6507581952.0\n",
      "epoch 7000, loss -6524680192.0\n",
      "epoch 8000, loss -6506629632.0\n",
      "epoch 9000, loss -6506633728.0\n",
      "epoch 10000, loss -6506636800.0\n",
      "epoch 1000, loss -8157452800.0\n",
      "epoch 2000, loss -8157471744.0\n",
      "epoch 3000, loss -8157048832.0\n",
      "epoch 4000, loss -5788042752.0\n",
      "epoch 5000, loss -4738948608.0\n",
      "epoch 6000, loss -4738991104.0\n",
      "epoch 7000, loss -4739004928.0\n",
      "epoch 8000, loss -4739010560.0\n",
      "epoch 9000, loss -4739013632.0\n",
      "epoch 10000, loss -4739014656.0\n",
      "epoch 1000, loss -4945701376.0\n",
      "epoch 2000, loss -4946417664.0\n",
      "epoch 3000, loss -4949358080.0\n",
      "epoch 4000, loss -4948404736.0\n",
      "epoch 5000, loss -4886679552.0\n",
      "epoch 6000, loss -4929268224.0\n",
      "epoch 7000, loss -4929284608.0\n",
      "epoch 8000, loss -4929285120.0\n",
      "epoch 9000, loss -4929285120.0\n",
      "epoch 10000, loss -4928954368.0\n",
      "epoch 1000, loss -4939017216.0\n",
      "epoch 2000, loss -4939017728.0\n",
      "epoch 3000, loss -4939017216.0\n",
      "epoch 4000, loss -4939017216.0\n",
      "epoch 5000, loss -4939017216.0\n",
      "epoch 6000, loss -4939017216.0\n",
      "epoch 7000, loss -4939017216.0\n",
      "epoch 8000, loss -4939017728.0\n",
      "epoch 9000, loss -4939017216.0\n",
      "epoch 10000, loss -4939011584.0\n",
      "CoDA-PCA:\n",
      "[229.51733, 21.916786, 38.84823, 228.68376]\n",
      "CLR-PCA:\n",
      "[212.0833, 17.84351, 73.573265, 246.33154]\n",
      "Naive regression:\n",
      "[230.17953, 21.89151, 38.426308, 228.93114]\n",
      "CoDA-Regress:\n",
      "[212.78229, 18.012194, 72.33263, 317.06146]\n",
      "loading Aitchinson/Data 3. Compositions and depths of 25 specimens of boxite (Percentages by weight).csv...\n",
      "25 samples 6 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  3.9752\n",
      "epoch 1000, loss 87.22818756103516\n",
      "epoch 2000, loss 86.88249206542969\n",
      "epoch 3000, loss 72.95799255371094\n",
      "epoch 4000, loss 72.73567962646484\n",
      "epoch 5000, loss 72.63038635253906\n",
      "epoch 6000, loss 71.07271575927734\n",
      "epoch 7000, loss 70.96707916259766\n",
      "epoch 8000, loss 70.99783325195312\n",
      "epoch 9000, loss 70.95065307617188\n",
      "epoch 10000, loss 70.93675231933594\n",
      "epoch 1000, loss 139.04232788085938\n",
      "epoch 2000, loss 139.02926635742188\n",
      "epoch 3000, loss 139.0292510986328\n",
      "epoch 4000, loss 139.0292510986328\n",
      "epoch 5000, loss 139.0292510986328\n",
      "epoch 6000, loss 139.0292205810547\n",
      "epoch 7000, loss 139.02926635742188\n",
      "epoch 8000, loss 139.02926635742188\n",
      "epoch 9000, loss 139.02926635742188\n",
      "epoch 10000, loss 139.0294952392578\n",
      "epoch 1000, loss 123.14588928222656\n",
      "epoch 2000, loss 122.43794250488281\n",
      "epoch 3000, loss 121.1876220703125\n",
      "epoch 4000, loss 120.43309020996094\n",
      "epoch 5000, loss 120.29584503173828\n",
      "epoch 6000, loss 120.28440856933594\n",
      "epoch 7000, loss 120.28350830078125\n",
      "epoch 8000, loss 120.28218078613281\n",
      "epoch 9000, loss 120.28177642822266\n",
      "epoch 10000, loss 120.38404846191406\n",
      "epoch 1000, loss 94.8891372680664\n",
      "epoch 2000, loss 93.98076629638672\n",
      "epoch 3000, loss 93.93734741210938\n",
      "epoch 4000, loss 93.95465850830078\n",
      "epoch 5000, loss 93.93802642822266\n",
      "epoch 6000, loss 94.05319213867188\n",
      "epoch 7000, loss 93.94245147705078\n",
      "epoch 8000, loss 93.93696594238281\n",
      "epoch 9000, loss 93.9371337890625\n",
      "epoch 10000, loss 93.93734741210938\n",
      "CoDA-PCA:\n",
      "[160.21925, 13.769252, 24.075836, 159.12427]\n",
      "CLR-PCA:\n",
      "[160.22293, 13.775093, 24.047392, 159.17717]\n",
      "Naive regression:\n",
      "[160.25475, 13.741725, 24.122057, 159.16808]\n",
      "CoDA-Regress:\n",
      "[116.22488, 13.737304, 37.725353, 192.4507]\n",
      "loading Aitchinson/Data 39. Microhardness of 18 glass specimens and their (Ge, Sb, Se) compositions.csv...\n",
      "18 samples 4 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  4.9764\n",
      "epoch 1000, loss 25.37578582763672\n",
      "epoch 2000, loss 25.34506607055664\n",
      "epoch 3000, loss 25.331806182861328\n",
      "epoch 4000, loss 25.33245277404785\n",
      "epoch 5000, loss 25.31108283996582\n",
      "epoch 6000, loss 25.308040618896484\n",
      "epoch 7000, loss 25.303422927856445\n",
      "epoch 8000, loss 25.30330467224121\n",
      "epoch 9000, loss 25.303329467773438\n",
      "epoch 10000, loss 25.316204071044922\n",
      "epoch 1000, loss 21.51144790649414\n",
      "epoch 2000, loss 21.46704864501953\n",
      "epoch 3000, loss 21.46875762939453\n",
      "epoch 4000, loss 21.465158462524414\n",
      "epoch 5000, loss 21.45144271850586\n",
      "epoch 6000, loss 21.45210075378418\n",
      "epoch 7000, loss 21.450437545776367\n",
      "epoch 8000, loss 21.44849967956543\n",
      "epoch 9000, loss 21.450475692749023\n",
      "epoch 10000, loss 21.44925880432129\n",
      "epoch 1000, loss 23.238719940185547\n",
      "epoch 2000, loss 23.224008560180664\n",
      "epoch 3000, loss 23.204784393310547\n",
      "epoch 4000, loss 23.19811248779297\n",
      "epoch 5000, loss 23.19527816772461\n",
      "epoch 6000, loss 23.189449310302734\n",
      "epoch 7000, loss 23.24505615234375\n",
      "epoch 8000, loss 23.187009811401367\n",
      "epoch 9000, loss 23.187896728515625\n",
      "epoch 10000, loss 23.186843872070312\n",
      "epoch 1000, loss 21.154022216796875\n",
      "epoch 2000, loss 21.029306411743164\n",
      "epoch 3000, loss 21.245662689208984\n",
      "epoch 4000, loss 20.983543395996094\n",
      "epoch 5000, loss 20.975383758544922\n",
      "epoch 6000, loss 20.970605850219727\n",
      "epoch 7000, loss 20.968151092529297\n",
      "epoch 8000, loss 20.96750259399414\n",
      "epoch 9000, loss 20.97827911376953\n",
      "epoch 10000, loss 20.967138290405273\n",
      "CoDA-PCA:\n",
      "[0.062315304, 0.003978611, 0.011366621, 0.057551865]\n",
      "CLR-PCA:\n",
      "[0.060506113, 0.0038533728, 0.0099628195, 0.05535987]\n",
      "Naive regression:\n",
      "[0.06243059, 0.0039659482, 0.011303257, 0.05774308]\n",
      "CoDA-Regress:\n",
      "[0.030676076, 0.004339939, 0.004640142, 0.010681896]\n",
      "loading Aitchinson/Data 4. Compositions, depths and porosities of 25 specimens of coxite (Percentages by weight).csv...\n",
      "25 samples 7 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  7.7641\n",
      "epoch 1000, loss 35.23039245605469\n",
      "epoch 2000, loss 35.13669204711914\n",
      "epoch 3000, loss 35.1477165222168\n",
      "epoch 4000, loss 35.14678192138672\n",
      "epoch 5000, loss 35.13460922241211\n",
      "epoch 6000, loss 35.13539123535156\n",
      "epoch 7000, loss 35.158660888671875\n",
      "epoch 8000, loss 35.134422302246094\n",
      "epoch 9000, loss 35.15629577636719\n",
      "epoch 10000, loss 35.134552001953125\n",
      "epoch 1000, loss 36.4410285949707\n",
      "epoch 2000, loss 36.416587829589844\n",
      "epoch 3000, loss 36.4161376953125\n",
      "epoch 4000, loss 36.41606521606445\n",
      "epoch 5000, loss 36.416019439697266\n",
      "epoch 6000, loss 36.41634750366211\n",
      "epoch 7000, loss 36.415870666503906\n",
      "epoch 8000, loss 36.41619110107422\n",
      "epoch 9000, loss 36.41585922241211\n",
      "epoch 10000, loss 36.41585159301758\n",
      "epoch 1000, loss 37.13645935058594\n",
      "epoch 2000, loss 37.13591003417969\n",
      "epoch 3000, loss 37.135833740234375\n",
      "epoch 4000, loss 37.1357536315918\n",
      "epoch 5000, loss 37.13606262207031\n",
      "epoch 6000, loss 37.135746002197266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7000, loss 37.13572692871094\n",
      "epoch 8000, loss 37.13570785522461\n",
      "epoch 9000, loss 37.135780334472656\n",
      "epoch 10000, loss 37.1357536315918\n",
      "epoch 1000, loss 36.98249435424805\n",
      "epoch 2000, loss 36.94123077392578\n",
      "epoch 3000, loss 36.94089126586914\n",
      "epoch 4000, loss 36.94050979614258\n",
      "epoch 5000, loss 36.94013214111328\n",
      "epoch 6000, loss 36.93976974487305\n",
      "epoch 7000, loss 36.93925476074219\n",
      "epoch 8000, loss 36.94196701049805\n",
      "epoch 9000, loss 36.9391975402832\n",
      "epoch 10000, loss 36.94491958618164\n",
      "CoDA-PCA:\n",
      "[0.29019755, 0.83887243, 0.1986119, 0.48815894]\n",
      "CLR-PCA:\n",
      "[0.28949237, 0.837194, 0.1975025, 0.48705664]\n",
      "Naive regression:\n",
      "[0.29020914, 0.83905107, 0.1986928, 0.48838246]\n",
      "CoDA-Regress:\n",
      "[0.2796732, 0.3318491, 0.19870318, 0.43150043]\n",
      "loading Aitchinson/Data 5. Sand, silt, clay compositions of 39 sediment samples at different water depths in an Arctic lake.csv...\n",
      "39 samples 4 features\n",
      "sparsity: 0.0%\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6b6e74bab444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mscore_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'39'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_39_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mdata_4_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA_Regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mco_feature_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0mscore_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'4'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_4_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-6b6e74bab444>\u001b[0m in \u001b[0;36mPCA_Regression\u001b[0;34m(data, co_feature_indices, target_index, other_feature_indices, alg, verbose)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mco_feature_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#normalise the compositional features. TODO anything extra to deal with non compositional features?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "\n",
    "#need to specify where the the targets and features are in the dataset, and whether there are non compositional features\n",
    "\n",
    "def PCA_Regression(data, co_feature_indices, target_index, \n",
    "                   other_feature_indices = [], alg=CodaPCA.Alg.CODAPCA, verbose=True):\n",
    "    \n",
    "    #can loop through/optimise this in another way?\n",
    "    \n",
    "    headers = data[1]\n",
    "    features = data[0][:,co_feature_indices]\n",
    "    targets = data[0][:,target_index]\n",
    "    \n",
    "    #normalise the compositional features. TODO anything extra to deal with non compositional features?\n",
    "    features = np.array([feat/sum(feat) for feat in features])\n",
    "\n",
    "    #can be empty\n",
    "    extra_features = data[0][:,other_feature_indices]\n",
    "    \n",
    "    #TODO double check this\n",
    "    features = np.hstack([features, extra_features])\n",
    "    \n",
    "    #compute the CoDA-PCA projection \n",
    "    #TODO add component number as a hyperparameter to optimise \n",
    "    n_components=len(co_feature_indices)-2\n",
    "\n",
    "    pca = CodaPCA.CodaPCA(n_components,lrate=1e-4,nn_shape=[100,100], alg=alg)\n",
    "    #TODO: check why this is numerically unstable\n",
    "    #pca = CodaPCA.NonParametricCodaPCA(n_components)\n",
    "\n",
    "    pca.fit(features)\n",
    "    \n",
    "    Y_coda = pca.transform(features)\n",
    "\n",
    "    pca_clr = CodaPCA.CLRPCA(n_components)\n",
    "    pca_clr.fit(features)\n",
    "    \n",
    "    Y_clr = pca_clr.transform(features)\n",
    "    \n",
    "\n",
    "    lm = Ridge()\n",
    "    #exp the projection to get out of clr space\n",
    "    coda_score = enhanced_cross_val(lm,Y_coda, targets)\n",
    "    clr_score = enhanced_cross_val(lm,Y_clr, targets) \n",
    "    naive_score = enhanced_cross_val(lm, features, targets)\n",
    "    \n",
    "    \n",
    "    regress_score = coda_val(features, targets, n_components)\n",
    "    \n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"CoDA-PCA:\")\n",
    "        print(coda_score)\n",
    "        print(\"CLR-PCA:\")\n",
    "        print(clr_score)\n",
    "        print (\"Naive regression:\")\n",
    "        print (naive_score)\n",
    "        print (\"CoDA-Regress:\")\n",
    "        print (regress_score)\n",
    "    \n",
    "\n",
    "    return coda_score,clr_score,naive_score,regress_score\n",
    "\n",
    "#training methodology as described in:\n",
    "#https://papers.nips.cc/paper/3215-learning-with-transformation-invariant-kernels.pdf\n",
    "def enhanced_cross_val(model, features, targets):\n",
    "    assert len(features) == len(targets), \"Mismatch in length of features and targets\"\n",
    "    \n",
    "    #define the number of splits and folds uised in the parameter selection\n",
    "    #stick to smaller splits since we have small datasets\n",
    "    splits = 4\n",
    "    param_splits = 3\n",
    "    \n",
    "    #split data \n",
    "    kf = KFold(splits)\n",
    "    kfold_scores = []\n",
    "    for train, test in kf.split(features):        \n",
    "        Y_train = targets[train]\n",
    "        X_train = features[train]\n",
    "        \n",
    "       \n",
    "        Y_test = targets[test]\n",
    "        X_test = features[test]\n",
    "        \n",
    "        #inner loop for parameter selection (regularisation term in Ridge Regression):\n",
    "        param_grid = [0.01,0.05,0.1,0.5,1.0,2.0,5.0,10.0,20.0,100.0]\n",
    "        for a in param_grid:\n",
    "            max_score = -np.inf\n",
    "            lm = Ridge(a)\n",
    "   \n",
    "            curr_score = np.mean(cross_val_score(lm, X_train, Y_train,cv=param_splits))\n",
    "            if curr_score > max_score:\n",
    "                max_score = curr_score\n",
    "                best_param = a\n",
    "        \n",
    "        #compute test score based on best parameter\n",
    "        lm = Ridge(best_param)\n",
    "        lm.fit(X_train, Y_train)\n",
    "        y_pred = lm.predict(X_test)\n",
    "        kfold_scores.append(sklearn.metrics.mean_squared_error(Y_test,y_pred))\n",
    "                \n",
    "    return kfold_scores\n",
    "\n",
    "\n",
    "\n",
    "#can automate this if we had assume a certain structure for the indices of features and targets, or an array per dataset \n",
    "\n",
    "score_dict = {}\n",
    "\n",
    "data_18_scores = PCA_Regression(read_csv(r_files[0], normalize=False), co_feature_indices=[0,1,2,3], target_index=4)\n",
    "#other \"target\" also at 5\n",
    "score_dict['18'] = data_18_scores\n",
    "\n",
    "data_21_scores = PCA_Regression(read_csv(r_files[1], normalize=False), co_feature_indices=[0,1,2,3], target_index=4) \n",
    "score_dict['21'] = data_21_scores\n",
    "\n",
    "data_3_scores = PCA_Regression(read_csv(r_files[2], normalize=False), co_feature_indices=[0,1,2,3,4], target_index=5) \n",
    "score_dict['3'] = data_3_scores\n",
    "\n",
    "data_34_scores = PCA_Regression(read_csv(r_files[3], normalize=False), co_feature_indices=[0,1,2], target_index=3)\n",
    "score_dict['34'] = data_34_scores\n",
    "\n",
    "data_39_scores = PCA_Regression(read_csv(r_files[4], normalize=False), co_feature_indices=[0,1,2], target_index=3)\n",
    "score_dict['39'] = data_39_scores\n",
    "\n",
    "data_4_scores = PCA_Regression(read_csv(r_files[5], normalize=False), co_feature_indices=[0,1,2,3], target_index=4)\n",
    "score_dict['4'] = data_4_scores\n",
    "\n",
    "data_5_scores =PCA_Regression(read_csv(r_files[6], normalize=False), co_feature_indices=[0,1,2], target_index=3)\n",
    "score_dict['5'] = data_5_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#note: plotly code works fine, but gives a jupyter warning when saving with a rendered table\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "table_rows = []\n",
    "for key in score_dict.keys():\n",
    "    mean_scores = list(map(np.mean, score_dict[key]))\n",
    "    table_rows.append([key,*mean_scores])\n",
    "    \n",
    "results = [go.Table(\n",
    "    header=dict(values=[\"Dataset\",\"CoDA-PCA\", \"CLR-PCA\", \"Naive Regression\"]),\n",
    "    cells=dict(values=np.array(table_rows).T))]\n",
    "\n",
    "iplot(results, filename = 'basic_table')    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_Classification(data, co_feature_indices, target_index, \n",
    "                   other_feature_indices = [], alg=CodaPCA.Alg.CODAPCA, verbose=False):\n",
    "       #can loop through/optimise this in another way?\n",
    "    \n",
    "    headers = data[1]\n",
    "    features = data[0][:,co_feature_indices]\n",
    "    targets = data[0][:,target_index]\n",
    "    \n",
    "    #normalise the compositional features. TODO anything extra to deal with non compositional features?\n",
    "    features = np.array([feat/sum(feat) for feat in features])\n",
    "\n",
    "    #can be empty\n",
    "    extra_features = data[0][:,other_feature_indices]\n",
    "    \n",
    "    #TODO double check this\n",
    "    features = np.hstack([features, extra_features])\n",
    "    \n",
    "    #compute the CoDA-PCA projection \n",
    "    #TODO add component number as a hyperparameter to optimise \n",
    "    n_components=len(co_feature_indices)-2\n",
    "\n",
    "    pca = CodaPCA.CodaPCA(n_components,lrate=1e-3,nn_shape=[100,100], alg=alg)\n",
    "    #TODO: check why this is numerically unstable\n",
    "    #pca = CodaPCA.NonParametricCodaPCA(n_components)\n",
    "\n",
    "    pca.fit(features)\n",
    "    \n",
    "    Y_coda = pca.project(features)\n",
    "\n",
    "    pca_clr = CodaPCA.CLRPCA(n_components)\n",
    "    pca_clr.fit(features)\n",
    "    \n",
    "    Y_clr = pca_clr.project(features)\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "    #exp the projection to get out of clr space\n",
    "    coda_score = enhanced_cross_val(classifier,np.exp(Y_coda), targets)\n",
    "    clr_score = enhanced_cross_val(classifier,np.exp(Y_clr), targets) \n",
    "    naive_score = enhanced_cross_val(classifier, features, targets)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"CoDA-PCA:\")\n",
    "        print(coda_score)\n",
    "        print(\"CLR-PCA:\")\n",
    "        print(clr_score)\n",
    "        print (\"Naive Classification:\")\n",
    "        print (naive_score)\n",
    "    \n",
    "\n",
    "    return coda_score,clr_score,naive_score\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "data_9 = pd.read_csv(c_files[-1])\n",
    "adult_inds = np.where(data_9['Adult/Child'] == 'A')\n",
    "child_inds = np.where(data_9['Adult/Child'] == 'C')\n",
    "\n",
    "adult = data_9.to_numpy()[adult_inds]\n",
    "child = data_9.to_numpy()[child_inds]\n",
    "features = data_9[data_9.columns[2:]].to_numpy()\n",
    "\n",
    "\n",
    "features = np.array([feat/sum(feat) for feat in features])\n",
    "\n",
    "\n",
    "\n",
    "# pca_clr = CodaPCA.CLRPCA(2)\n",
    "# pca_clr.fit(features)\n",
    "# test = pca_clr.transform(features)\n",
    "\n",
    "pca = CodaPCA.CodaPCA(2,lrate=1e-3,nn_shape=[100,100], alg=CodaPCA.Alg.CODAAE)\n",
    "pca.fit(features)\n",
    "test = pca.transform(features)\n",
    "\n",
    "plt.scatter(x = test[adult_inds][:,0], y = test[adult_inds][:,1], c =\"red\")\n",
    "plt.scatter(x = test[child_inds][:,0], y = test[child_inds][:,1], c =\"orange\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(c_files[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "bee_data = pd.read_csv(\"beeMicrobiome.csv\")\n",
    "bee_classes = bee_data[bee_data.columns[3:]]\n",
    "\n",
    "\n",
    "#group into classes\n",
    "apiary_a = np.where(bee_classes['Apiary'] == 'A')\n",
    "apiary_b = np.where(bee_classes['Apiary'] == 'B')\n",
    "apiary_c = np.where(bee_classes['Apiary'] == 'C')\n",
    "apiary_d = np.where(bee_classes['Apiary'] == 'D')\n",
    "apiary_e = np.where(bee_classes['Apiary'] == 'E')\n",
    "\n",
    "bee_features = bee_classes.to_numpy()[:,1:]\n",
    "\n",
    "\n",
    "bee_features = np.array([feat/sum(feat) for feat in bee_features])\n",
    "\n",
    "\n",
    "pca = CodaPCA.CodaPCA(2,lrate=1e-3,nn_shape=[100,100], alg=CodaPCA.Alg.CODAPCA)\n",
    "pca.fit(bee_features)\n",
    "test = pca.transform(bee_features)\n",
    "\n",
    "plt.scatter(x = test[apiary_a][:,0], y = test[apiary_a][:,1], c =\"red\")\n",
    "plt.scatter(x = test[apiary_b][:,0], y = test[apiary_b][:,1], c =\"orange\")\n",
    "plt.scatter(x = test[apiary_c][:,0], y = test[apiary_c][:,1], c =\"yellow\")\n",
    "plt.scatter(x = test[apiary_d][:,0], y = test[apiary_d][:,1], c =\"green\")\n",
    "plt.scatter(x = test[apiary_e][:,0], y = test[apiary_e][:,1], c =\"blue\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test[apiary_a][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "\n",
    "pca = CodaPCA.CodaPCA(3,lrate=1e-3,nn_shape=[100,100], alg=CodaPCA.Alg.CODAPCA)\n",
    "pca.fit(bee_features)\n",
    "test = pca.transform(bee_features)\n",
    "\n",
    "ax.scatter(xs = test[apiary_a][:,0], ys = test[apiary_a][:,1], zs = test[apiary_a][:,2],c =\"red\")\n",
    "ax.scatter(xs = test[apiary_b][:,0], ys = test[apiary_b][:,1],zs = test[apiary_b][:,2], c =\"orange\")\n",
    "ax.scatter(xs = test[apiary_c][:,0], ys = test[apiary_c][:,1],zs = test[apiary_c][:,2], c =\"yellow\")\n",
    "ax.scatter(xs = test[apiary_d][:,0], ys = test[apiary_d][:,1],zs = test[apiary_d][:,2], c =\"green\")\n",
    "ax.scatter(xs = test[apiary_e][:,0], ys = test[apiary_e][:,1],zs = test[apiary_e][:,2], c =\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = CodaPCA.CodaPCA(1,lrate=1e-3,nn_shape=[100,100], alg=CodaPCA.Alg.CODAPCA)\n",
    "pca.fit(bee_features)\n",
    "test = pca.transform(bee_features)\n",
    "\n",
    "\n",
    "y = np.zeros(len(test[apiary_a]))\n",
    "plt.scatter(test[apiary_a], y = np.zeros(len(test[apiary_a])), c =\"red\")\n",
    "plt.scatter(test[apiary_b], y = np.zeros(len(test[apiary_b])),c =\"orange\")\n",
    "plt.scatter(test[apiary_c],  y = np.zeros(len(test[apiary_c])),c =\"yellow\")\n",
    "plt.scatter(test[apiary_d],  y = np.zeros(len(test[apiary_d])),c =\"green\")\n",
    "plt.scatter(test[apiary_e], y = np.zeros(len(test[apiary_e])), c =\"blue\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv(\"wine.data\", header=None)\n",
    "wine_1 = np.where(wine[0] == 1)\n",
    "wine_2 = np.where(wine[0] == 2)\n",
    "wine_3 = np.where(wine[0] == 3)\n",
    "\n",
    "\n",
    "# adult = data_9.to_numpy()[adult_inds]\n",
    "# child = data_9.to_numpy()[child_inds]\n",
    "features = wine[wine.columns[1:]].to_numpy()\n",
    "\n",
    "features = np.array([feat/sum(feat) for feat in features])\n",
    "\n",
    "\n",
    "\n",
    "# # pca_clr = CodaPCA.CLRPCA(2)\n",
    "# # pca_clr.fit(features)\n",
    "# # test = pca_clr.transform(features)\n",
    "\n",
    "# pca = CodaPCA.CodaPCA(2,lrate=1e-3,nn_shape=[100,100], alg=CodaPCA.Alg.CODAPCA)\n",
    "# pca.fit(features)\n",
    "# test = pca.transform(features)\n",
    "\n",
    "\n",
    "pca_clr = CodaPCA.CLRPCA(2)\n",
    "pca_clr.fit(features)\n",
    "test = pca_clr.transform(features)\n",
    "\n",
    "plt.scatter(x = test[wine_1][:,0], y = test[wine_1][:,1], c =\"red\")\n",
    "plt.scatter(x = test[wine_2][:,0], y = test[wine_2][:,1], c =\"orange\")\n",
    "plt.scatter(x = test[wine_3][:,0], y = test[wine_3][:,1], c =\"blue\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "naive_pca = PCA(2)\n",
    "naive_pca.fit(features)\n",
    "test = pca.transform(features)\n",
    "\n",
    "plt.scatter(x = test[wine_1][:,0], y = test[wine_1][:,1], c =\"red\")\n",
    "plt.scatter(x = test[wine_2][:,0], y = test[wine_2][:,1], c =\"orange\")\n",
    "plt.scatter(x = test[wine_3][:,0], y = test[wine_3][:,1], c =\"blue\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(222, projection='3d')\n",
    "\n",
    "\n",
    "\n",
    "# pca = CodaPCA.CodaPCA(3,lrate=1e-3,nn_shape=[100,100], alg=CodaPCA.Alg.CODAPCA)\n",
    "# pca.fit(features)\n",
    "# test = pca.transform(features)\n",
    "\n",
    "pca = CodaPCA.NonParametricCodaPCA(3)\n",
    "pca.fit(features)\n",
    "test = pca.transform(features)\n",
    "\n",
    "\n",
    "# pca_clr = CodaPCA.CLRPCA(3)\n",
    "# pca_clr.fit(features)\n",
    "# test = pca_clr.transform(features)\n",
    "\n",
    "\n",
    "# naive_pca = PCA(2)\n",
    "# naive_pca.fit(features)\n",
    "# test = pca.transform(features)\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(xs = test[wine_1][:,0], ys = test[wine_1][:,1],zs = test[wine_1][:,1], c =\"red\")\n",
    "ax.scatter(xs= test[wine_2][:,0], ys = test[wine_2][:,1],zs = test[wine_2][:,1], c =\"green\")\n",
    "ax.scatter(xs = test[wine_3][:,0], ys = test[wine_3][:,1],zs = test[wine_3][:,1], c =\"blue\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

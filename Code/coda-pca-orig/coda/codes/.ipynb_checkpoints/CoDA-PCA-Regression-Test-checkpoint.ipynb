{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import CodaPCA\n",
    "import numpy as np\n",
    "from runpca import read_csv\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "#change module for newer sklearn versions\n",
    "from sklearn.cross_validation  import cross_val_score\n",
    "from sklearn.cross_validation  import KFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in the data. Given an array of which files are regression, classification or unlabelled\n",
    "data_path = os.getcwd() + \"\\\\Aitchinson\"\n",
    "\n",
    "regression_list = [3,4,5,18,21,34,39]\n",
    "classification_list = [7,8,9,11,12,16,17,19,23,24,25,26,28,29,33,37]\n",
    "unlabelled_list=[1,2,6,10,13,14,15,20,22,27,30,31,32,35,36,38,40]\n",
    "\n",
    "r_files = []\n",
    "c_files = []\n",
    "u_files = []\n",
    "\n",
    "\n",
    "for file in os.listdir(data_path):\n",
    "    for i in regression_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            r_files.append(\"Aitchinson/\" + file)\n",
    "    for i in classification_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            c_files.append(\"Aitchinson/\" + file)\n",
    "    for i in unlabelled_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            u_files.append(\"Aitchinson/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Aitchinson/Data 18. Compositions and total pebble counts of 92 glacial tills.csv...\n",
      "92 samples 5 features\n",
      "sparsity: 10.434782608695652%\n",
      "[epoch     0] L=  0.9200\n",
      "loading Aitchinson/Data 21. Permeabilities of bayesite for 21 mixtures of fibres and bonding pressures..csv...\n",
      "21 samples 6 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  5.3480\n",
      "loading Aitchinson/Data 3. Compositions and depths of 25 specimens of boxite (Percentages by weight).csv...\n",
      "25 samples 6 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  5.0100\n",
      "loading Aitchinson/Data 34. Foraminiferal compositions at 30 different depths.csv...\n",
      "30 samples 5 features\n",
      "sparsity: 3.3333333333333335%\n",
      "[epoch     0] L=  7.6882\n",
      "loading Aitchinson/Data 39. Microhardness of 18 glass specimens and their (Ge, Sb, Se) compositions.csv...\n",
      "18 samples 4 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  1.8184\n",
      "loading Aitchinson/Data 4. Compositions, depths and porosities of 25 specimens of coxite (Percentages by weight).csv...\n",
      "25 samples 7 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  6.2721\n",
      "loading Aitchinson/Data 5. Sand, silt, clay compositions of 39 sediment samples at different water depths in an Arctic lake.csv...\n",
      "39 samples 4 features\n",
      "sparsity: 0.0%\n",
      "[epoch     0] L=  2.8499\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#need to specify where the the targets and features are in the dataset, and whether there are non compositional features\n",
    "\n",
    "def PCA_Regression(data, co_feature_indices, target_index, \n",
    "                   other_feature_indices = [], alg=CodaPCA.Alg.CODAPCA, verbose=False):\n",
    "    \n",
    "    #can loop through/optimise this in another way?\n",
    "    \n",
    "    headers = data[1]\n",
    "    features = data[0][:,co_feature_indices]\n",
    "    targets = data[0][:,target_index]\n",
    "    \n",
    "    #normalise the compositional features. TODO anything extra to deal with non compositional features?\n",
    "    features = np.array([feat/sum(feat) for feat in features])\n",
    "\n",
    "    #can be empty\n",
    "    extra_features = data[0][:,other_feature_indices]\n",
    "    \n",
    "    #TODO double check this\n",
    "    features = np.hstack([features, extra_features])\n",
    "    \n",
    "    #compute the CoDA-PCA projection \n",
    "    #TODO add component number as a hyperparameter to optimise \n",
    "    n_components=len(co_feature_indices)-2\n",
    "\n",
    "    pca = CodaPCA.CodaPCA(n_components,lrate=1e-3,nn_shape=[100,100], alg=alg)\n",
    "    pca.fit(features)\n",
    "    \n",
    "    Y_coda = pca.project(features)\n",
    "\n",
    "    pca_clr = CodaPCA.CLRPCA(n_components)\n",
    "    pca_clr.fit(features)\n",
    "    \n",
    "    Y_clr = pca_clr.project(features)\n",
    "\n",
    "    lm = Ridge()\n",
    "    #exp the projection to get out of clr space\n",
    "    coda_score = enhanced_cross_val(lm,np.exp(Y_coda), targets)\n",
    "    clr_score = enhanced_cross_val(lm,np.exp(Y_clr), targets) \n",
    "    naive_score = enhanced_cross_val(lm, features, targets)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"CoDA-PCA:\")\n",
    "        print(coda_score)\n",
    "        print(\"CLR-PCA:\")\n",
    "        print(clr_score)\n",
    "        print (\"Naive regression:\")\n",
    "        print (naive_score)\n",
    "    \n",
    "\n",
    "    return coda_score,clr_score,naive_score\n",
    "\n",
    "#training methodology as described in:\n",
    "#https://papers.nips.cc/paper/3215-learning-with-transformation-invariant-kernels.pdf\n",
    "def enhanced_cross_val(model, features, targets):\n",
    "    assert len(features) == len(targets), \"Mismatch in length of features and targets\"\n",
    "    \n",
    "    #define the number of splits and folds uised in the parameter selection\n",
    "    #stick to smaller splits since we have small datasets\n",
    "    splits = 4\n",
    "    param_splits = 3\n",
    "    \n",
    "    #split data \n",
    "    kf = KFold(len(features), n_folds=splits)\n",
    "    kfold_scores = []\n",
    "    for train, test in kf:        \n",
    "        Y_train = targets[train]\n",
    "        X_train = features[train]\n",
    "        \n",
    "       \n",
    "        Y_test = targets[test]\n",
    "        X_test = features[test]\n",
    "        \n",
    "        #inner loop for parameter selection (regularisation term in Ridge Regression):\n",
    "        param_grid = [0.01,0.05,0.1,0.5,1.0,2.0,5.0,10.0,20.0,100.0]\n",
    "        for a in param_grid:\n",
    "            max_score = -np.inf\n",
    "            lm = Ridge(a)\n",
    "   \n",
    "            curr_score = np.mean(cross_val_score(lm, X_train, Y_train,cv=param_splits))\n",
    "            if curr_score > max_score:\n",
    "                max_score = curr_score\n",
    "                best_param = a\n",
    "        \n",
    "        #compute test score based on best parameter\n",
    "        lm = Ridge(best_param)\n",
    "        lm.fit(X_train, Y_train)\n",
    "        kfold_scores.append(lm.score(X_test, Y_test))\n",
    "        \n",
    "    return kfold_scores\n",
    "\n",
    "\n",
    "\n",
    "#can automate this if we had assume a certain structure for the indices of features and targets, or an array per dataset \n",
    "\n",
    "score_dict = {}\n",
    "\n",
    "data_18_scores = PCA_Regression(read_csv(r_files[0], normalize=False), co_feature_indices=[0,1,2,3], target_index=4)\n",
    "#other \"target\" also at 5\n",
    "score_dict['18'] = data_18_scores\n",
    "\n",
    "data_21_scores = PCA_Regression(read_csv(r_files[1], normalize=False), co_feature_indices=[0,1,2,3], target_index=4) \n",
    "score_dict['21'] = data_21_scores\n",
    "\n",
    "data_3_scores = PCA_Regression(read_csv(r_files[2], normalize=False), co_feature_indices=[0,1,2,3,4], target_index=5) \n",
    "score_dict['3'] = data_3_scores\n",
    "\n",
    "data_34_scores = PCA_Regression(read_csv(r_files[3], normalize=False), co_feature_indices=[0,1,2,3], target_index=4)\n",
    "score_dict['34'] = data_34_scores\n",
    "\n",
    "data_39_scores = PCA_Regression(read_csv(r_files[4], normalize=False), co_feature_indices=[0,1,2], target_index=3)\n",
    "score_dict['39'] = data_39_scores\n",
    "\n",
    "data_4_scores = PCA_Regression(read_csv(r_files[5], normalize=False), co_feature_indices=[0,1,2,3,4], target_index=5)\n",
    "score_dict['4'] = data_4_scores\n",
    "\n",
    "data_5_scores =PCA_Regression(read_csv(r_files[6], normalize=False), co_feature_indices=[0,1,2], target_index=3)\n",
    "score_dict['5'] = data_5_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-efb3d46a1c48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscore_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmean_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import plotly\n",
    "for key in score_dict.keys():\n",
    "    mean_scores = map(np.mean, score_dict[key])\n",
    "    print(key + str(list(mean_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes: \n",
    "\n",
    "### Derivation of CoDA-PCA loss: \n",
    "First few steps are easy, to get the answer it is fairly obvious after recognising that $$\\phi(z) = z\\log z - z = \\sum_i z_i \\log z_i - z_i$$ and also: $$\\phi(Z) = \\sum_{i:z_i \\in Z} \\phi(z_i)$$\n",
    "\n",
    "### Implementation:\n",
    "Might be useful to have some kind of explained variance metric for CoDA-PCA ala eigenvalues for standard PCA. Will probably need to derive this, and then add to the package (after PyTorch implementation?) \n",
    "\n",
    "Seems hard to make any model predict well... TODO: check details of the implementation. Could also be that there is no real way to get decent regression results.  \n",
    "\n",
    "Add function for predict given a composition i.e. project onto the new coordinate space then perform regression. \n",
    "\n",
    "Non Parametric Implementation is far easier to understand (not sure how TensorFlow works..), so I think I'm better off reimplementing that. Can do the other algorithms later if we need to.   \n",
    "\n",
    "Check: Seems like the \"Alternating minimisation\" is taking the gradient of the loss with respect to U, and again with respect to B, concatenating these and then optimising the loss based on that. (So U will take a step in the direction of its gradient, B likewise, both at the same time rather than \"alternating\" in different iterations which is what I originally thought). \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

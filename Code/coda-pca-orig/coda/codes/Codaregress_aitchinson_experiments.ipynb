{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 11:45:27.166184 140395222361920 deprecation_wrapper.py:119] From /home/sean/Documents/Honours/honours/Code/coda-pca-orig/coda/codes/CodaPCA.py:14: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W0716 11:45:27.167003 140395222361920 deprecation_wrapper.py:119] From /home/sean/Documents/Honours/honours/Code/coda-pca-orig/coda/codes/CodaPCA.py:14: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import CodaPCA\n",
    "import CodaRegressmb as CodaRegress\n",
    "import numpy as np\n",
    "from runpca import read_csv\n",
    "import os\n",
    "import sklearn\n",
    "import torch\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#change module for newer sklearn versions\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from sklearn.model_selection  import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data. Given an array of which files are regression, classification or unlabelled\n",
    "data_path = os.getcwd() + \"/Aitchinson\"\n",
    "\n",
    "regression_list = [3,4,5,18,21,34,39]\n",
    "classification_list = [7,8,9,11,12,16,17,19,23,24,25,26,28,29,33,37]\n",
    "unlabelled_list=[1,2,6,10,13,14,15,20,22,27,30,31,32,35,36,38,40]\n",
    "\n",
    "r_files = []\n",
    "c_files = []\n",
    "u_files = []\n",
    "\n",
    "\n",
    "for file in os.listdir(data_path):\n",
    "    for i in regression_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            r_files.append(\"Aitchinson/\" + file)\n",
    "    for i in classification_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            c_files.append(\"Aitchinson/\" + file)\n",
    "    for i in unlabelled_list:\n",
    "        if os.path.isfile(os.path.join(data_path,file)) and 'Data ' + str(i) + '.' in file:\n",
    "            u_files.append(\"Aitchinson/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coda_val(features, targets, n_components, folds):\n",
    "    targets = targets.reshape(-1,1)\n",
    "    param_splits = 2\n",
    "    kfold_scores = []\n",
    "    for train, test in folds:        \n",
    "        Y_train = targets[train]\n",
    "        X_train = features[train]\n",
    "        \n",
    "       \n",
    "        Y_test = targets[test]\n",
    "        X_test = features[test]\n",
    "        \n",
    "        \n",
    "        kf_inner = KFold(param_splits)\n",
    "        \n",
    "        inner_folds = [i for i in kf_inner.split(X_train)]      \n",
    "            \n",
    "        #inner loop for parameter selection (lambda term in combined loss):\n",
    "        param_grid = [0.01,0.05,0.1,0.5,1.0,5.0,10.0,50.0,100.0]\n",
    "        \n",
    "        \n",
    "        for a in param_grid:\n",
    "            max_error = np.inf\n",
    "            \n",
    "   \n",
    "            \n",
    "            cval_error = []\n",
    "            #find the parameter which obtains the best inner cross val score\n",
    "            for train_inner, test_inner in inner_folds:\n",
    "                \n",
    "                model = CodaRegress.CoDA_Regress(features.shape[1], n_components, [10,], [2,])\n",
    "                \n",
    "                model.fit(torch.FloatTensor(X_train[train_inner]),  torch.FloatTensor(Y_train[train_inner]), a, lr=1e-2)\n",
    "\n",
    "                y_inner_pred = model.predict(torch.FloatTensor(X_train[test_inner]))\n",
    "    \n",
    "                cval_error.append(sklearn.metrics.mean_squared_error(Y_train[test_inner],y_inner_pred.detach().numpy()))\n",
    "        \n",
    "        \n",
    "                \n",
    "    \n",
    "   \n",
    "            curr_error = np.mean(cval_error)\n",
    "            if curr_error < max_error:\n",
    "                max_error = curr_error\n",
    "                best_param = a\n",
    "                print (\"Current best\", best_param)\n",
    "        \n",
    "        #compute test score based on best parameter\n",
    "        best_model = CodaRegress.CoDA_Regress(features.shape[1], n_components, [10,], [2,])\n",
    "        best_model.fit(torch.FloatTensor(X_train),  torch.FloatTensor(Y_train), a, lr=1e-2)\n",
    "\n",
    "        y_pred = best_model.predict(torch.FloatTensor(X_test))\n",
    "        kfold_scores.append(sklearn.metrics.mean_squared_error(Y_test,y_pred.detach().numpy()))\n",
    "                \n",
    "    return kfold_scores\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#need to specify where the the targets and features are in the dataset, and whether there are non compositional features\n",
    "\n",
    "def PCA_Regression(data, co_feature_indices, target_index, \n",
    "                   other_feature_indices = [], alg=CodaPCA.Alg.CODAPCA, verbose=True):\n",
    "    \n",
    "    #can loop through/optimise this in another way?\n",
    "    \n",
    "#     headers = data[1]\n",
    "#     features = data[0][:,co_feature_indices]\n",
    "#     targets = data[0][:,target_index]\n",
    "    \n",
    "    headers = data\n",
    "    features = data[:,co_feature_indices]\n",
    "    targets = data[:,target_index]\n",
    "    \n",
    "    #normalise the compositional features. TODO anything extra to deal with non compositional features?\n",
    "    features = np.array([feat/sum(feat) for feat in features])\n",
    "\n",
    "    #can be empty\n",
    "    #extra_features = data[0][:,other_feature_indices]\n",
    "    \n",
    "    #TODO double check this\n",
    "    #features = np.hstack([features, extra_features])\n",
    "    \n",
    "    #compute the CoDA-PCA projection \n",
    "    #TODO add component number as a hyperparameter to optimise \n",
    "    n_components=len(co_feature_indices)-2\n",
    "\n",
    "    pca = CodaPCA.CodaPCA(n_components,lrate=1e-4,nn_shape=[100,100], alg=alg)\n",
    "    #TODO: check why this is numerically unstable\n",
    "    #pca = CodaPCA.NonParametricCodaPCA(n_components)\n",
    "\n",
    "    pca.fit(features)\n",
    "    \n",
    "    Y_coda = pca.transform(features)\n",
    "\n",
    "    pca_clr = CodaPCA.CLRPCA(n_components)\n",
    "    pca_clr.fit(features)\n",
    "    \n",
    "    Y_clr = pca_clr.transform(features)\n",
    "    \n",
    "    \n",
    "    splits = 4\n",
    "        \n",
    "    #split data \n",
    "    kf = KFold(splits)\n",
    "        \n",
    "    folds = [i for i in kf.split(features)]      \n",
    "    \n",
    "\n",
    "    lm = Ridge()\n",
    "    \n",
    "    coda_score = enhanced_cross_val(lm,Y_coda, targets, folds)\n",
    "    clr_score = enhanced_cross_val(lm,Y_clr, targets, folds) \n",
    "    naive_score = enhanced_cross_val(lm, features, targets, folds)\n",
    "    \n",
    "    regress_score = coda_val(features, targets, n_components, folds)\n",
    "    \n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"CoDA-PCA:\")\n",
    "        print(coda_score)\n",
    "        print(\"CLR-PCA:\")\n",
    "        print(clr_score)\n",
    "        print (\"Naive regression:\")\n",
    "        print (naive_score)\n",
    "        print (\"CoDA-Regress:\")\n",
    "        print (regress_score)\n",
    "    \n",
    "\n",
    "    return coda_score,clr_score,naive_score,regress_score\n",
    "\n",
    "#training methodology as described in:\n",
    "#https://papers.nips.cc/paper/3215-learning-with-transformation-invariant-kernels.pdf\n",
    "def enhanced_cross_val(model, features, targets, folds):\n",
    "    assert len(features) == len(targets), \"Mismatch in length of features and targets\"\n",
    "    \n",
    "    #define the number of splits and folds uised in the parameter selection\n",
    "    #stick to smaller splits since we have small datasets\n",
    "    #splits = 4\n",
    "    param_splits = 3\n",
    "    #split data \n",
    "    #kf = KFold(splits)\n",
    "    kfold_scores = []\n",
    "    \n",
    "    for train, test in folds:        \n",
    "        Y_train = targets[train]\n",
    "        X_train = features[train]\n",
    "        \n",
    "       \n",
    "        Y_test = targets[test]\n",
    "        X_test = features[test]\n",
    "        \n",
    "        #inner loop for parameter selection (regularisation term in Ridge Regression):\n",
    "        param_grid = [0.01,0.05,0.1,0.5,1.0,5.0,10.0,50.0,100.0]\n",
    "        for a in param_grid:\n",
    "            max_score = -np.inf\n",
    "            lm = Ridge(a)\n",
    "   \n",
    "            curr_score = np.mean(cross_val_score(lm, X_train, Y_train,cv=param_splits))\n",
    "            if curr_score > max_score:\n",
    "                max_score = curr_score\n",
    "                best_param = a\n",
    "        \n",
    "        #compute test score based on best parameter\n",
    "        lm = Ridge(best_param)\n",
    "        lm.fit(X_train, Y_train)\n",
    "        y_pred = lm.predict(X_test)\n",
    "        kfold_scores.append(sklearn.metrics.mean_squared_error(Y_test,y_pred))\n",
    "                \n",
    "    return kfold_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Aitchinson/Data 21. Permeabilities of bayesite for 21 mixtures of fibres and bonding pressures..csv...\n",
      "21 samples 6 features\n",
      "sparsity: 0.0%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-94518485c050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#TODO r_files isn't always consistent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata_21_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA_Regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mco_feature_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mscore_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'21'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_21_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-26a24ceb750a>\u001b[0m in \u001b[0;36mPCA_Regression\u001b[0;34m(data, co_feature_indices, target_index, other_feature_indices, alg, verbose)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mco_feature_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "\n",
    "#can automate this if we had assume a certain structure for the indices of features and targets, or an array per dataset \n",
    "\n",
    "score_dict = {}\n",
    "\n",
    "#TODO r_files isn't always consistent\n",
    "\n",
    "data_21_scores = PCA_Regression(read_csv(r_files[0], normalize=False), co_feature_indices=[0,1,2,3], target_index=4)\n",
    "score_dict['21'] = data_21_scores\n",
    "\n",
    "data_21_scores2 = PCA_Regression(read_csv(r_files[0], normalize=False), co_feature_indices=[0,1,2,3], target_index=5)\n",
    "score_dict['21,2'] = data_21_scores2\n",
    "\n",
    "data_34_scores = PCA_Regression(read_csv(r_files[1], normalize=False), co_feature_indices=[0,1,2,3], target_index=4) \n",
    "score_dict['34'] = data_34_scores\n",
    "\n",
    "data_3_scores = PCA_Regression(read_csv(r_files[2], normalize=False), co_feature_indices=[0,1,2,3,4], target_index=5) \n",
    "score_dict['3'] = data_3_scores\n",
    "\n",
    "data_39_scores = PCA_Regression(read_csv(r_files[3], normalize=False), co_feature_indices=[0,1,2], target_index=3)\n",
    "score_dict['39'] = data_39_scores\n",
    "\n",
    "data_4_scores = PCA_Regression(read_csv(r_files[4], normalize=False), co_feature_indices=[0,1,2,3,4], target_index=5)\n",
    "score_dict['4'] = data_4_scores\n",
    "\n",
    "data_4_scores2 = PCA_Regression(read_csv(r_files[4], normalize=False), co_feature_indices=[0,1,2,3,4], target_index=6)\n",
    "score_dict['4,2'] = data_4_scores2\n",
    "\n",
    "data_5_scores = PCA_Regression(read_csv(r_files[5], normalize=False), co_feature_indices=[0,1,2], target_index=3)\n",
    "score_dict['5'] = data_5_scores\n",
    "\n",
    "data_18_scores = PCA_Regression(read_csv(r_files[6], normalize=False), co_feature_indices=[0,1,2], target_index=3)\n",
    "score_dict['18'] = data_5_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict['5'] = data_5_scores\n",
    "score_dict['18'] = data_18_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#note: plotly code works fine, but gives a jupyter warning when saving with a rendered table\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "table_rows = []\n",
    "for key in score_dict.keys():\n",
    "    mean_scores = list(map(np.mean, score_dict[key]))\n",
    "    table_rows.append([key,*mean_scores])\n",
    "    \n",
    "results = [go.Table(\n",
    "    header=dict(values=[\"Dataset\",\"CoDA-PCA\", \"CLR-PCA\", \"Naive Regression\", \"CoDA-Regress\"]),\n",
    "    cells=dict(values=np.array(table_rows).T))]\n",
    "\n",
    "iplot(results, filename = 'basic_table')    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5 = read_csv(r_files[5], normalize=False)\n",
    "\n",
    "data_5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_5\n",
    "features = data[0][:,[0,1,2]]\n",
    "targets = data[0][:,3]\n",
    "    \n",
    "#normalise the compositional features. TODO anything extra to deal with non compositional features?\n",
    "features = np.array([feat/sum(feat) for feat in features])\n",
    "\n",
    "\n",
    "model = CodaRegress.CoDA_Regress(features.shape[1], 2, [10,], [2,])\n",
    "model.fit(torch.FloatTensor(features[:30]),  torch.FloatTensor(targets[:30].reshape(-1,1)), 100, lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(torch.FloatTensor(features))\n",
    "print (pred)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , 10.        ,  7.        ,  6.49315753],\n",
       "       [ 1.        ,  9.        , 10.        ,  9.92850738],\n",
       "       [ 1.        ,  9.        , 10.        ,  3.28117527],\n",
       "       [ 1.        ,  7.        , 10.        ,  5.55998836],\n",
       "       [ 1.        ,  8.        ,  6.        , 10.52704078],\n",
       "       [ 1.        ,  9.        , 10.        , 11.45608601],\n",
       "       [ 3.        ,  9.        ,  8.        , 11.18129382],\n",
       "       [ 1.        ,  7.        ,  9.        ,  5.82068852],\n",
       "       [ 2.        ,  9.        ,  7.        , 11.73519852],\n",
       "       [ 2.        ,  8.        , 10.        , 11.99193745],\n",
       "       [ 2.        ,  8.        ,  7.        ,  6.08879226],\n",
       "       [ 3.        ,  9.        ,  6.        , 15.46444438],\n",
       "       [ 2.        ,  7.        ,  6.        ,  8.0425786 ],\n",
       "       [ 2.        ,  7.        ,  8.        ,  6.60997232],\n",
       "       [ 2.        ,  6.        ,  7.        ,  7.17303   ],\n",
       "       [ 2.        ,  8.        , 10.        , 14.09949921],\n",
       "       [ 3.        ,  9.        ,  8.        , 13.20600639],\n",
       "       [ 3.        ,  8.        ,  9.        , 13.79625834],\n",
       "       [ 2.        ,  7.        ,  9.        , 13.32740272],\n",
       "       [ 3.        ,  8.        ,  6.        , 13.6999852 ],\n",
       "       [ 3.        ,  8.        ,  9.        , 11.19924614],\n",
       "       [ 4.        ,  8.        ,  8.        , 12.49773227],\n",
       "       [ 4.        ,  8.        ,  7.        , 12.44578593],\n",
       "       [ 3.        ,  7.        ,  6.        , 13.61963408],\n",
       "       [ 4.        ,  5.        , 10.        , 19.27239325],\n",
       "       [ 4.        ,  7.        , 10.        , 17.29459964],\n",
       "       [ 4.        ,  5.        ,  7.        , 17.63734801],\n",
       "       [ 3.        ,  8.        ,  6.        , 15.25432513],\n",
       "       [ 4.        ,  5.        ,  6.        , 16.8504111 ],\n",
       "       [ 5.        ,  5.        ,  8.        , 19.86473249],\n",
       "       [ 5.        ,  4.        ,  6.        , 19.36921476],\n",
       "       [ 5.        ,  5.        ,  9.        , 16.56052347],\n",
       "       [ 5.        ,  7.        , 10.        , 19.2668765 ],\n",
       "       [ 4.        ,  4.        ,  9.        , 19.29727453],\n",
       "       [ 6.        ,  5.        ,  7.        , 22.33866072],\n",
       "       [ 5.        ,  5.        , 10.        , 24.16475208],\n",
       "       [ 5.        ,  6.        ,  7.        , 23.64337052],\n",
       "       [ 5.        ,  5.        ,  7.        , 26.95269511],\n",
       "       [ 6.        ,  4.        ,  8.        , 21.91014076],\n",
       "       [ 5.        ,  6.        ,  7.        , 20.77753044],\n",
       "       [ 5.        ,  4.        ,  6.        , 24.11891416],\n",
       "       [ 5.        ,  5.        , 10.        , 27.39788452],\n",
       "       [ 5.        ,  6.        ,  6.        , 24.88988093],\n",
       "       [ 5.        ,  6.        , 10.        , 23.01114539],\n",
       "       [ 5.        ,  4.        ,  9.        , 28.98087091],\n",
       "       [ 6.        ,  4.        ,  9.        , 29.60808411],\n",
       "       [ 7.        ,  3.        ,  7.        , 29.40817095],\n",
       "       [ 6.        ,  5.        ,  7.        , 27.59689224],\n",
       "       [ 7.        ,  5.        ,  6.        , 30.82966885],\n",
       "       [ 6.        ,  3.        ,  9.        , 29.3249485 ],\n",
       "       [ 7.        ,  3.        , 10.        , 26.67421221],\n",
       "       [ 8.        ,  5.        ,  9.        , 31.73128577],\n",
       "       [ 7.        ,  4.        , 10.        , 35.93795064],\n",
       "       [ 7.        ,  4.        ,  9.        , 31.78451528],\n",
       "       [ 7.        ,  2.        ,  9.        , 31.14281264],\n",
       "       [ 6.        ,  3.        , 10.        , 36.48224137],\n",
       "       [ 7.        ,  4.        ,  6.        , 28.90606079],\n",
       "       [ 7.        ,  3.        , 10.        , 37.51744765],\n",
       "       [ 7.        ,  4.        ,  7.        , 30.38561557],\n",
       "       [ 7.        ,  3.        ,  8.        , 30.66418506],\n",
       "       [ 8.        ,  3.        , 10.        , 36.87508927],\n",
       "       [ 7.        ,  2.        ,  8.        , 39.54303745],\n",
       "       [ 8.        ,  2.        ,  7.        , 36.13103941],\n",
       "       [ 7.        ,  3.        , 10.        , 32.70177029],\n",
       "       [ 9.        ,  1.        ,  8.        , 32.56989144],\n",
       "       [ 8.        ,  3.        , 10.        , 37.51339643],\n",
       "       [ 8.        ,  3.        ,  7.        , 38.11549966],\n",
       "       [ 9.        ,  2.        ,  8.        , 41.19682988],\n",
       "       [ 7.        ,  2.        ,  7.        , 41.39938016],\n",
       "       [ 8.        ,  3.        ,  9.        , 42.17926864],\n",
       "       [ 8.        ,  0.        ,  6.        , 39.77945429],\n",
       "       [ 9.        ,  1.        ,  9.        , 45.34560979],\n",
       "       [ 8.        ,  2.        ,  9.        , 36.93867133],\n",
       "       [ 9.        ,  1.        ,  7.        , 42.12304003],\n",
       "       [ 8.        ,  2.        ,  6.        , 39.49952524],\n",
       "       [10.        ,  1.        , 10.        , 39.01660181],\n",
       "       [ 9.        ,  2.        ,  8.        , 45.16100417],\n",
       "       [10.        ,  1.        , 10.        , 43.67087974],\n",
       "       [10.        ,  2.        ,  7.        , 42.26352176],\n",
       "       [10.        ,  2.        , 10.        , 47.83705227],\n",
       "       [11.        ,  0.        ,  8.        , 41.44978278],\n",
       "       [10.        ,  1.        ,  9.        , 41.36571516],\n",
       "       [10.        ,  2.        ,  8.        , 41.55773237],\n",
       "       [ 9.        ,  1.        ,  7.        , 51.64097156],\n",
       "       [10.        ,  1.        ,  6.        , 46.11051937],\n",
       "       [10.        ,  1.        ,  8.        , 43.90813464],\n",
       "       [11.        ,  0.        ,  6.        , 53.11334346],\n",
       "       [11.        ,  1.        ,  6.        , 53.43666092],\n",
       "       [10.        ,  1.        ,  9.        , 44.65885364],\n",
       "       [11.        ,  1.        ,  7.        , 45.12664618],\n",
       "       [11.        ,  1.        , 10.        , 48.04577059],\n",
       "       [11.        ,  0.        ,  7.        , 52.29120259],\n",
       "       [11.        ,  1.        ,  7.        , 53.59494871],\n",
       "       [12.        ,  0.        ,  9.        , 55.73490279],\n",
       "       [12.        ,  0.        , 10.        , 47.52154546],\n",
       "       [11.        ,  0.        ,  7.        , 54.06673494],\n",
       "       [11.        ,  0.        ,  6.        , 53.31863331],\n",
       "       [11.        ,  1.        ,  9.        , 55.09381059],\n",
       "       [11.        ,  0.        , 10.        , 53.41828858],\n",
       "       [11.        ,  0.        , 10.        , 59.32775483]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100\n",
    "\n",
    "x_1 = np.ceil(np.linspace(0,10,n) + 2*np.random.rand(n))\n",
    "x_2 = np.ceil(np.clip(0, np.linspace(0,10,n) - 3*np.random.rand(n), 10)[::-1])\n",
    "x_3 = np.ceil(5* np.ones(n) + 5*np.random.rand(n))\n",
    "\n",
    "X = np.vstack([x_1, x_2, x_3])\n",
    "X = [[X[0][i], X[1][i], X[2][i]] for i in range(0,X.shape[1])]\n",
    "Y = np.linspace(0,50, n) + 10*np.random.rand(n)\n",
    "Y = Y.reshape(-1,1)\n",
    "data = np.hstack((X,Y))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch     0] L=  3.8219\n",
      "epoch 1000, loss -167412800.0\n",
      "epoch 2000, loss -167413456.0\n",
      "epoch 3000, loss -167413744.0\n",
      "epoch 4000, loss -167413952.0\n",
      "epoch 5000, loss -167414048.0\n",
      "epoch 6000, loss -167414128.0\n",
      "epoch 7000, loss -167414128.0\n",
      "epoch 8000, loss -167414144.0\n",
      "epoch 9000, loss -167413824.0\n",
      "epoch 10000, loss -167414144.0\n",
      "epoch 1000, loss 156.55712890625\n",
      "epoch 2000, loss 49.288753509521484\n",
      "epoch 3000, loss 44.14034652709961\n",
      "epoch 4000, loss 18.136119842529297\n",
      "epoch 5000, loss 15.124115943908691\n",
      "epoch 6000, loss 14.37248420715332\n",
      "epoch 7000, loss 14.054534912109375\n",
      "epoch 8000, loss 13.885627746582031\n",
      "epoch 9000, loss 13.78336238861084\n",
      "epoch 10000, loss 13.722427368164062\n",
      "Current best 0.01\n",
      "epoch 1000, loss -837070080.0\n",
      "epoch 2000, loss -837070464.0\n",
      "epoch 3000, loss -837070784.0\n",
      "epoch 4000, loss -836963840.0\n",
      "epoch 5000, loss -837066880.0\n",
      "epoch 6000, loss -837070976.0\n",
      "epoch 7000, loss -837062784.0\n",
      "epoch 8000, loss -837070976.0\n",
      "epoch 9000, loss -837059456.0\n",
      "epoch 10000, loss -837068160.0\n",
      "epoch 1000, loss 18.04732894897461\n",
      "epoch 2000, loss 17.99960708618164\n",
      "epoch 3000, loss 17.988754272460938\n",
      "epoch 4000, loss 17.985668182373047\n",
      "epoch 5000, loss 17.984771728515625\n",
      "epoch 6000, loss 17.98468017578125\n",
      "epoch 7000, loss 17.984678268432617\n",
      "epoch 8000, loss 17.984682083129883\n",
      "epoch 9000, loss 17.98468780517578\n",
      "epoch 10000, loss 17.984760284423828\n",
      "Current best 0.05\n",
      "epoch 1000, loss -1674140928.0\n",
      "epoch 2000, loss -1674141184.0\n",
      "epoch 3000, loss -1674141184.0\n",
      "epoch 4000, loss -1674141312.0\n",
      "epoch 5000, loss -1674141568.0\n",
      "epoch 6000, loss -1674141440.0\n",
      "epoch 7000, loss -1674141568.0\n",
      "epoch 8000, loss -1674141568.0\n",
      "epoch 9000, loss -1674141568.0\n",
      "epoch 10000, loss -1674141568.0\n",
      "epoch 1000, loss 155.8935546875\n",
      "epoch 2000, loss 58.180721282958984\n",
      "epoch 3000, loss 54.02608108520508\n",
      "epoch 4000, loss 26.52124786376953\n",
      "epoch 5000, loss 24.641698837280273\n",
      "epoch 6000, loss 24.02694320678711\n",
      "epoch 7000, loss 23.74437713623047\n",
      "epoch 8000, loss 23.5902156829834\n",
      "epoch 9000, loss 23.495872497558594\n",
      "epoch 10000, loss 23.43267822265625\n",
      "Current best 0.1\n",
      "epoch 1000, loss -8370706944.0\n",
      "epoch 2000, loss -8370708480.0\n",
      "epoch 3000, loss -8370706944.0\n",
      "epoch 4000, loss -8370709504.0\n",
      "epoch 5000, loss -8370708992.0\n",
      "epoch 6000, loss -8370708480.0\n",
      "epoch 7000, loss -8370708480.0\n",
      "epoch 8000, loss -8370708480.0\n",
      "epoch 9000, loss -8370708480.0\n",
      "epoch 10000, loss -8370579968.0\n",
      "epoch 1000, loss 66.59095764160156\n",
      "epoch 2000, loss 66.56320190429688\n",
      "epoch 3000, loss 66.55274200439453\n",
      "epoch 4000, loss 66.54820251464844\n",
      "epoch 5000, loss 66.547119140625\n",
      "epoch 6000, loss 66.54703521728516\n",
      "epoch 7000, loss 66.54704284667969\n",
      "epoch 8000, loss 66.54702758789062\n",
      "epoch 9000, loss 66.54704284667969\n",
      "epoch 10000, loss 66.54703521728516\n",
      "Current best 0.5\n",
      "epoch 1000, loss -16741372928.0\n",
      "epoch 2000, loss -16741417984.0\n",
      "epoch 3000, loss -16741416960.0\n",
      "epoch 4000, loss -16741383168.0\n",
      "epoch 5000, loss -16737551360.0\n",
      "epoch 6000, loss -16741273600.0\n",
      "epoch 7000, loss -16741167104.0\n",
      "epoch 8000, loss -16741414912.0\n",
      "epoch 9000, loss -16741392384.0\n",
      "epoch 10000, loss -16741393408.0\n",
      "epoch 1000, loss 250.61070251464844\n",
      "epoch 2000, loss 156.82810974121094\n",
      "epoch 3000, loss 153.0709228515625\n",
      "epoch 4000, loss 123.78802490234375\n",
      "epoch 5000, loss 121.82347869873047\n",
      "epoch 6000, loss 121.21468353271484\n",
      "epoch 7000, loss 120.89662170410156\n",
      "epoch 8000, loss 120.7359848022461\n",
      "epoch 9000, loss 120.63601684570312\n",
      "epoch 10000, loss 120.57426452636719\n",
      "Current best 1.0\n",
      "epoch 1000, loss -83706994688.0\n",
      "epoch 2000, loss -83707109376.0\n",
      "epoch 3000, loss -83707117568.0\n",
      "epoch 4000, loss -83655606272.0\n",
      "epoch 5000, loss -83707052032.0\n",
      "epoch 6000, loss -83706937344.0\n",
      "epoch 7000, loss -83677118464.0\n",
      "epoch 8000, loss -83707125760.0\n",
      "epoch 9000, loss -83704610816.0\n",
      "epoch 10000, loss -83706986496.0\n",
      "epoch 1000, loss 552.205810546875\n",
      "epoch 2000, loss 552.1116333007812\n",
      "epoch 3000, loss 552.0551147460938\n",
      "epoch 4000, loss 552.0460815429688\n",
      "epoch 5000, loss 552.0404663085938\n",
      "epoch 6000, loss 552.04052734375\n",
      "epoch 7000, loss 552.0404052734375\n",
      "epoch 8000, loss 552.04052734375\n",
      "epoch 9000, loss 552.0405883789062\n",
      "epoch 10000, loss 552.0415649414062\n",
      "Current best 5.0\n",
      "epoch 1000, loss -167413317632.0\n",
      "epoch 2000, loss -167414104064.0\n",
      "epoch 3000, loss -167414185984.0\n",
      "epoch 4000, loss -167414153216.0\n",
      "epoch 5000, loss -167414153216.0\n",
      "epoch 6000, loss -167400898560.0\n",
      "epoch 7000, loss -167414104064.0\n",
      "epoch 8000, loss -167412989952.0\n",
      "epoch 9000, loss -167413989376.0\n",
      "epoch 10000, loss -167408648192.0\n",
      "epoch 1000, loss 1246.8616943359375\n",
      "epoch 2000, loss 1147.74169921875\n",
      "epoch 3000, loss 1143.4793701171875\n",
      "epoch 4000, loss 1102.5875244140625\n",
      "epoch 5000, loss 1093.8509521484375\n",
      "epoch 6000, loss 1092.5732421875\n",
      "epoch 7000, loss 1092.0888671875\n",
      "epoch 8000, loss 1091.9180908203125\n",
      "epoch 9000, loss 1091.68798828125\n",
      "epoch 10000, loss 1091.656494140625\n",
      "Current best 10.0\n",
      "epoch 1000, loss -837070880768.0\n",
      "epoch 2000, loss -837070880768.0\n",
      "epoch 3000, loss -837070880768.0\n",
      "epoch 4000, loss -837070880768.0\n",
      "epoch 5000, loss -837070880768.0\n",
      "epoch 6000, loss -837070815232.0\n",
      "epoch 7000, loss -837070815232.0\n",
      "epoch 8000, loss -837070684160.0\n",
      "epoch 9000, loss -837070815232.0\n",
      "epoch 10000, loss -837070880768.0\n",
      "epoch 1000, loss 5531.3154296875\n",
      "epoch 2000, loss 5443.43603515625\n",
      "epoch 3000, loss 5439.01806640625\n",
      "epoch 4000, loss 5437.69970703125\n",
      "epoch 5000, loss 5437.16162109375\n",
      "epoch 6000, loss 5436.2880859375\n",
      "epoch 7000, loss 5436.2919921875\n",
      "epoch 8000, loss 5435.375\n",
      "epoch 9000, loss 5435.44384765625\n",
      "epoch 10000, loss 5434.41748046875\n",
      "Current best 50.0\n",
      "epoch 1000, loss -1674140712960.0\n",
      "epoch 2000, loss -1674141892608.0\n",
      "epoch 3000, loss -1674141761536.0\n",
      "epoch 4000, loss -1674141761536.0\n",
      "epoch 5000, loss -1674141761536.0\n",
      "epoch 6000, loss -1674141761536.0\n",
      "epoch 7000, loss -1673948430336.0\n",
      "epoch 8000, loss -1674117382144.0\n",
      "epoch 9000, loss -1674141761536.0\n",
      "epoch 10000, loss -1674139795456.0\n",
      "epoch 1000, loss 10982.505859375\n",
      "epoch 2000, loss 10832.7939453125\n",
      "epoch 3000, loss 10817.3798828125\n",
      "epoch 4000, loss 10809.4111328125\n",
      "epoch 5000, loss 10801.9267578125\n",
      "epoch 6000, loss 10799.4375\n",
      "epoch 7000, loss 10798.2822265625\n",
      "epoch 8000, loss 10797.3662109375\n",
      "epoch 9000, loss 10796.1337890625\n",
      "epoch 10000, loss 10795.2763671875\n",
      "Current best 100.0\n",
      "epoch 1000, loss -1674135732224.0\n",
      "epoch 2000, loss -1674141761536.0\n",
      "epoch 3000, loss -1674139271168.0\n",
      "epoch 4000, loss -1674140712960.0\n",
      "epoch 5000, loss -1674141499392.0\n",
      "epoch 6000, loss -1674141106176.0\n",
      "epoch 7000, loss -1673461628928.0\n",
      "epoch 8000, loss -1674112401408.0\n",
      "epoch 9000, loss -1674129833984.0\n",
      "epoch 10000, loss -1674140975104.0\n",
      "epoch 1000, loss -167413648.0\n",
      "epoch 2000, loss -167413808.0\n",
      "epoch 3000, loss -167413936.0\n",
      "epoch 4000, loss -167413984.0\n",
      "epoch 5000, loss -167414112.0\n",
      "epoch 6000, loss -167365328.0\n",
      "epoch 7000, loss -167413904.0\n",
      "epoch 8000, loss -167414096.0\n",
      "epoch 9000, loss -167413936.0\n",
      "epoch 10000, loss -167413824.0\n",
      "epoch 1000, loss 138.34423828125\n",
      "epoch 2000, loss 130.02047729492188\n",
      "epoch 3000, loss 16.178211212158203\n",
      "epoch 4000, loss 9.920129776000977\n",
      "epoch 5000, loss 9.428921699523926\n",
      "epoch 6000, loss 9.364636421203613\n",
      "epoch 7000, loss 9.357010841369629\n",
      "epoch 8000, loss 9.351712226867676\n",
      "epoch 9000, loss 9.34533405303955\n",
      "epoch 10000, loss 9.33376407623291\n",
      "Current best 0.01\n",
      "epoch 1000, loss -837069440.0\n",
      "epoch 2000, loss -837070016.0\n",
      "epoch 3000, loss -837070336.0\n",
      "epoch 4000, loss -837070592.0\n",
      "epoch 5000, loss -837070720.0\n",
      "epoch 6000, loss -837070784.0\n",
      "epoch 7000, loss -837070784.0\n",
      "epoch 8000, loss -837070848.0\n",
      "epoch 9000, loss -837070784.0\n",
      "epoch 10000, loss -837070848.0\n",
      "epoch 1000, loss 15.885017395019531\n",
      "epoch 2000, loss 15.724721908569336\n",
      "epoch 3000, loss 15.701175689697266\n",
      "epoch 4000, loss 15.700942993164062\n",
      "epoch 5000, loss 15.70094108581543\n",
      "epoch 6000, loss 15.700943946838379\n",
      "epoch 7000, loss 15.700944900512695\n",
      "epoch 8000, loss 15.700944900512695\n",
      "epoch 9000, loss 15.700942993164062\n",
      "epoch 10000, loss 15.700943946838379\n",
      "Current best 0.05\n",
      "epoch 1000, loss -1674139008.0\n",
      "epoch 2000, loss -1674142208.0\n",
      "epoch 3000, loss -1674142336.0\n",
      "epoch 4000, loss -1674141824.0\n",
      "epoch 5000, loss -1674142592.0\n",
      "epoch 6000, loss -1674008960.0\n",
      "epoch 7000, loss -1672964352.0\n",
      "epoch 8000, loss -1674063104.0\n",
      "epoch 9000, loss -1674141440.0\n",
      "epoch 10000, loss -1674101376.0\n",
      "epoch 1000, loss 148.92665100097656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2000, loss 54.58909606933594\n",
      "epoch 3000, loss 21.111629486083984\n",
      "epoch 4000, loss 18.425792694091797\n",
      "epoch 5000, loss 18.10063934326172\n",
      "epoch 6000, loss 18.050552368164062\n",
      "epoch 7000, loss 18.042776107788086\n",
      "epoch 8000, loss 18.038734436035156\n",
      "epoch 9000, loss 18.031999588012695\n",
      "epoch 10000, loss 18.019657135009766\n",
      "Current best 0.1\n",
      "epoch 1000, loss -8370705920.0\n",
      "epoch 2000, loss -8370706432.0\n",
      "epoch 3000, loss -8370707456.0\n",
      "epoch 4000, loss -8370707456.0\n",
      "epoch 5000, loss -8370707968.0\n",
      "epoch 6000, loss -8370707968.0\n",
      "epoch 7000, loss -8370708480.0\n",
      "epoch 8000, loss -8370707456.0\n",
      "epoch 9000, loss -8370707456.0\n",
      "epoch 10000, loss -8370707968.0\n",
      "epoch 1000, loss 192.81190490722656\n",
      "epoch 2000, loss 127.7292251586914\n",
      "epoch 3000, loss 61.376548767089844\n",
      "epoch 4000, loss 57.164520263671875\n",
      "epoch 5000, loss 56.72965621948242\n",
      "epoch 6000, loss 56.663814544677734\n",
      "epoch 7000, loss 56.65459442138672\n",
      "epoch 8000, loss 56.64927291870117\n",
      "epoch 9000, loss 56.64261245727539\n",
      "epoch 10000, loss 56.630615234375\n",
      "Current best 0.5\n",
      "epoch 1000, loss -16741372928.0\n",
      "epoch 2000, loss -16741413888.0\n",
      "epoch 3000, loss -16741416960.0\n",
      "epoch 4000, loss -16741417984.0\n",
      "epoch 5000, loss -16741404672.0\n",
      "epoch 6000, loss -16738562048.0\n",
      "epoch 7000, loss -16741292032.0\n",
      "epoch 8000, loss -16740813824.0\n",
      "epoch 9000, loss -16741384192.0\n",
      "epoch 10000, loss -16741412864.0\n",
      "epoch 1000, loss 248.1092071533203\n",
      "epoch 2000, loss 234.5092010498047\n",
      "epoch 3000, loss 116.40167236328125\n",
      "epoch 4000, loss 105.78697967529297\n",
      "epoch 5000, loss 105.03736114501953\n",
      "epoch 6000, loss 104.9334716796875\n",
      "epoch 7000, loss 104.91697692871094\n",
      "epoch 8000, loss 104.91288757324219\n",
      "epoch 9000, loss 104.90805053710938\n",
      "epoch 10000, loss 104.89971923828125\n",
      "Current best 1.0\n",
      "epoch 1000, loss -83707084800.0\n",
      "epoch 2000, loss -83707084800.0\n",
      "epoch 3000, loss -83707076608.0\n",
      "epoch 4000, loss -83707076608.0\n",
      "epoch 5000, loss -83707084800.0\n",
      "epoch 6000, loss -83707084800.0\n",
      "epoch 7000, loss -83707084800.0\n",
      "epoch 8000, loss -83707076608.0\n",
      "epoch 9000, loss -83706642432.0\n",
      "epoch 10000, loss -83707084800.0\n",
      "epoch 1000, loss 492.81292724609375\n",
      "epoch 2000, loss 492.34747314453125\n",
      "epoch 3000, loss 492.3425598144531\n",
      "epoch 4000, loss 492.34246826171875\n",
      "epoch 5000, loss 492.3446350097656\n",
      "epoch 6000, loss 492.34259033203125\n",
      "epoch 7000, loss 492.34259033203125\n",
      "epoch 8000, loss 492.3426513671875\n",
      "epoch 9000, loss 492.3425598144531\n",
      "epoch 10000, loss 492.34332275390625\n",
      "Current best 5.0\n",
      "epoch 1000, loss -167413628928.0\n",
      "epoch 2000, loss -167414169600.0\n",
      "epoch 3000, loss -167414153216.0\n",
      "epoch 4000, loss -167414136832.0\n",
      "epoch 5000, loss -167414169600.0\n",
      "epoch 6000, loss -167413317632.0\n",
      "epoch 7000, loss -167396491264.0\n",
      "epoch 8000, loss -167414136832.0\n",
      "epoch 9000, loss -167414005760.0\n",
      "epoch 10000, loss -167413727232.0\n",
      "epoch 1000, loss 973.723388671875\n",
      "epoch 2000, loss 973.3404541015625\n",
      "epoch 3000, loss 973.3404541015625\n",
      "epoch 4000, loss 973.343017578125\n",
      "epoch 5000, loss 973.3405151367188\n",
      "epoch 6000, loss 973.34033203125\n",
      "epoch 7000, loss 973.3412475585938\n",
      "epoch 8000, loss 973.3403930664062\n",
      "epoch 9000, loss 973.3402099609375\n",
      "epoch 10000, loss 973.4336547851562\n",
      "Current best 10.0\n",
      "epoch 1000, loss -837070815232.0\n",
      "epoch 2000, loss -837070880768.0\n",
      "epoch 3000, loss -836979785728.0\n",
      "epoch 4000, loss -837070749696.0\n",
      "epoch 5000, loss -837067472896.0\n",
      "epoch 6000, loss -837070815232.0\n",
      "epoch 7000, loss -837069111296.0\n",
      "epoch 8000, loss -837070749696.0\n",
      "epoch 9000, loss -837070946304.0\n",
      "epoch 10000, loss -837070880768.0\n",
      "epoch 1000, loss 4817.66796875\n",
      "epoch 2000, loss 4817.60302734375\n",
      "epoch 3000, loss 4817.51123046875\n",
      "epoch 4000, loss 4818.25146484375\n",
      "epoch 5000, loss 4817.1796875\n",
      "epoch 6000, loss 4817.12353515625\n",
      "epoch 7000, loss 4817.08203125\n",
      "epoch 8000, loss 4817.04541015625\n",
      "epoch 9000, loss 4816.9541015625\n",
      "epoch 10000, loss 4816.857421875\n",
      "Current best 50.0\n",
      "epoch 1000, loss -1674137829376.0\n",
      "epoch 2000, loss -1674141892608.0\n",
      "epoch 3000, loss -1674141892608.0\n",
      "epoch 4000, loss -1674091823104.0\n",
      "epoch 5000, loss -1674129047552.0\n",
      "epoch 6000, loss -1673986310144.0\n",
      "epoch 7000, loss -1674142154752.0\n",
      "epoch 8000, loss -1674087890944.0\n",
      "epoch 9000, loss -1673428729856.0\n",
      "epoch 10000, loss -1674132193280.0\n",
      "epoch 1000, loss 9715.0517578125\n",
      "epoch 2000, loss 9641.5361328125\n",
      "epoch 3000, loss 9626.8017578125\n",
      "epoch 4000, loss 9623.451171875\n",
      "epoch 5000, loss 9622.3564453125\n",
      "epoch 6000, loss 9621.5615234375\n",
      "epoch 7000, loss 9621.10546875\n",
      "epoch 8000, loss 9620.662109375\n",
      "epoch 9000, loss 9620.4111328125\n",
      "epoch 10000, loss 9620.2177734375\n",
      "Current best 100.0\n",
      "epoch 1000, loss -1674140319744.0\n",
      "epoch 2000, loss -1674143072256.0\n",
      "epoch 3000, loss -1674143072256.0\n",
      "epoch 4000, loss -1674142023680.0\n",
      "epoch 5000, loss -1674141761536.0\n",
      "epoch 6000, loss -1674081992704.0\n",
      "epoch 7000, loss -1674117382144.0\n",
      "epoch 8000, loss -1674142810112.0\n",
      "epoch 9000, loss -1674142941184.0\n",
      "epoch 10000, loss -1674112532480.0\n",
      "epoch 1000, loss -150675712.0\n",
      "epoch 2000, loss -150676384.0\n",
      "epoch 3000, loss -150676512.0\n",
      "epoch 4000, loss -150664512.0\n",
      "epoch 5000, loss -150675488.0\n",
      "epoch 6000, loss -150649424.0\n",
      "epoch 7000, loss -150676368.0\n",
      "epoch 8000, loss -150676016.0\n",
      "epoch 9000, loss -150676528.0\n",
      "epoch 10000, loss -150672800.0\n",
      "epoch 1000, loss 9.526739120483398\n",
      "epoch 2000, loss 9.256719589233398\n",
      "epoch 3000, loss 9.242385864257812\n",
      "epoch 4000, loss 9.242385864257812\n",
      "epoch 5000, loss 9.242387771606445\n",
      "epoch 6000, loss 9.242386817932129\n",
      "epoch 7000, loss 9.242384910583496\n",
      "epoch 8000, loss 9.242384910583496\n",
      "epoch 9000, loss 9.243154525756836\n",
      "epoch 10000, loss 9.242415428161621\n",
      "Current best 0.01\n",
      "epoch 1000, loss -753382528.0\n",
      "epoch 2000, loss -753382976.0\n",
      "epoch 3000, loss -753383232.0\n",
      "epoch 4000, loss -753383360.0\n",
      "epoch 5000, loss -753383424.0\n",
      "epoch 6000, loss -753383424.0\n",
      "epoch 7000, loss -753383488.0\n",
      "epoch 8000, loss -753383552.0\n",
      "epoch 9000, loss -753383488.0\n",
      "epoch 10000, loss -753383360.0\n",
      "epoch 1000, loss 13.478019714355469\n",
      "epoch 2000, loss 13.21531867980957\n",
      "epoch 3000, loss 13.169330596923828\n",
      "epoch 4000, loss 13.169301986694336\n",
      "epoch 5000, loss 13.169301986694336\n",
      "epoch 6000, loss 13.169301986694336\n",
      "epoch 7000, loss 13.169301986694336\n",
      "epoch 8000, loss 13.169302940368652\n",
      "epoch 9000, loss 13.169303894042969\n",
      "epoch 10000, loss 13.169302940368652\n",
      "Current best 0.05\n",
      "epoch 1000, loss -1506765952.0\n",
      "epoch 2000, loss -1506766592.0\n",
      "epoch 3000, loss -1506766848.0\n",
      "epoch 4000, loss -1506766848.0\n",
      "epoch 5000, loss -1506767104.0\n",
      "epoch 6000, loss -1506767104.0\n",
      "epoch 7000, loss -1506767232.0\n",
      "epoch 8000, loss -1506766976.0\n",
      "epoch 9000, loss -1506767232.0\n",
      "epoch 10000, loss -1506766976.0\n",
      "epoch 1000, loss 18.30597686767578\n",
      "epoch 2000, loss 18.0855712890625\n",
      "epoch 3000, loss 18.077802658081055\n",
      "epoch 4000, loss 18.077804565429688\n",
      "epoch 5000, loss 18.077804565429688\n",
      "epoch 6000, loss 18.077800750732422\n",
      "epoch 7000, loss 18.077804565429688\n",
      "epoch 8000, loss 18.077802658081055\n",
      "epoch 9000, loss 18.07781982421875\n",
      "epoch 10000, loss 18.077804565429688\n",
      "Current best 0.1\n",
      "epoch 1000, loss -7533831680.0\n",
      "epoch 2000, loss -7533834240.0\n",
      "epoch 3000, loss -7533836288.0\n",
      "epoch 4000, loss -7533830656.0\n",
      "epoch 5000, loss -7533834240.0\n",
      "epoch 6000, loss -7533833216.0\n",
      "epoch 7000, loss -7533807104.0\n",
      "epoch 8000, loss -7533834752.0\n",
      "epoch 9000, loss -7532735488.0\n",
      "epoch 10000, loss -7533833728.0\n",
      "epoch 1000, loss 83.38026428222656\n",
      "epoch 2000, loss 82.67178344726562\n",
      "epoch 3000, loss 82.67155456542969\n",
      "epoch 4000, loss 82.67147827148438\n",
      "epoch 5000, loss 82.67144012451172\n",
      "epoch 6000, loss 82.67141723632812\n",
      "epoch 7000, loss 82.67141723632812\n",
      "epoch 8000, loss 82.6714096069336\n",
      "epoch 9000, loss 82.6714096069336\n",
      "epoch 10000, loss 82.67138671875\n",
      "Current best 0.5\n",
      "epoch 1000, loss -15067669504.0\n",
      "epoch 2000, loss -15067669504.0\n",
      "epoch 3000, loss -15067670528.0\n",
      "epoch 4000, loss -15067669504.0\n",
      "epoch 5000, loss -15067670528.0\n",
      "epoch 6000, loss -15067671552.0\n",
      "epoch 7000, loss -15067671552.0\n",
      "epoch 8000, loss -15067670528.0\n",
      "epoch 9000, loss -15067669504.0\n",
      "epoch 10000, loss -15067668480.0\n",
      "epoch 1000, loss 135.4017791748047\n",
      "epoch 2000, loss 133.84178161621094\n",
      "epoch 3000, loss 133.8415069580078\n",
      "epoch 4000, loss 133.84140014648438\n",
      "epoch 5000, loss 133.84136962890625\n",
      "epoch 6000, loss 133.8413543701172\n",
      "epoch 7000, loss 133.84136962890625\n",
      "epoch 8000, loss 133.84146118164062\n",
      "epoch 9000, loss 133.84136962890625\n",
      "epoch 10000, loss 133.84133911132812\n",
      "Current best 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000, loss -75338358784.0\n",
      "epoch 2000, loss -75338350592.0\n",
      "epoch 3000, loss -75338350592.0\n",
      "epoch 4000, loss -75337170944.0\n",
      "epoch 5000, loss -75338342400.0\n",
      "epoch 6000, loss -75338096640.0\n",
      "epoch 7000, loss -75320877056.0\n",
      "epoch 8000, loss -75337785344.0\n",
      "epoch 9000, loss -75338358784.0\n",
      "epoch 10000, loss -75337940992.0\n",
      "epoch 1000, loss 498.6622619628906\n",
      "epoch 2000, loss 498.6347351074219\n",
      "epoch 3000, loss 498.6337890625\n",
      "epoch 4000, loss 498.63385009765625\n",
      "epoch 5000, loss 498.7197265625\n",
      "epoch 6000, loss 498.63385009765625\n",
      "epoch 7000, loss 498.6338195800781\n",
      "epoch 8000, loss 498.6338195800781\n",
      "epoch 9000, loss 498.6339111328125\n",
      "epoch 10000, loss 498.7012939453125\n",
      "Current best 5.0\n",
      "epoch 1000, loss -150676701184.0\n",
      "epoch 2000, loss -150676783104.0\n",
      "epoch 3000, loss -150676750336.0\n",
      "epoch 4000, loss -150676783104.0\n",
      "epoch 5000, loss -150676783104.0\n",
      "epoch 6000, loss -150676471808.0\n",
      "epoch 7000, loss -150674276352.0\n",
      "epoch 8000, loss -150674538496.0\n",
      "epoch 9000, loss -150673752064.0\n",
      "epoch 10000, loss -150674817024.0\n",
      "epoch 1000, loss 1019.0392456054688\n",
      "epoch 2000, loss 1015.588134765625\n",
      "epoch 3000, loss 1014.1688232421875\n",
      "epoch 4000, loss 1014.935302734375\n",
      "epoch 5000, loss 1011.3362426757812\n",
      "epoch 6000, loss 1008.2327880859375\n",
      "epoch 7000, loss 1000.8322143554688\n",
      "epoch 8000, loss 989.0238647460938\n",
      "epoch 9000, loss 988.6674194335938\n",
      "epoch 10000, loss 988.571533203125\n",
      "Current best 10.0\n",
      "epoch 1000, loss -753383505920.0\n",
      "epoch 2000, loss -753383505920.0\n",
      "epoch 3000, loss -753383505920.0\n",
      "epoch 4000, loss -753383505920.0\n",
      "epoch 5000, loss -753383505920.0\n",
      "epoch 6000, loss -753383571456.0\n",
      "epoch 7000, loss -753383571456.0\n",
      "epoch 8000, loss -753383505920.0\n",
      "epoch 9000, loss -753383374848.0\n",
      "epoch 10000, loss -753383505920.0\n",
      "epoch 1000, loss 4938.982421875\n",
      "epoch 2000, loss 4934.18505859375\n",
      "epoch 3000, loss 4930.86865234375\n",
      "epoch 4000, loss 4928.65185546875\n",
      "epoch 5000, loss 4925.80859375\n",
      "epoch 6000, loss 4919.9365234375\n",
      "epoch 7000, loss 4905.81201171875\n",
      "epoch 8000, loss 4905.78564453125\n",
      "epoch 9000, loss 4905.7861328125\n",
      "epoch 10000, loss 4905.7861328125\n",
      "Current best 50.0\n",
      "epoch 1000, loss -1506767011840.0\n",
      "epoch 2000, loss -1506767011840.0\n",
      "epoch 3000, loss -1506767142912.0\n",
      "epoch 4000, loss -1506767011840.0\n",
      "epoch 5000, loss -1506767011840.0\n",
      "epoch 6000, loss -1506767011840.0\n",
      "epoch 7000, loss -1506767142912.0\n",
      "epoch 8000, loss -1506748661760.0\n",
      "epoch 9000, loss -1506766094336.0\n",
      "epoch 10000, loss -1506767011840.0\n",
      "epoch 1000, loss 9801.716796875\n",
      "epoch 2000, loss 9801.955078125\n",
      "epoch 3000, loss 9801.7080078125\n",
      "epoch 4000, loss 9801.70703125\n",
      "epoch 5000, loss 9801.7099609375\n",
      "epoch 6000, loss 9801.7109375\n",
      "epoch 7000, loss 9801.7099609375\n",
      "epoch 8000, loss 9801.70703125\n",
      "epoch 9000, loss 9801.7099609375\n",
      "epoch 10000, loss 9801.70703125\n",
      "Current best 100.0\n",
      "epoch 1000, loss -1506767011840.0\n",
      "epoch 2000, loss -1506767011840.0\n",
      "epoch 3000, loss -1506767011840.0\n",
      "epoch 4000, loss -1506767011840.0\n",
      "epoch 5000, loss -1506766880768.0\n",
      "epoch 6000, loss -1506767011840.0\n",
      "epoch 7000, loss -1506767011840.0\n",
      "epoch 8000, loss -1506740273152.0\n",
      "epoch 9000, loss -1506766880768.0\n",
      "epoch 10000, loss -1506767011840.0\n",
      "epoch 1000, loss -16737680.0\n",
      "epoch 2000, loss -16737698.0\n",
      "epoch 3000, loss -16737713.0\n",
      "epoch 4000, loss -16737725.0\n",
      "epoch 5000, loss -16737735.0\n",
      "epoch 6000, loss -16737743.0\n",
      "epoch 7000, loss -16737747.0\n",
      "epoch 8000, loss -16737749.0\n",
      "epoch 9000, loss -16737748.0\n",
      "epoch 10000, loss -16737677.0\n",
      "epoch 1000, loss 9.51065444946289\n",
      "epoch 2000, loss 9.258286476135254\n",
      "epoch 3000, loss 9.242385864257812\n",
      "epoch 4000, loss 9.242385864257812\n",
      "epoch 5000, loss 9.242389678955078\n",
      "epoch 6000, loss 9.242385864257812\n",
      "epoch 7000, loss 9.242385864257812\n",
      "epoch 8000, loss 9.242388725280762\n",
      "epoch 9000, loss 9.242385864257812\n",
      "epoch 10000, loss 9.242385864257812\n",
      "Current best 0.01\n",
      "epoch 1000, loss -83688736.0\n",
      "epoch 2000, loss -83688760.0\n",
      "epoch 3000, loss -83688776.0\n",
      "epoch 4000, loss -83688792.0\n",
      "epoch 5000, loss -83688808.0\n",
      "epoch 6000, loss -83688832.0\n",
      "epoch 7000, loss -83688824.0\n",
      "epoch 8000, loss -83688840.0\n",
      "epoch 9000, loss -83688832.0\n",
      "epoch 10000, loss -83688672.0\n",
      "epoch 1000, loss 37.6612434387207\n",
      "epoch 2000, loss 36.61876678466797\n",
      "epoch 3000, loss 36.618587493896484\n",
      "epoch 4000, loss 36.61852264404297\n",
      "epoch 5000, loss 36.618492126464844\n",
      "epoch 6000, loss 36.61847686767578\n",
      "epoch 7000, loss 36.618465423583984\n",
      "epoch 8000, loss 36.61846160888672\n",
      "epoch 9000, loss 36.61845397949219\n",
      "epoch 10000, loss 36.61845779418945\n",
      "Current best 0.05\n",
      "epoch 1000, loss -167377440.0\n",
      "epoch 2000, loss -167377536.0\n",
      "epoch 3000, loss -167377616.0\n",
      "epoch 4000, loss -167377584.0\n",
      "epoch 5000, loss -167370224.0\n",
      "epoch 6000, loss -167375568.0\n",
      "epoch 7000, loss -167375312.0\n",
      "epoch 8000, loss -167376720.0\n",
      "epoch 9000, loss -167369920.0\n",
      "epoch 10000, loss -167377040.0\n",
      "epoch 1000, loss 18.35944938659668\n",
      "epoch 2000, loss 18.114749908447266\n",
      "epoch 3000, loss 18.077848434448242\n",
      "epoch 4000, loss 18.077802658081055\n",
      "epoch 5000, loss 18.077804565429688\n",
      "epoch 6000, loss 18.077804565429688\n",
      "epoch 7000, loss 18.077802658081055\n",
      "epoch 8000, loss 18.078155517578125\n",
      "epoch 9000, loss 18.077808380126953\n",
      "epoch 10000, loss 18.077804565429688\n",
      "Current best 0.1\n",
      "epoch 1000, loss -836888320.0\n",
      "epoch 2000, loss -836888448.0\n",
      "epoch 3000, loss -836888448.0\n",
      "epoch 4000, loss -836888512.0\n",
      "epoch 5000, loss -836888576.0\n",
      "epoch 6000, loss -836888512.0\n",
      "epoch 7000, loss -836888576.0\n",
      "epoch 8000, loss -836888576.0\n",
      "epoch 9000, loss -836888576.0\n",
      "epoch 10000, loss -836888512.0\n",
      "epoch 1000, loss 57.543861389160156\n",
      "epoch 2000, loss 57.35930633544922\n",
      "epoch 3000, loss 57.34044647216797\n",
      "epoch 4000, loss 57.340415954589844\n",
      "epoch 5000, loss 57.340431213378906\n",
      "epoch 6000, loss 57.34180450439453\n",
      "epoch 7000, loss 57.340423583984375\n",
      "epoch 8000, loss 57.34042739868164\n",
      "epoch 9000, loss 57.340579986572266\n",
      "epoch 10000, loss 57.340423583984375\n",
      "Current best 0.5\n",
      "epoch 1000, loss -1673776128.0\n",
      "epoch 2000, loss -1673776768.0\n",
      "epoch 3000, loss -1673776896.0\n",
      "epoch 4000, loss -1673777024.0\n",
      "epoch 5000, loss -1673777024.0\n",
      "epoch 6000, loss -1673768704.0\n",
      "epoch 7000, loss -1673598336.0\n",
      "epoch 8000, loss -1673768960.0\n",
      "epoch 9000, loss -1673221120.0\n",
      "epoch 10000, loss -1673545600.0\n",
      "epoch 1000, loss 134.97479248046875\n",
      "epoch 2000, loss 133.8415069580078\n",
      "epoch 3000, loss 133.8413848876953\n",
      "epoch 4000, loss 133.84136962890625\n",
      "epoch 5000, loss 133.84133911132812\n",
      "epoch 6000, loss 133.84136962890625\n",
      "epoch 7000, loss 133.84141540527344\n",
      "epoch 8000, loss 133.8413848876953\n",
      "epoch 9000, loss 133.8413543701172\n",
      "epoch 10000, loss 133.84133911132812\n",
      "Current best 1.0\n",
      "epoch 1000, loss -8368884224.0\n",
      "epoch 2000, loss -8368885248.0\n",
      "epoch 3000, loss -8368886272.0\n",
      "epoch 4000, loss -8368835584.0\n",
      "epoch 5000, loss -8368799232.0\n",
      "epoch 6000, loss -8365932544.0\n",
      "epoch 7000, loss -8367458304.0\n",
      "epoch 8000, loss -8368839168.0\n",
      "epoch 9000, loss -8368533504.0\n",
      "epoch 10000, loss -8367974400.0\n",
      "epoch 1000, loss 525.184814453125\n",
      "epoch 2000, loss 525.0322875976562\n",
      "epoch 3000, loss 523.2291259765625\n",
      "epoch 4000, loss 522.7548217773438\n",
      "epoch 5000, loss 521.9916381835938\n",
      "epoch 6000, loss 521.0960693359375\n",
      "epoch 7000, loss 518.3889770507812\n",
      "epoch 8000, loss 499.1479187011719\n",
      "epoch 9000, loss 498.97320556640625\n",
      "epoch 10000, loss 498.81243896484375\n",
      "Current best 5.0\n",
      "epoch 1000, loss -16737771520.0\n",
      "epoch 2000, loss -16737771520.0\n",
      "epoch 3000, loss -16737771520.0\n",
      "epoch 4000, loss -16737771520.0\n",
      "epoch 5000, loss -16737772544.0\n",
      "epoch 6000, loss -16737771520.0\n",
      "epoch 7000, loss -16737771520.0\n",
      "epoch 8000, loss -16737771520.0\n",
      "epoch 9000, loss -16737771520.0\n",
      "epoch 10000, loss -16737771520.0\n",
      "epoch 1000, loss 1056.3367919921875\n",
      "epoch 2000, loss 1054.9005126953125\n",
      "epoch 3000, loss 1054.9002685546875\n",
      "epoch 4000, loss 1054.9002685546875\n",
      "epoch 5000, loss 1054.9002685546875\n",
      "epoch 6000, loss 1054.900390625\n",
      "epoch 7000, loss 1054.89990234375\n",
      "epoch 8000, loss 1054.90087890625\n",
      "epoch 9000, loss 1054.900146484375\n",
      "epoch 10000, loss 1054.900390625\n",
      "Current best 10.0\n",
      "epoch 1000, loss -83688849408.0\n",
      "epoch 2000, loss -83688849408.0\n",
      "epoch 3000, loss -83688857600.0\n",
      "epoch 4000, loss -83688849408.0\n",
      "epoch 5000, loss -83685376000.0\n",
      "epoch 6000, loss -83688841216.0\n",
      "epoch 7000, loss -83688759296.0\n",
      "epoch 8000, loss -83688857600.0\n",
      "epoch 9000, loss -83688865792.0\n",
      "epoch 10000, loss -83688759296.0\n",
      "epoch 1000, loss 4905.857421875\n",
      "epoch 2000, loss 4905.7861328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3000, loss 4905.78564453125\n",
      "epoch 4000, loss 4905.7880859375\n",
      "epoch 5000, loss 4905.83642578125\n",
      "epoch 6000, loss 4905.78662109375\n",
      "epoch 7000, loss 4905.787109375\n",
      "epoch 8000, loss 4905.7978515625\n",
      "epoch 9000, loss 4905.78662109375\n",
      "epoch 10000, loss 4905.78564453125\n",
      "Current best 50.0\n",
      "epoch 1000, loss -167377715200.0\n",
      "epoch 2000, loss -167377715200.0\n",
      "epoch 3000, loss -167377715200.0\n",
      "epoch 4000, loss -167377715200.0\n",
      "epoch 5000, loss -167377715200.0\n",
      "epoch 6000, loss -167377731584.0\n",
      "epoch 7000, loss -167377731584.0\n",
      "epoch 8000, loss -167377731584.0\n",
      "epoch 9000, loss -167377715200.0\n",
      "epoch 10000, loss -167377715200.0\n",
      "epoch 1000, loss 9848.5439453125\n",
      "epoch 2000, loss 9826.0029296875\n",
      "epoch 3000, loss 9809.474609375\n",
      "epoch 4000, loss 9801.712890625\n",
      "epoch 5000, loss 9801.7080078125\n",
      "epoch 6000, loss 9801.7109375\n",
      "epoch 7000, loss 9801.716796875\n",
      "epoch 8000, loss 9801.71875\n",
      "epoch 9000, loss 9801.7099609375\n",
      "epoch 10000, loss 9801.7060546875\n",
      "Current best 100.0\n",
      "epoch 1000, loss -167377715200.0\n",
      "epoch 2000, loss -167377715200.0\n",
      "epoch 3000, loss -167377715200.0\n",
      "epoch 4000, loss -167377715200.0\n",
      "epoch 5000, loss -167377715200.0\n",
      "epoch 6000, loss -167377715200.0\n",
      "epoch 7000, loss -167377715200.0\n",
      "epoch 8000, loss -167377666048.0\n",
      "epoch 9000, loss -167377715200.0\n",
      "epoch 10000, loss -167377682432.0\n",
      "CoDA-PCA:\n",
      "[597.0651251402028, 86.80465410185226, 81.03835583736978, 613.4700612788904]\n",
      "CLR-PCA:\n",
      "[434.9678053644901, 38.32415328658511, 140.94255307789376, 284.17231428107345]\n",
      "Naive regression:\n",
      "[620.6173039031717, 91.49904714186422, 89.24510831339641, 638.893795289947]\n",
      "CoDA-Regress:\n",
      "[524.7376723407696, 53.78943652400323, 29.7167057441039, 44.681936740452905]\n"
     ]
    }
   ],
   "source": [
    "synth_scores = PCA_Regression(data, co_feature_indices=[0,1,2], target_index=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[344.5945490895788, 224.6017065025106, 360.06381366209484, 163.2314378373324]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean(synth_scores[i]) for i in range(0,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[170.76430143923963, 224.6017065025106, 360.06381366209484, 189.66704073881988]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean(synth_scores[i]) for i in range(0,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1,2,3,4,5,6,7]\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection  import KFold\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoDA_Regress(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, dimension, encoder_shape, decoder_shape):\n",
    "        super(CoDA_Regress, self).__init__()\n",
    "        \n",
    "        #define regression layer\n",
    "        self.linear = nn.Linear(dimension, 1)\n",
    "        \n",
    "        encoder_dict = OrderedDict()\n",
    "        \n",
    "        #first layer will be twice input size, since we are feeding in both c_kl and X \n",
    "        encoder_dict[\"layer0\"] = nn.Linear(2 * input_dim, encoder_shape[0])\n",
    "\n",
    "        for i in range(0,len(encoder_shape)-1):\n",
    "            encoder_dict[\"layer\"  + str(i)] = nn.Linear(encoder_shape[i], encoder_shape[i+1])\n",
    "            encoder_dict[\"layer_ac\"  + str(i)] = nn.ELU()\n",
    "        encoder_dict[\"final_layer\"] = nn.Linear(encoder_shape[-1], dimension)\n",
    "        encoder_dict[\"final_ac\"] = nn.ELU()\n",
    "\n",
    "        self.encoder = nn.Sequential(encoder_dict)\n",
    "        \n",
    "        decoder_dict = OrderedDict()\n",
    "        decoder_dict[\"layer0\"] = nn.Linear(dimension, decoder_shape[0])\n",
    "\n",
    "        for i in range(0,len(decoder_shape)-1):\n",
    "            decoder_dict[\"layer\"  + str(i)] = nn.Linear(decoder_shape[i], decoder_shape[i+1])\n",
    "            decoder_dict[\"layer_ac\"  + str(i)] = nn.ELU()\n",
    "\n",
    "        #final layer will map back to input dim\n",
    "        decoder_dict[\"final_layer\"] = nn.Linear(decoder_shape[-1], input_dim)\n",
    "        decoder_dict[\"final_ac\"] = nn.ELU()\n",
    "\n",
    "        self.decoder = nn.Sequential(decoder_dict)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        EPS = 1e-6   # to avoid log(0)\n",
    "\n",
    "        #run the encoding and store the low level representation as A\n",
    "        x_ckl = torch.log(torch.clamp(check(x), EPS, 1))\n",
    "        \n",
    "        #pass in both x and x_ckl as per paper\n",
    "        A = self.encoder(torch.cat((x, x_ckl), 1))\n",
    "        reconstruction = self.decoder(A)\n",
    "        pred = self.linear(A)\n",
    "        #return both the predicted target, and the reconstruction so both can be inputs to the combined loss\n",
    "        return pred, reconstruction, A\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, lam, lr, epochs = 10000):\n",
    "        \n",
    "        loss_function = Combined_Loss(lam)\n",
    "        optim = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    \n",
    "        for epoch in range(0,epochs):\n",
    "            pred, recon, A = self.forward(torch.FloatTensor(X))\n",
    "            loss = loss_function(recon, torch.FloatTensor(X), pred, torch.FloatTensor(y))\n",
    "        \n",
    "            optim.zero_grad()\n",
    "        \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "            epoch += 1\n",
    "    \n",
    "            if (epoch % 1000 == 0):\n",
    "                print(\"epoch {}, loss {}\".format(epoch, loss))\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def transform(self, X):\n",
    "        pred, recon, A = self.forward(X)\n",
    "        return A\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred, recon, A = self.forward(X)\n",
    "        return pred\n",
    "    \n",
    "    #recon remains in CLR space, since the loss is derived for similarity to x_ckl\n",
    "    def project(self, X):\n",
    "        pred, recon, A = self.forward(X)\n",
    "        return recon\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combined_Loss(torch.nn.Module):\n",
    "    def __init__(self, lam):\n",
    "        super(Combined_Loss,self).__init__()\n",
    "        self.CoDA_Loss = CoDA_Loss()\n",
    "        self.MSE = nn.MSELoss()\n",
    "        self.lam = lam\n",
    "        \n",
    "    def forward(self,Y,X,y_hat,y):\n",
    "        #X is original data, Y is CoDA reconstruction, y is targets, y_hat \n",
    "        #input needs to be normalised by g(x) (geometric mean) for X_hat\n",
    "        #TODO centering matrix? Reduce mean? Mask for near zero values?  \n",
    "        \n",
    "        #extract reconstruction and original data from concatenation\n",
    "        return  self.MSE(y_hat, y) + self.lam * self.CoDA_Loss(Y,X)  \n",
    "\n",
    "class CoDA_Loss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CoDA_Loss,self).__init__()\n",
    "        \n",
    "    def forward(self,Y,X):\n",
    "        #X is original data, Y is CoDA reconstruction\n",
    "        X_check = check(X)\n",
    "        coda_loss =  torch.sum(torch.exp(torch.clamp(Y, -30, 30))) - torch.sum(X_check * Y)\n",
    "        return coda_loss\n",
    "\n",
    "def check(X):\n",
    "    #assume input is tensor so we can use the numpy() method\n",
    "    assert type(X) == torch.Tensor\n",
    "    gmean = torch.prod(X, 1) ** (1./X.shape[1])\n",
    "    return torch.div(X.t(), torch.clamp(gmean, min=1e-8)).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 282.,  368.,  607.,  532.,  360.,  470.,  102.,  544.,  387.,  294.,\n",
       "         503.,  697.,  393.,  665.,  347.,  791.,  225.,  175.,  333.,  269.,\n",
       "         118.,  154.,  276.,  480.,  373.,  369.,  126.,  460.,  441.,  502.,\n",
       "         126.,  376.,  118.,  303.,  250.,  582.,   69.,  226.,  359.,  453.,\n",
       "         427.,  334.,  364.,  869.,  441.,  615.,  532.,  417.,  360.,  580.,\n",
       "         147.,  500.,  943.,  305., 1151.,  457.,  637.,  284.,  386.,  221.,\n",
       "         208.,  573.,  565.,  170.,  261., 1097.,  408.,   24.,  890.,  168.,\n",
       "          22.,  601.,  364.,  342.,  867.,  691.,  462.,  318.,  461.,  777.,\n",
       "         397.,  347.,  744.,  576.,  321.,  382.,  645.,  459.,  681.,  245.,\n",
       "         575.,  698.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Data 18. Compositions and total pebble counts of 92 glacial tills.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# headers = data[1]\n",
    "# features = data[0][:,co_feature_indices]\n",
    "# targets = data[0][:,target_index]\n",
    "    \n",
    "# #normalise the compositional features. TODO anything extra to deal with non compositional features?\n",
    "\n",
    "\n",
    "features = data[data.columns[1:-1]]\n",
    "targets = data[data.columns[-1]]\n",
    "\n",
    "features = np.array([feat/sum(feat) for feat in features.values])\n",
    "\n",
    "\n",
    "features = torch.FloatTensor(features)\n",
    "targets = torch.FloatTensor(targets)\n",
    "\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7750, 0.1950, 0.0300],\n",
       "        [0.7190, 0.2490, 0.0320],\n",
       "        [0.5070, 0.3610, 0.1320],\n",
       "        [0.5236, 0.4102, 0.0662],\n",
       "        [0.7000, 0.2650, 0.0350],\n",
       "        [0.6650, 0.3220, 0.0130],\n",
       "        [0.4310, 0.5530, 0.0160],\n",
       "        [0.5340, 0.3680, 0.0980],\n",
       "        [0.1550, 0.5440, 0.3010],\n",
       "        [0.3170, 0.4150, 0.2680],\n",
       "        [0.6570, 0.2780, 0.0650],\n",
       "        [0.7040, 0.2900, 0.0060],\n",
       "        [0.1740, 0.5360, 0.2900],\n",
       "        [0.1060, 0.6980, 0.1960],\n",
       "        [0.3820, 0.4310, 0.1870],\n",
       "        [0.1080, 0.5270, 0.3650],\n",
       "        [0.1840, 0.5070, 0.3090],\n",
       "        [0.0460, 0.4740, 0.4800],\n",
       "        [0.1560, 0.5040, 0.3400],\n",
       "        [0.3190, 0.4510, 0.2300],\n",
       "        [0.0950, 0.5350, 0.3700],\n",
       "        [0.1710, 0.4800, 0.3490],\n",
       "        [0.1050, 0.5540, 0.3410],\n",
       "        [0.0478, 0.5443, 0.4080],\n",
       "        [0.0260, 0.4520, 0.5220],\n",
       "        [0.1140, 0.5270, 0.3590],\n",
       "        [0.0670, 0.4690, 0.4640],\n",
       "        [0.0690, 0.4970, 0.4340],\n",
       "        [0.0400, 0.4490, 0.5110],\n",
       "        [0.0741, 0.5165, 0.4094],\n",
       "        [0.0480, 0.4950, 0.4570],\n",
       "        [0.0450, 0.4850, 0.4700],\n",
       "        [0.0660, 0.5210, 0.4130],\n",
       "        [0.0671, 0.4735, 0.4595],\n",
       "        [0.0741, 0.4565, 0.4695],\n",
       "        [0.0600, 0.4890, 0.4510],\n",
       "        [0.0630, 0.5380, 0.3990],\n",
       "        [0.0250, 0.4800, 0.4950],\n",
       "        [0.0200, 0.4780, 0.5020]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Data 5. Sand, silt, clay compositions of 39 sediment samples at different water depths in an Arctic lake.csv\")\n",
    "\n",
    "features = data[data.columns[1:-1]]\n",
    "targets = data[data.columns[-1]]\n",
    "\n",
    "features = np.array([feat/sum(feat) for feat in features.values])\n",
    "\n",
    "\n",
    "features = torch.FloatTensor(features)\n",
    "targets = torch.FloatTensor(targets)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000, loss 6006.15478515625\n",
      "epoch 2000, loss 6000.9638671875\n",
      "epoch 3000, loss 5998.43115234375\n",
      "epoch 4000, loss 5998.88330078125\n",
      "epoch 5000, loss 5998.185546875\n",
      "epoch 6000, loss 5998.57470703125\n",
      "epoch 7000, loss 6000.91064453125\n",
      "epoch 8000, loss 5997.6123046875\n",
      "epoch 9000, loss 6000.8720703125\n",
      "epoch 10000, loss 5997.3291015625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f4ad4460f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training code stub, read in data as X and targets as y\n",
    "#TODO substitute this into model class, and set up API similar to original CoDA-PCA paper\n",
    "features = torch.FloatTensor(features)\n",
    "\n",
    "X = features\n",
    "y = targets.reshape(-1,1)\n",
    "model = CoDA_Regress(X.shape[1], 2, [100,], [3,])\n",
    "\n",
    "model.fit(torch.FloatTensor(X),  torch.FloatTensor(y), 100, lr=1e-2)\n",
    "\n",
    "test = model.transform(X).detach().numpy()\n",
    "plt.scatter(test[:,0], test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch     0] L=  4.2488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f4ac0d4c18>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Data 5. Sand, silt, clay compositions of 39 sediment samples at different water depths in an Arctic lake.csv\")\n",
    "\n",
    "features = data[data.columns[1:-1]]\n",
    "targets = data[data.columns[-1]]\n",
    "\n",
    "features = np.array([feat/sum(feat) for feat in features.values])\n",
    "\n",
    "\n",
    "#features = torch.FloatTensor(features)\n",
    "targets = torch.FloatTensor(targets)\n",
    "\n",
    "import CodaPCA\n",
    "pca = CodaPCA.CodaPCA(2,lrate=1e-3,nn_shape=[50,50], alg=CodaPCA.Alg.CODAPCA)\n",
    "\n",
    "test = pca.fit_transform(features)\n",
    "\n",
    "plt.scatter(test[:,0], test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]),\n",
       "  array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])),\n",
       " (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]),\n",
       "  array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])),\n",
       " (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 30, 31, 32, 33, 34, 35, 36, 37, 38]),\n",
       "  array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])),\n",
       " (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       "  array([30, 31, 32, 33, 34, 35, 36, 37, 38]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(4)\n",
    "splits = [i for i in kf.split(features)]\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000, loss 229.2849884033203\n",
      "epoch 2000, loss 226.8304443359375\n",
      "epoch 3000, loss 226.8303985595703\n",
      "epoch 4000, loss 226.8304443359375\n",
      "epoch 5000, loss 226.83045959472656\n",
      "epoch 6000, loss 226.83035278320312\n",
      "epoch 7000, loss 226.83038330078125\n",
      "epoch 8000, loss 226.83041381835938\n",
      "epoch 9000, loss 226.8317108154297\n",
      "epoch 10000, loss 226.83042907714844\n",
      "epoch 1000, loss 1625.788818359375\n",
      "epoch 2000, loss 613.612548828125\n",
      "epoch 3000, loss 246.9186553955078\n",
      "epoch 4000, loss 144.53707885742188\n",
      "epoch 5000, loss 126.25094604492188\n",
      "epoch 6000, loss 123.6268081665039\n",
      "epoch 7000, loss 123.3441390991211\n",
      "epoch 8000, loss 123.31414794921875\n",
      "epoch 9000, loss 123.2872314453125\n",
      "epoch 10000, loss 123.2830810546875\n",
      "epoch 1000, loss 184.08709716796875\n",
      "epoch 2000, loss 183.98304748535156\n",
      "epoch 3000, loss 183.98287963867188\n",
      "epoch 4000, loss 183.98281860351562\n",
      "epoch 5000, loss 183.9828338623047\n",
      "epoch 6000, loss 183.9828643798828\n",
      "epoch 7000, loss 184.0028839111328\n",
      "epoch 8000, loss 183.98275756835938\n",
      "epoch 9000, loss 184.0028533935547\n",
      "epoch 10000, loss 183.98281860351562\n",
      "epoch 1000, loss 86.65428161621094\n",
      "epoch 2000, loss 86.65421295166016\n",
      "epoch 3000, loss 86.6541976928711\n",
      "epoch 4000, loss 86.6541748046875\n",
      "epoch 5000, loss 86.65414428710938\n",
      "epoch 6000, loss 86.65412139892578\n",
      "epoch 7000, loss 86.65425109863281\n",
      "epoch 8000, loss 86.65412139892578\n",
      "epoch 9000, loss 86.65419006347656\n",
      "epoch 10000, loss 86.65410614013672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[152.93797, 904.40265, 278.76932, 1039.7075]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training code stub, read in data as X and targets as y\n",
    "#TODO substitute this into model class, and set up API similar to original CoDA-PCA paper\n",
    "\n",
    "\n",
    "X = features\n",
    "y = targets.reshape(-1,1)\n",
    "#define the combined loss with hyperparameter lambda\n",
    "scores = []\n",
    "for ind in splits: \n",
    "    model = CoDA_Regress(X.shape[1], 2, [101,], [3,])\n",
    "    \n",
    "    #model = Ridge(1)\n",
    "    \n",
    "    model.fit(torch.FloatTensor(X[ind[0]]),  torch.FloatTensor(y[ind[0]]), 0, lr=1e-2)\n",
    "\n",
    "    y_pred = model.predict(torch.FloatTensor(X[ind[1]]))\n",
    "\n",
    "#     print(y_pred)\n",
    "#     print(y[ind[1]].detach().numpy())\n",
    "    \n",
    "    scores.append(sklearn.metrics.mean_squared_error(y[ind[1]].detach().numpy(),y_pred.detach().numpy()))\n",
    "    \n",
    "scores\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

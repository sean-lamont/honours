\chapter{Experimental Methodology}
\label{cha:methodology}

\subsection{Experiments}
In this section we present the results from experiments on the combined model. Experiments were done for both regression and classification tasks using several data sources. We present the results of our model in comparison to several baselines on each of these sources. 

\subsubsection{Model Evaluation}
The results for the smaller Aitchinson datasets were obtained using  k-fold cross validation combined with an inner hyperparameter selection loop. 

K-Fold cross validation is the standard approach for model evaluation in most machine learning contexts. The procedure is as follows: Choose $k$, which is the number of groups to split the data into. Shuffle the data, split into $k$ groups and take one as the test set. Train the model on the $k-1$ groups, and evaluate the performance on the test set. Repeat this procedure, changing which group is used for the test set. 

This approach is highly unbiased, since the data is shuffled and so trained and tested on separate partitions of the data. In order for the model to perform well overall, it must do well on each partition of the data. This prevents a good model score being due to a lucky initial testing split. Likewise, if a model performs well on only some splits, then this suggests it does not generalise well.  

Given the sensitivity of our model to the choice of $\lambda$, one solution is to use K-Fold cross validation with an additional inner loop for hyperparameter selection. The high level idea of this approach is as follows: Given a set of hyperparameters $\Lambda = \{\lambda_1 ,.., \lambda_n\}$. For each training partition of the initial K-Fold, $X_k$ we do another cross validation of $X_k$, for each $\lambda \in \Lambda$. The value of $\lambda$ which results in the best mean cross validation score is then used to train the model on $X_k$. Since we still do not have access to the testing data, this method will still give valid results. There is a large computational cost to this approach, with training taking $O(k|\Lambda|)$. This method, found in \cite{walder} is primarily useful for small datasets, due to the high associated computational costs. As such, it was performed on the initial testing of the model with the Aitchinson data.     

\begin{figure*}
    \centering
    \includegraphics[width = \columnwidth]{figs/cross_val(1)(1).pdf}
    \caption{Hyperparameter selection loop for model evaluation: $\mathbf{X_{train}}$ is partitioned (here into 5). The final $\lambda^*$ is selected as the best performing $\lambda_i$ on the mean inner cross validation score, $L(\lambda_i)$. $\lambda^*$ is then used for training on $\mathbf{X_{train}}$, and this is repeated for each training split in the outer cross validation loop. }
    \label{fig:cross_val}
\end{figure*}

\subsection{Data Sources}

\subsubsection{Aitchinson Data}
The first data source that we test on is the supplementary data presented by Aitchinson [ref.]. This data is commonly used in CoDA research, and consists of 40 small sample (up to 92) compositional datasets. As part of this project, this data was digitised and uploaded to Zenodo. Zenodo is an open access research repository, and so this will allow other researchers to easily access this data in the future. The digitised data can be found here..  


\subsubsection{Metagenome Data}
The next source was from a recent paper in Nature \citep{Stewart2019}, which provided the metagenome analyses from several hundred cattle rumen. The depth of each genome (which acts as a proxy for species count) is given for each of the cattle in the sample. The collection of depths per cattle is compositional due to the capacity of the sequencing device being limited, as we saw from \cite{Gloor2017}. There were 4941 genomes identified between the 283 cattle. This data provides a good opportunity for testing the model on high dimensional data, since $d >> n$ for dimension $d$ and sample size $n$.  

\subsubsection{Additional Microbiome Data}


\subsection{Model Tuning}
\subsubsection{Early Stopping}
Add figures and cross val results showing dip in training loss, increase in val loss and decrease in cross val performance
A useful technique in the training of machine learning algorithms is early stopping. This is often employed to reduce model overfitting, as the model weights can become overly optimised on the training data if trained for too long, and so lose their ability to generalise. Early stopping is a simple solution to this; it terminates the training early if the training and validation loss diverge. 



\subsubsection{Numerical Stability}
The implementation of Machine Learning algorithms on digital computers often leads to numerical instability. Given that real numbers can only be approximated to a finite precision with floating point numbers, operations on very large or small numbers can lead to numbers beyond the scope of this representation. In addition, the finite precision means that there is an inherent error associated with operations on these numbers. These errors accumulate, and can lead to results which do not align with what it would be analytically. Even when a high precision is used, it is well known that matrix operations are particularly prone to numerical instability. Given that these form the basis for many machine learning algorithms, numerical analysis is an important consideration.

clamping, NaN, exploding gradients, vanishing gradients, learning rates, local optima 


\subsection{Hyperparameter Grid Search}
We will see from the preliminary results using the Aitchinson test data that the choice of hyperparameters can have a large influence on the performance of our model. The goal of our experiments is to determine the conditions under which our model is effective. Appropriate analysis of the hyperparameters is crucial to this understanding. 


\subsection{Baselines}
To test the performance of our model, we need suitable baseline algorithms to compare against. For regression problems, we refer to Ordinary Least Squares Regression (\ref{regression}). For classification, we consider softmax based Logistic Regression presented in \ref{logisticreg}. From these algorithms, we construct the following baseline approaches:

\begin{itemize}
    \item Naive Regression: This baseline is simply to apply OLS regression directly on the features and targets, in the case of a regression problem 
    \item Naive Logistic Regression: Similarly, this baseline applies logistic regression directly for classification tasks
    \item CLR + (Logistic) Regression: This baseline applies the CLR transformation, then standard PCA to preprocess the features. The resulting representation is then used as the features for either Regression or Logistic Regression
    \item CoDA-PCA + (Logistic) Regression: This baseline applies the CoDA-PCA transformation \citep{Avalos2018}, then standard PCA to preprocess the features. The resulting representation is then used as the features for either Regression or Logistic Regression
\end{itemize}


There are a vast number of methods in the literature which are suitable for regression and classification problems. 
Undoubtedly, many of these would perform better than our baselines. The baselines we take are chosen to reflect the loss functions which are used in our combined model. We are interested in whether the end to end architecture can improve performance for the reasons we hypothesised (making the representation a function of the targets). Hence by using the baselines as regression and logistic regression, this allows us to isolate the difference in results as being due to the end to end architecture, rather than due to a \\

For classification, use random guess and also pick the most frequent. To ensure the model is actually learning meaningful patterns from the data, it must perform better than a simple "choose most frequent" approach. This prevents drawing false conclusions about model performance when there is a non uniform distribution of classes.











%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
